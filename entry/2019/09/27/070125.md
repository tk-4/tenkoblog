---
title: do280_復習
date: 2019-09-26T22:01:25.000Z
id: "26006613440657388"
draft: true
---
[:contents]


# 試験範囲
- OpenShift Container Platform の全般的な設定と管理
  - コマンドラインと Web コンソールを理解して使用する
  - プロジェクトを作成して削除する
  - Kubernetes リソースをインポート、エクスポート、設定する
  - 永続レジストリストレージを設定する
  - リソースとクラスタのステータスを確認する
  - ログを確認する
  - 一般的な問題をトラブルシューティングする

- Docker のイメージ管理
  - イメージレジストリを理解して操作する
  - イメージをリストする
  - アーカイブファイルからイメージをロードする
  - イメージタグを使用する
  - イメージをプルおよびプッシュする

- ユーザーおよびポリシーの管理
  - ユーザーを作成して削除する
  - ユーザーのパスワードを変更する
  - ユーザーおよびグループの権限を変更する

- アプリケーションの作成と管理
  - 永続アプリケーション・ストレージをプロビジョニングする
  - Source-to-Image (S2I) を使用してアプリケーションをデプロイする
  - Git を使用してアプリケーションを設定する
  - アプリケーション・テンプレートを編集してインポートする
  - 既存のコンポーネントからアプリケーションをアセンブルする
  - マルチコンテナ・アプリケーションをデプロイする
  - コンテナ化されたサービスを作成する
  - 外部ルートを作成して編集する
  - TLS 証明書を使用してルートをセキュリティ保護する

- 監視とチューニング
  - メトリクスをインストールして設定する
  - リソース使用量を制限する
  - 増加する要求に合わせてアプリケーションを拡張する
  - クラスタノードへの Pod 配置を制御する

------------------------------------------------------------------

# OpenShift Container Platform の全般的な設定と管理

------------------------------------------------------------------

## 基本的な用語(補足）
- bc … build configuration
- dc … deploy configuration
- is … Image Stream
- rs … Replica Set。1POD内のコンテナ数
- S2I … ソースをデプロイするとコンテナイメージまで作成してデプロイしてくれる。
```
Projectの中にbc/dcができる。
外とコンテナとかやり取りするときにisが動く。
コンテナはPodという単位で管理する。
たいてい1コンテナに1POD
PODに対しては直接接続させることもできるが、基本的にはrouter -> service -> POD という形になる。
なお、service は対象のPOD内コンテナを宣言しているだけなので、実質はrouter -> PODというアクセスの流れになる。
etcdの中身は、KEY-VALUE形式のテキストの集まり。

```

## コマンドラインと Web コンソールを理解して使用する

- ログイン

```
$ oc login -u <username> -p <password> <ocp-url>
```

- 現在のユーザの確認

```
$ oc whoami

```


- メジャーなリソースを一式もってくる（ただし、descrubeほどではない)

```
$ oc get all
```


- 現在のステータスを確認する
```
$ oc status [-v]
```


- ヘルプ
```
$ oc help
$ oc <subcommand> --help
```
---------------------------------

## プロジェクトを作成して削除する

- プロジェクトの作成

```
$ oc new-project <project-name>
```

- アプリケーションの追加

```
-- SCM編 --

$ oc new-app <appname:version>~<SCM-URL>

-- Dockerレジストリ編　その１ --
$ oc new-app --docker-image=registry.lab.example.com/openshift/hello-openshift --name=hello

これで、http://registry.lab.example.com/openshift の配下にある、hello-openshift っていうコンテナイメージを持ってきて、それをhelloっていう名前のアプリケーションとして登録しろ、っていう意味になる模様。

-- Dockerレジストリ編　その２ --
$ oc new-app <app-name> <ENV-DATA> -l <label-name>

Docker設定ファイル(/etc/sysconfig/docker)に指定されているレジストリから該当するアプリのコンテナをもってきて、それに対して環境変数を指定したりラベルを付けてアプリケーション登録をするやりかた。

-- Dockerレジストリ編　その３ --
$ oc new-app <container-url> -i <appname:version> --name=<app-name>
Dockerレジストリに複数のバージョンがある場合、バージョンを指定して持ってくる。

 -e <ENV_NAME>=<ENV_PARAM>などで環境変数を付与することもできる。

-- File指定編 --

$ oc new-app --file=<filename>
ex)
$ oc new-app --file=mysql-ephemeral.yml

これは、あくまでetcdにYMLファイルを登録する、というコマンドみたい。
この中に、さらにアプリケーションのテンプレートが含まれていたら、そのあとにアプリケーションがデプロイされる。
```




- プロジェクトの削除

```
$ oc delete project <project-name>
```

---------------------------
## Kubernetes リソースをインポート、エクスポート、設定する

- リソースをエクスポートする。

```
$ oc export <resorce-name> 

通常のコマンドの場合、画面に表示されるだけ。YAMLとかJSONとかで出力したい場合はリダイレクトする。
```

- リソースを作成する

```
$ oc create <options>

よく使うのは、exportをリダイレクトして出力したYAMLファイルを編集し、そこから類似のリソースを作る場合。

$ oc create -f <YAMLFILE>
```


---------------------------
## 永続レジストリストレージを設定する

とりあえず、永続のNFS領域がEXPORTされていないと意味がない。

起動時に再度読み込ませるようにするには、/etc/exports.d/ の配下にファイル。

中身としては以下のような形。

```
/var/export/dbvol *(rw,async,all_squash)
```

---------------------------

## リソースとクラスタのステータスを確認する

- ノード状態の確認

```
$ oc get nodes
```
追加情報確認用で、-o wide などのオプションがある。


- Pod状態の確認

```
$ oc get pods
```

追加情報確認用で、-o wide などのオプションがある。

- routes状態の確認

```
$ oc get routes
```

- helloという名称のアプリケーションのレプリカ数を2にあげる

```
$ oc scale --replicas=2 dc hello
```

- 詳細情報の確認
```
oc describe <object-name> <options>
```

--------------------------------

## ログを確認する

- ログの確認

```
$ oc logs <options>
基本的にはPOD名を入れてログを見る。
-f bc/hello とかで、tail -f としてbcのhelloが含まれるもののログをtailしてくれる？
```

- Eventの確認

```
$ oc get events
```

--------------------------------
## 一般的な問題をトラブルシューティングする


### 各種リソースの設定の修正

```
$ oc edit <resource-name>
```
これをやると、vimで対象リソースのYAMLファイルが開かれる。

### アプリに接続できない場合

各コンテナに対しては、Nodeからでないとアクセスができない。
ユーザがアクセスできるのは、exposeされたサービスのみ。

そのため、切り分けを行う場合にはNodeからアクセスできるか、という確認手段をとる。
1. NodeからPodにアクセスできるか
2. NodeからClusterIPにアクセスできるか
3. Userからsvcにアクセスできるか

1.の接続対象の確認は $ oc get pods -o wide

2.の接続対象の確認は $ oc get svc

3.の接続対象の確認は $ oc get routes

svc->podに行くには、selectorセクションにおいて宣言されたapplication nameがPodのLabel名と一致している必要がある。

```
 $ oc describe svc
Selector: app=hello_openshift

 $ oc describe pods
 Labels: app=hello-openshift
 
 この場合、_ と　- が異なるので接続できない。(svc側でEndPointがNoneになる）

 $ oc edit svc <service-name> 
 などのコマンドで修正してやる必要がある。

 また、route側でもspec: to : の項目の赤にnameというものがあり、これもルーティングに必要。
 これが一致していないとアプリケーションエラーになる。
 
 $ oc edit route <route-name>
などで設定を修正する。
```

### コンテナの中身のバックアップ
永続化しない場合、消える。

コンテナの中身を外に出したい場合はoc rsyncを使う。

```
$ oc rsync <pod>:<pod_dir> <local_dir> -c <container>
containerに入るのはコンテナID。数字とか。PODに複数コンテナがいる場合につけるオプション。
```

### コンテナに対して直接接続する

Port-Forward機能を活用する。

```
$ oc port-forward <pod> [<local-port>:]<remote_port>
```
MYSQLコマンドを直接たたいてテーブル見たいときとかに使う。
ポートフォワードしているコンソールとは別のコンソール開いて操作する感じ。

```
mysqlコマンドの復習
mysql -h<hostname> -u<username> -p<password>

show databases;

exit
```

### ビルドに失敗する場合

まずはイベントやログを確認する。
必要に応じて、Deploy Settingを修正する。

```
$ oc get events
$ oc logs <build-container-name>
$ oc describe <resource-name>

$ oc edit dc/<app-name(DeploymentConfig)>
```

ビルドの止め方と再ビルドのさせ方

Quotaとかに引っかかってビルドが止まる場合、いったんビルド止めて問題解消してもう一度走らせるとかある。

```
とめかた
$ oc rollout cancel

再開
$ oc rollout latest
```

### NodeがReadyになっていないとき

Node上で以下コマンドでサービス起動状態を確認する。

```
systemctl status atomic-openshift-node.service -l

-lオプションってなんだっけ？
  ->ログを全部出すっぽい。
```

### どうしてもRunningにならないとき
isを見てみる。

```
$ oc describe is
```
------------------------------------------------------------------

# Docker のイメージ管理

------------------------------------------------------------------

## イメージレジストリを理解して操作する

- dockerビルドの方法

git cloneした後にビルドする。
```
$ docker build -t <image-name>:<tag-name> .
-> これやると、自身のdockerイメージリポジトリの中に入ってくる。

```

--------------------------------
## イメージをリストする

- isにあるアプリケーション情報の表示
```
$ oc describe is <app-name> -n <namespace>

使えるバージョンとか出てきたりする。整ってないやつとか。
```
- 自身のdockerイメージリポジトリの情報を表示する。

```
$ docker images
リポジトリ名、TAG情報などが表示される。
```

--------------------------------
## アーカイブファイルからイメージをロードする

tarからイメージをロードする場合、以下のようなコマンドを使う。

```
docker load -i <tar-filename>
```

```
[student@workstation schedule-is]$ ls -la
total 169576
drwxr-xr-x.  2 student student        69 Sep 26 11:47 .
drwxr-xr-x. 10 student student       176 Sep 26 11:47 ..
-rw-r--r--.  1 root    root    173638144 Sep 26 11:47 phpmyadmin-latest.tar
-rwxr-xr-x.  1 student student       463 Aug 17  2018 trust_internal_registry.sh


[student@workstation schedule-is]$ docker load -i phpmyadmin-latest.tar
cd7100a72410: Loading layer [==================================================>] 4.403 MB/4.403 MB

Loaded image ID: sha256:93d0d7db5ce202f07f3cedd5c027e2f97874d0df904594f53d46024b021603ec
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ docker images
REPOSITORY                                      TAG                 IMAGE ID            CREATED             SIZE
<none>                                          <none>              93d0d7db5ce2        15 months ago       166 MB



```
--------------------------------
## イメージタグを使用する

docker tagコマンドを使う。

```
$ docker tag <Image ID> <repository-value>/<image-name>:<tag-name>
```

--------------------------------
## イメージをプルおよびプッシュする

- docker pullコマンドを使う。

```
$ docker pull <registry-url>/<contener-name>
-> これができないと、oc get podsのSTATUSがImagePullBackOffとかになる。
```

- docker pushコマンドを使う。

```
$ docker push <repository-value>/<image-name>:<tag-name>
-> デフォルトで設定されているレジストリにPushされるんであって、<repository-value>は関係ない？それとも、複数リポジトリがある時にどこにあげるか、って指定かな。後者かな。

```
pushするときには、トークン、ログインが必要になる時もある。レジストリの証明書関係が出たら捨てる…

```
[student@workstation schedule-is]$ TOKEN=$(oc whoami -t)

[student@workstation schedule-is]$ docker login -u developer -p ${TOKEN} docker-registry-default.apps.lab.example.com
Login Succeeded

[student@workstation schedule-is]$ docker push docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin:4.7
The push refers to a repository [docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin]

```

------------------------------

# ユーザーおよびポリシーの管理

## ユーザーを作成して削除する

### Reguler Account

OpenShiftのMasterノードに対しての認証と、その中のProjectにあるuserオブジェクトは別物。
例えば、Htpasswdを使う場合は以下のようにMasterNodeでユーザを追加・削除する。

```
-- ユーザの追加(user-object) --
$ oc create user <username>
-> このあと、権限を与えたりする。

-- ユーザの追加(htpasswd) --
# htpasswd -b /etc/origin/master/htpasswd <username> <password>

-- ユーザの削除(user-object) --
$ oc delete user <username>
-> このあと、権限を与えたりする。

-- ユーザの削除(htpasswd) --
# htpasswd -D /etc/origin/master/htpasswd <username>

```

### Service Account
通常のログイン可能なユーザではなく、コンテナに特殊な権限(rootでの起動など)を与えるために利用するもの。

Security Context Constraints(SCCs)と呼ばれる仕組みがある。

```
$ oc get scc
```
これでSCC名に付与されるセキュリティ権限を確認できる。

たとえば、コンテナのroot起動を許可する場合はanyuidを利用する。

```
$ oc create serviceaccount <service-account-name>
$ oc patch dc/<app-name> --patch {{途中はいろいろ {"ServiceAccountName":"<svc-account-name>}}}
これで、dcに登録されているアプリケーションに対し、sccを使うサービスアカウントを登録する。
$ oc adm policy add-scc-to-user anyuid -z <svc-account-name>

```
ログイン可能なユーザアカウントに対してscc権限を付与することもできるが、セキュリティ強度が落ちるのでよろしくない。


### SecretやConfigMapについて
ここに書くのが正しいか微妙だが、secretやConfigMapなどのオブジェクトを使って、ENV上からは実データ(パスワードなど）が直接見えないようにすることが可能。

- secretの作成
```
$ oc create secret generic <secret-name> --from-literal=key1=secret1 --from-literal=key2=secret2
```

- secretの確認
```
$ oc get secret <secret-name>
```
- secretの付与？
```
$ oc secret add --for=mount serviceaccount/serviceaccountname secret/secretname
```

- secretの用途
  - ユーザアカウントとパスワード
  - TLSのKeyPair
ただし、ハッシュ化されるだけであり、暗号化されるわけではない。

- ConfigMapの作成
ConfigMapは、ENVの情報などを使いまわしするようなときに使う。

```
$ oc create configmap <config-map-name> --from-liretal=<env-name>=<value>
```
これは平文。


--------------------------------
## ユーザーのパスワードを変更する

これは、htpasswdの場合でいいのか？再度作成するだけだと思う。

```
-- ユーザの追加(htpasswd) --
# htpasswd -b /etc/origin/master/htpasswd <username> <password>
```
--------------------------------
## ユーザーおよびグループの権限を変更する

- 権限の付与

基本的に権限の付与は、masterノードのsystem:adminしか実施できない。

```
# oc adm policy add-cluster-role-to-user cluster-admin <username>

oc adm policy <target-action> <role> <group-name>
```

権限の種類はいろいろある。

- Cluster role系

 名前 | 目的 | 備考
 ---|---|---
 cluster-status | クラスタ情報の確認 | system:authenticated system:authenticated:oauthとセット
 cluster-admin |クラスタ管理者 | -
   |  |  



- Reguler role系

 名前 | 目的 | 備考
 ---|---|---
 admin | 管理者 | プロジェクト内なら何でもできる
 edit | 開発者 | リソースの作成とかはできるけどProjectそのものはいじれない？
 basic-users  | 閲覧者 | Read Only
  self-provisioner | projectの作成 | system:authenticated system:authenticated:oauthとセット

system:authenticated system:authenticated:oauthは、ユーザがログインしたときに所属するグループ名。

このグループから、self-provisionerというroleを削除することでProject作成に関する権限を消すことができる。

例えば、該当プロジェクトにおいて、特定ユーザの権限を操作したければ、以下コマンドを使う。

```
$ oc adm policy add-role-to-user edit <username> -n <namespace(project-name)>
```

例えば、該当プロジェクトにおいて、全ユーザのリソース作成とか消したければ、以下コマンドを使う。

```
$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated 
system:authenticated:oauth
```
------------------------------------------------------------------

# アプリケーションの作成と管理

------------------------------------------------------------------

## 永続アプリケーション・ストレージをプロビジョニングする

### 考え方
永続ストレージに保存しないとデータは消える。

Volume Access には3種類のモードがある。

名前 | 目的 | 備考
 ---|---|---
 ReadWriteOnce | RWO  | 占有。ブロックストレージとかにつかう
 ReadOnlyMany | ROX | みんなで参照する
 ReadWriteMany  | RWX | みんなで更新する

PV/PVCという概念がある。
- PV…物理リソースとの間の接続枠みたいなもの
- PVC…PVとコンテナ間をつなぐ

PVでサイズとかが宣言できるが、それはPVCを自動割り当てするときに使われるだけでクウォータなどではない。

### NFSのマウント
NFSサーバ側では、/etc/exportsファイルの中に書く。

```
/var/export/vol *(rw,async,all_squash)
```
all_squashとは、みんなが匿名ユーザでアクセスしに来る方法。

マウント可否状態の確認
```
# showmount -e
```

ノードからのマウント確認
```
# mount -t nfs <nfs-fqdn>:<nfs-path> <mount-point>
ex)
# mount -t nfs services.lab.example.com:/var/export/dbvol /mnt
```

OpenShiftでのボリューム(PV)の宣言はYAMLでのインポートで実施。

```
$ oc create -f <yaml-file-name>
```
AccessModesやcapacity storage sizeなどの宣言がある。

EmptyDirとかあると、ボリュームのマウントに失敗してる。
なので、PVCを宣言する。

```
$ oc set volume dc/<app-name> --add --overwirte --name=<pv-name> --claim-name=<name> --claim-size=<size> --claim-mode=<mode>
```

PVCの確認
```
$ oc get pvc
```

--------------------------------
## Source-to-Image (S2I) を使用してアプリケーションをデプロイする

```
-- SCM編 --

$ oc new-app <appname:version>~<SCM-URL>

SCM-URLだけでも自動判別できたりするけど…大体はじかれて、オプションパラメータを求められるケースが多い。

```

--------------------------------
## Git を使用してアプリケーションを設定する

- Gitそもそもの使い方

```
所定のディレクトリに移動して
$ git clone <git-url>

$ git clone http://services.lab.example.com/node-hello
-> node-helloディレクトリができ、その中にGIT(SCM)上にあるデータがクローンされる。
```

--------------------------------
## アプリケーション・テンプレートを編集してインポートする

基本的に全ユーザに使えるテンプレートはopenshift というネームスペースに配置。
プロジェクト内だけに使わせたいものは対象のプロジェクトに配置。

テンプレートそのものは、テンプレート用のYAMLファイルを用いてCreateする。

```
$ oc create -f <yaml-filename>

ラベルを付けたい場合はこんな感じ。

$ oc create -f <yaml-filename> -l <label-name>
ex) oc create -f dc.yml -l name=mylabel

```

確認方法
```
[student@workstation ~]$ oc get template
No resources found.

これが対象のプロジェクトで確認した結果

[student@workstation ~]$ oc get template -n openshift
NAME                                            DESCRIPTION                                                                        PARAMETERS        OBJECTS
3scale-gateway                                  3scale API Gateway                                                                 15 (6 blank)      2

これがopenshift ネームスペースで確認した結果。
```



--------------------------------
## 既存のコンポーネントからアプリケーションをアセンブルする
--------------------------------
## マルチコンテナ・アプリケーションをデプロイする
--------------------------------
## コンテナ化されたサービスを作成する

```
個人作成ツールによるレジストリ内のイメージの確認
[student@workstation ~]$ docker-registry-cli registry.lab.example.com search php ssl

ISの登録状況。これで使えないやつとかのエラー原因が出てくる。

[student@workstation secure-route]$ oc describe is php -n openshift

```


--------------------------------
## 外部ルートを作成して編集する

- 外部ルートの作成

CLIで実施する場合、アプリケーションのデプロイをやったとしても、ルートは作成されない。
ルートの作成そのものは、service(svc)に対してexposeする、というオペレーションになる。

```
$ oc expose sercice(svc) <app-name> --name <route-name> --hostname=<fqdn>
```

演習では、アプリケーション名とroute名を同じ名前でばかり作成していたので区別があまりつかなかった。

--hostname の指定は、外部に対して公開するFQDNを指定するオプション。
デフォルトでは、以下のような作りで作成される。
```
<route-name>-<project-name>.<default-domain>
```
たとえば、プロジェクト名がtest、route名がquoteであり、デフォルトドメイン名がapps.example.comの場合、以下のようなFQDNが自動でhostnameに付与される。

```
quote-test.apps.example.com
```

このデフォルトドメインの定義は、以下ファイルを編集することで設定が可能。
```
/etc/origin/master/master-config.yaml

routingConfig:
  Subdomain: apps.example.com

```


--------------------------------
## TLS 証明書を使用してルートをセキュリティ保護する

標準ではrouterはHTTP/HTTPSで待ち受けしている。

TLSは、3つのパターンがある。
- Edge Termination : routerに証明書を入れるパターン
- Pass-through Termination : PodまでTCP443のままパススルーしてコンテナに証明書入れる
- Re-encryption Termination : routerでTLSを終端しつつ、コンテナまでもTCP443する

証明書を作る方法はいつものやつ（空で打てない・・）

```
$ openssl genrsa -out example.key 2048
$ openssl req -new -key example.key -out example.csr -subj "/C=us/ST=CA/L=Los /O=Example/OU=IT.CN=test.example.com"

↑これで対話式じゃなくて値を与えてあるのね

$ openssl x509 -req -days 366 -in example.csr -signkey example.key -out example.crt
```

これをrouterに対してインポートする？新規作成する？

```
$ oc create route edge --service=test --hostname=test.example.com --key=example.key --cert=example.crt

oc create route <type> --service=<route-name> とかかな？
```

ワイルドカード証明書を使うには、事前に設定が必要。
その設定をロードさせるため、routerを再起動させる必要がある。
ってことは、初期の段階で投入しといたほうがいい設定ということか。

手続きとしては、一度routerのレプリカ数を0＝サービス停止し、ワイルドカード有効化フラグを立てて再度レプリカ数を増やす。
```
$ oc scale dc/router --replicas=0
$ oc set env dc/router ROUTER_ALLOW_WILDCARD_ROUTES=true
$ oc scale dc/router --replicas=1
```

この後、ワイルドカードポリシーを作成する。

```
oc expose svc test --wildcard-policy=Subdomain --hostname='www.lab.example.com'
```
このとき、wwwは何でもいい、って言ってた。
教材によると、要するにこの指定方法だとrouterに定義されているSubdomain全体を処理することになるからみたい。

その後の確認コマンドの結果の例
```
[student@workstation secure-route]$ oc get route
NAME      HOST/PORT                    PATH      SERVICES   PORT       TERMINATION   WILDCARD
hello     hello.apps.lab.example.com             hello      8080-tcp   edge          None

```

んー、複数サービスがある場合はどうなるんだ…？

設定情報をテキスト形式で確認したい場合、以下のようなコマンドをたたく。
```
$ oc get route <route-name> -o yaml
```


------------------------------------------------------------------

# 監視とチューニング

------------------------------------------------------------------

## メトリクスをインストールして設定する

これまじで出るの…？

- 3.10までは、Hawkular
- 3.11以降はPrometheusがメトリクス。
  
インストールはAnsibleでやるよ。
解説はInstallガイドの34章にある。
```
これをやる前にインベントリファイルが必要だけど、作れる気がしない。

[student@workstation schedule-is]$ cat /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml ]

[student@workstation install-metrics]$ ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml

$ oc get pods -n openshift-infra
で状態確認

```


--------------------------------
## リソース使用量を制限する

LimitとQuota

ドキュメントはCluster Administrator　の15章。

クウォータは、LimitとQuotaセットでないと動かない。
設定は、管理者じゃないとうごかない。developerではだめ。(消せちゃ意味がない）

確認は、oc describe limit とかが必須。そうでないと詳細が出ない。(名前しか出ない）

- Limit Range
requestされるリソース1件に対して許可できる範囲
  - MAX
  - MIN
  - DEFAULT
  
何も要求されなければDEFAULTになる。

- QUOTA
PROJECT全体の最大リソースサイズ。

- 作成方法
```
- quota
$ oc create quota <quota-name> <quota-options>

- limit
これはYAMLでインポートするしかないらしい
kind: "LimitRange"にしたテンプレートでYAMLを作成し、
$ oc create -f <yaml-file>

サンプルYAML
[student@workstation ~]$ cat ~/DO280/labs/monitor-limit/limits.yml 
apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "project-limits"
spec:
  limits:
    - type: "Container"
      default:
        cpu: "250m"

```

- 現状のLimit、quota使用状況の確認方法

```
$ oc describe limits

[student@workstation ~]$ oc describe limits
Name:       project-limits
Namespace:  resources
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    250m             250m           -

かかっているlimitによっていろいろ変わる。

こっちはクウォータ

[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         0     900m

```


リソースの要求使用量とかは、dcに記録されている。
なので、そこの値を変更することでLimitに引っかかった場合にデプロイが可能になることも。

```
$ oc set resources dc <app-name> --requests=<resouce-name=size>
```

--------------------------------
## 増加する要求に合わせてアプリケーションを拡張する

スケーリングの話か？

```
[student@workstation ~]$ oc scale --replicas=5 dc scaling
deploymentconfig "scaling" scaled
[student@workstation ~]$ oc describe dc scaling | grep Replicas
Replicas:	5
	Replicas:	5 current / 5 desired
```

一応、オートスケーリングも可能みたい。
メトリクスと合わせてCPU使用率80%とかでやる。



--------------------------------
## クラスタノードへの Pod 配置を制御する

### ラベリングによるノードへの配置コントロール

ノードへのラベルのつけ方は以下の通り。

```
oc label node <node-name> region=<region-name> zone=<zone-name> --overwrite
```
- 確認方法
```
oc get node <node-name> -L region -L zone

[student@workstation ~]$ oc get nodes -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
```

dcに対してラベリングの要素をつけるときには、nodeSelectorを使う。

```
まずは今のdcを出力
[student@workstation ~]$ oc get dc hello -o yaml > dc.yml

ここの部分を追加する。
[before]
spec:
    spec:
      containers:

[after]
    spec:
      nodeSelector:
        region: apps
      containers:

そのあと、その設定を今のdcに反映させる。
[student@workstation ~]$ oc apply -f dc.yml 
Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
deploymentconfig "hello" configured

```

### スケジューリングによるノードへの配置コントロール

メンテナンスするときとか。
配置変更は、Nodeをscheduling対象からはずす。
その後、oc adm drain <node-name> などで、生きてるPODを殺す。

```
$ oc adm manage-node --schedulable=falce <node-name>
これで以降の配置の対象から外れる。

$ oc get node
これでそのノードが割り当て対象になっているかどうかがわかる。

正常時
[student@workstation ~]$ oc get nodes -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra

メンテ時
[student@workstation ~]$ oc get nodes
NAME                     STATUS                     ROLES     AGE       VERSION
master.lab.example.com   Ready                      master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready                      compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready,SchedulingDisabled   compute   1d        v1.9.1+a0ce1bc657

$ oc adm drain <node-name>
これでそのノードにいるPODとかを全部殺す。
で、メンテする。

メンテ終わったら戻す。
$ oc adm manage-node --schedulable=true <node-name>


```
