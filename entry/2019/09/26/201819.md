---
title: DO280-DAY3
date: 2019-09-26T11:18:19.000Z
id: "26006613440812889"
draft: true
---
# CHAPTER 7 MANAGING APPLICATION DEPLOYMENTS(P227)

Replication Controller
K8Sのしくみ。PODの数だけを維持する。
PODの中身がどうなってるかとかは気にしない。数だけ気にしてる感じ。監視は別。

RCは、DCが持っているテンプレート情報からPODの数を確認し、利用する。
PODの識別はラベルによる識別。特定のラベルがついたPODがいくついるかを確認している。
足りなくなったら新たなPODを立ててくれたりする。

RCはScalingなどはない。
horizon autoscalerとか別のものでscalingが可能だが、手動によるscalingはできなくなる。自動のみ。

最初に起動するレプリカ数は
DeployConfig内のspec: replicas: というパラメータできまる。

selectorのdeploymentconfig とついているのが監視するラベル名。

```
[student@workstation ~]$ oc get dc
NAME               REVISION   DESIRED   CURRENT   TRIGGERED BY
docker-registry    1          2         2         config
registry-console   1          1         1         config
router             1          2         2         config

```

scaleするときはoc scale dc/<pod-name> --replicas=2 とか。あとで確認しよう。

いまはRC変更しても意味ないのでDCで調整する。

autoscaleはHorizonalPodAutoscaler(hpa)

oc autoscale dc/appname --min 1 --max 10 --cpu-percent=80

状態確認

oc get hpa/frontend

[student@workstation ~]$ oc get hpa
No resources found.


## Guide Exercise: Scaling An Application (P232)

1.
```
[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".
[student@workstation ~]$ oc whoami
developer
[student@workstation ~]$ oc new-project scaling
Now using project "scaling" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation ~]$ 

```

2.
```
[student@workstation ~]$ oc new-app -o yaml -i php:7.0 http://registry.lab.example.com/scaling > ~/scaling.yml
[student@workstation ~]$ vi scaling.yml 
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ cat scaling.yml 
apiVersion: v1
items:
- apiVersion: v1
  kind: ImageStream
  metadata:
    annotations:
      openshift.io/generated-by: OpenShiftNewApp
    creationTimestamp: null
    labels:
      app: scaling
    name: scaling
  spec:
    lookupPolicy:
      local: false
  status:
    dockerImageRepository: ""
- apiVersion: v1
  kind: BuildConfig
  metadata:
    annotations:
      openshift.io/generated-by: OpenShiftNewApp
    creationTimestamp: null
    labels:
      app: scaling
    name: scaling
  spec:
    nodeSelector: null
    output:
      to:
        kind: ImageStreamTag
        name: scaling:latest
    postCommit: {}
    resources: {}
    source:
      git:
        uri: http://registry.lab.example.com/scaling
      type: Git
    strategy:
      sourceStrategy:
        from:
          kind: ImageStreamTag
          name: php:7.0
          namespace: openshift
      type: Source
    triggers:
    - github:
        secret: TkLwnOisVUk9B7fXn8rb
      type: GitHub
    - generic:
        secret: TBPD5HrMT10ebTugHqly
      type: Generic
    - type: ConfigChange
    - imageChange: {}
      type: ImageChange
  status:
    lastVersion: 0
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      openshift.io/generated-by: OpenShiftNewApp
    creationTimestamp: null
    labels:
      app: scaling
    name: scaling
  spec:
    replicas: 1
    selector:
      app: scaling
      deploymentconfig: scaling
    strategy:
      resources: {}
    template:
      metadata:
        annotations:
          openshift.io/generated-by: OpenShiftNewApp
        creationTimestamp: null
        labels:
          app: scaling
          deploymentconfig: scaling
      spec:
        containers:
        - image: scaling:latest
          name: scaling
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
    test: false
    triggers:
    - type: ConfigChange
    - imageChangeParams:
        automatic: true
        containerNames:
        - scaling
        from:
          kind: ImageStreamTag
          name: scaling:latest
      type: ImageChange
  status:
    availableReplicas: 0
    latestVersion: 0
    observedGeneration: 0
    replicas: 0
    unavailableReplicas: 0
    updatedReplicas: 0
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      openshift.io/generated-by: OpenShiftNewApp
    creationTimestamp: null
    labels:
      app: scaling
    name: scaling
  spec:
    ports:
    - name: 8080-tcp
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: scaling
      deploymentconfig: scaling
  status:
    loadBalancer: {}
kind: List
metadata: {}
[student@workstation ~]$ 

[student@workstation ~]$ vi scaling.yml 
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc create -f scaling.yml 
imagestream "scaling" created
buildconfig "scaling" created
deploymentconfig "scaling" created
service "scaling" created

[student@workstation ~]$ oc get pods
NAME              READY     STATUS    RESTARTS   AGE
scaling-1-build   1/1       Running   0          18s
[student@workstation ~]$ watch -n 3 oc get builds
[student@workstation ~]$ oc get dc/scaling
NAME      REVISION   DESIRED   CURRENT   TRIGGERED BY
scaling   1          3         3         config,image(scaling:latest)
[student@workstation ~]$ watch -n 3 oc get builds
[student@workstation ~]$ oc get pods
NAME              READY     STATUS      RESTARTS   AGE
scaling-1-6dld9   1/1       Running     0          50s
scaling-1-bn4th   1/1       Running     0          50s
scaling-1-build   0/1       Completed   0          1m
scaling-1-k6csj   1/1       Running     0          50s


```


3. 


```
[student@workstation ~]$ oc expose service scaling --hostname=scaling.apps.lab.example.com
route "scaling" exposed
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc get svc
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
scaling   ClusterIP   172.30.65.169   <none>        8080/TCP   2m
[student@workstation ~]$ oc get svc -o wide
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
scaling   ClusterIP   172.30.65.169   <none>        8080/TCP   2m        app=scaling,deploymentconfig=scaling


```

4.
```

student@workstation ~]$ oc get pods -o wide
NAME              READY     STATUS      RESTARTS   AGE       IP             NODE
scaling-1-6dld9   1/1       Running     0          3m        10.128.0.62    node1.lab.example.com
scaling-1-bn4th   1/1       Running     0          3m        10.129.0.118   node2.lab.example.com
scaling-1-build   0/1       Completed   0          3m        10.129.0.116   node2.lab.example.com
scaling-1-k6csj   1/1       Running     0          3m        10.129.0.119   node2.lab.example.com



```

5.

```
[student@workstation ~]$ for i in {1..5} ; do curl -s \
> http://scaling.apps.lab.example.com | grep IP ; done
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.129.0.118 

```

6.

```
[student@workstation ~]$ oc describe dc scaling 
Name:		scaling
Namespace:	scaling
Created:	5 minutes ago
Labels:		app=scaling
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	1
Selector:	app=scaling,deploymentconfig=scaling
Replicas:	3
Triggers:	Config, Image(scaling@latest, auto=true)
Strategy:	Rolling
Template:
Pod Template:
  Labels:	app=scaling
		deploymentconfig=scaling
  Annotations:	openshift.io/generated-by=OpenShiftNewApp
  Containers:
   scaling:
    Image:		docker-registry.default.svc:5000/scaling/scaling@sha256:016803b996b721d956f3a4d38243644703e22a7561210c915959841ac1d3779d
    Port:		8080/TCP
    Environment:	<none>
    Mounts:		<none>
  Volumes:		<none>

Deployment #1 (latest):
	Name:		scaling-1
	Created:	5 minutes ago
	Status:		Complete
	Replicas:	3 current / 3 desired
	Selector:	app=scaling,deployment=scaling-1,deploymentconfig=scaling
	Labels:		app=scaling,openshift.io/deployment-config.name=scaling
	Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed

Events:
  Type		Reason			Age	From				Message
  ----		------			----	----				-------
  Normal	DeploymentCreated	5m	deploymentconfig-controller	Created new replication controller "scaling-1" for version 1


[student@workstation ~]$ oc describe dc scaling | grep Replicas
Replicas:	3
	Replicas:	3 current / 3 desired
[student@workstation ~]$ 


[student@workstation ~]$ oc scale --replicas=5 dc scaling
deploymentconfig "scaling" scaled
[student@workstation ~]$ oc describe dc scaling | grep Replicas
Replicas:	5
	Replicas:	5 current / 5 desired

[student@workstation ~]$ oc get pods -o wide
NAME              READY     STATUS      RESTARTS   AGE       IP             NODE
scaling-1-6dld9   1/1       Running     0          6m        10.128.0.62    node1.lab.example.com
scaling-1-bn4th   1/1       Running     0          6m        10.129.0.118   node2.lab.example.com
scaling-1-build   0/1       Completed   0          6m        10.129.0.116   node2.lab.example.com
scaling-1-hfchs   1/1       Running     0          33s       10.129.0.120   node2.lab.example.com
scaling-1-k6csj   1/1       Running     0          6m        10.129.0.119   node2.lab.example.com
scaling-1-tfcmf   1/1       Running     0          33s       10.128.0.63    node1.lab.example.com


[student@workstation ~]$ for i in {1..5} ; do curl -s http://scaling.apps.lab.example.com | grep IP ; done
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.128.0.63 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.129.0.120 
[student@workstation ~]$ for i in {1..5} ; do curl -s http://scaling.apps.lab.example.com | grep IP ; done
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.128.0.63 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.129.0.120 
[student@workstation ~]$ for i in {1..5} ; do curl -s http://scaling.apps.lab.example.com | grep IP ; done
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.128.0.63 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.129.0.120 
[student@workstation ~]$ for i in {1..10} ; do curl -s http://scaling.apps.lab.example.com | grep IP ; done
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.128.0.63 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.129.0.120 
 <br/> Server IP: 10.128.0.62 
 <br/> Server IP: 10.128.0.63 
 <br/> Server IP: 10.129.0.118 
 <br/> Server IP: 10.129.0.119 
 <br/> Server IP: 10.129.0.120 

```

7. 

```
[student@workstation ~]$ oc delete project scaling
project "scaling" deleted
[student@workstation ~]$ 
[student@workstation ~]$ oc get project 
NAME       DISPLAY NAME   STATUS
hogehoge   hoge           Active
scaling                   Terminating
[student@workstation ~]$ oc project hogehoge
Now using project "hogehoge" on server "https://master.lab.example.com:443".
[student@workstation ~]$ oc project hogehoge
Already on project "hogehoge" on server "https://master.lab.example.com:443".
[student@workstation ~]$ oc get project 
NAME       DISPLAY NAME   STATUS
hogehoge   hoge           Active
[student@workstation ~]$ 

```

## Controlling Pod Scheduling(240)

Labelでの判別処理の話。
インストール時に値を入れている。
node-role.kubanetes.io/compute=true 


affinity POD内のコンテナをできるだけ近接に配置する
anti-affinity POD内のコンテナをできるだけ同じNodeに固めない

以下はあくまでラベル。
region システムの分離単位。地域で分けなくても、会社でわけてもいい。
zone 同じ共有リソース（ラックとかスイッチとか電源とか）を使っている単位

ただし、PODは基本はregion はまたがない。ただし、ZONEは可能な限り跨ぐように配置される。

```
oc label node <node-name> region=<region-name> zone=<zone-name> --overwrite
```
確認方法
```
oc get node <node-name> -L region -L zone
```

メンテナンスするときとか。
配置変更は、Nodeをscheduling対象からはずす。
その後、oc adm drain <node-name> などで、生きてるPODを殺す。


ノードのラベルの確認方法は？
これか。
```
[student@workstation ~]$ oc describe node
Name:               master.lab.example.com
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=master.lab.example.com
                    node-role.kubernetes.io/master=true
                    openshift-infra=apiserver
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Tue, 24 Sep 2019 14:13:08 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Thu, 26 Sep 2019 10:24:03 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Thu, 26 Sep 2019 10:24:03 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 26 Sep 2019 10:24:03 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Thu, 26 Sep 2019 10:24:03 +0900   Tue, 24 Sep 2019 14:13:50 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.250.10
  Hostname:    master.lab.example.com
Capacity:
 cpu:     2
 memory:  1882608Ki
 pods:    20
Allocatable:
 cpu:     2
 memory:  1780208Ki
 pods:    20
System Info:
 Machine ID:                 a1fb71323fcb41eda3f879506ed98dba
 System UUID:                A4A7FBC4-2706-41B1-B33F-656D9E2DB6F5
 Boot ID:                    a4d39d2b-05dd-40ed-aaa8-ad221ac27d52
 Kernel Version:             3.10.0-862.el7.x86_64
 OS Image:                   Red Hat Enterprise Linux Server 7.5 (Maipo)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://1.13.1
 Kubelet Version:            v1.9.1+a0ce1bc657
 Kube-Proxy Version:         v1.9.1+a0ce1bc657
ExternalID:                  master.lab.example.com
Non-terminated Pods:         (3 in total)
  Namespace                  Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                           ------------  ----------  ---------------  -------------
  kube-service-catalog       apiserver-5gn6t                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-service-catalog       controller-manager-b4zff       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-web-console      webconsole-579cc76bb6-584f7    100m (5%)     0 (0%)      100Mi (5%)       0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  100m (5%)     0 (0%)      100Mi (5%)       0 (0%)
Events:         <none>


Name:               node1.lab.example.com
Roles:              compute
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=node1.lab.example.com
                    node-role.kubernetes.io/compute=true
                    region=infra
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Tue, 24 Sep 2019 14:13:08 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Thu, 26 Sep 2019 10:24:08 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Thu, 26 Sep 2019 10:24:08 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 26 Sep 2019 10:24:08 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Thu, 26 Sep 2019 10:24:08 +0900   Wed, 25 Sep 2019 13:45:19 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.250.11
  Hostname:    node1.lab.example.com
Capacity:
 cpu:     2
 memory:  8009700Ki
 pods:    20
Allocatable:
 cpu:     2
 memory:  7907300Ki
 pods:    20
System Info:
 Machine ID:                         a1fb71323fcb41eda3f879506ed98dba
 System UUID:                        49513816-4254-449C-9991-7CFC78540365
 Boot ID:                            2eb4056a-5214-4661-a7d9-f5f2077b1061
 Kernel Version:                     3.10.0-862.el7.x86_64
 OS Image:                           Red Hat Enterprise Linux Server 7.5 (Maipo)
 Operating System:                   linux
 Architecture:                       amd64
 Container Runtime Version:          docker://1.13.1
 Kubelet Version:                    v1.9.1+a0ce1bc657
 Kube-Proxy Version:                 v1.9.1+a0ce1bc657
ExternalID:                          node1.lab.example.com
Non-terminated Pods:                 (7 in total)
  Namespace                          Name                        CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                          ----                        ------------  ----------  ---------------  -------------
  default                            docker-registry-1-ktnz7     100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  default                            docker-registry-1-sl8rx     100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  default                            registry-console-1-lfk8w    0 (0%)        0 (0%)      0 (0%)           0 (0%)
  default                            router-1-hv56n              100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  openshift-ansible-service-broker   asb-1-28fcj                 0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-ansible-service-broker   asb-etcd-1-w9dqs            0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-template-service-broker  apiserver-t74f2             0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  300m (15%)    0 (0%)      768Mi (9%)       0 (0%)
Events:         <none>


Name:               node2.lab.example.com
Roles:              compute
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=node2.lab.example.com
                    node-role.kubernetes.io/compute=true
                    region=infra
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Tue, 24 Sep 2019 14:13:08 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Thu, 26 Sep 2019 10:24:01 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Thu, 26 Sep 2019 10:24:01 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 26 Sep 2019 10:24:01 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Thu, 26 Sep 2019 10:24:01 +0900   Wed, 25 Sep 2019 13:45:48 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.250.12
  Hostname:    node2.lab.example.com
Capacity:
 cpu:     2
 memory:  8009696Ki
 pods:    20
Allocatable:
 cpu:     2
 memory:  7907296Ki
 pods:    20
System Info:
 Machine ID:                         a1fb71323fcb41eda3f879506ed98dba
 System UUID:                        08D3AE73-D34C-4302-A413-190C915B0A78
 Boot ID:                            bcae51bd-c485-499f-a05c-89ec351acf77
 Kernel Version:                     3.10.0-862.el7.x86_64
 OS Image:                           Red Hat Enterprise Linux Server 7.5 (Maipo)
 Operating System:                   linux
 Architecture:                       amd64
 Container Runtime Version:          docker://1.13.1
 Kubelet Version:                    v1.9.1+a0ce1bc657
 Kube-Proxy Version:                 v1.9.1+a0ce1bc657
ExternalID:                          node2.lab.example.com
Non-terminated Pods:                 (4 in total)
  Namespace                          Name                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                          ----                  ------------  ----------  ---------------  -------------
  default                            router-1-g5vnp        100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  openshift-template-service-broker  apiserver-p6krn       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  secure-review                      mysql-1-p9799         0 (0%)        0 (0%)      512Mi (6%)       512Mi (6%)
  secure-review                      phpmyadmin-2-5lsgl    0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  100m (5%)     0 (0%)      768Mi (9%)       512Mi (6%)
Events:         <none>
[student@workstation ~]$ 
```


-----------------------------
## Guide Excerise: Contrilling Pod Scheduling(P245)

- pre
```
[student@workstation ~]$ lab schedule-control setup

Checking prerequisites for GE: Controlling Pod Scheduling

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Overall setup status...........................................  SUCCESS


```

1.

```
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
  * hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review

Using project "hogehoge".
[student@workstation ~]$ oc whoami
admin
[student@workstation ~]$ 
[student@workstation ~]$ oc get nodes -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
[student@workstation ~]$ 
[student@workstation ~]$ oc new-project schedule-control
Now using project "schedule-control" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


[student@workstation ~]$ oc new-app --name=hello --docker-image=registry.lab.example.com/openshift/hello-openshift
--> Found Docker image 7af3297 (17 months old) from registry.lab.example.com for "registry.lab.example.com/openshift/hello-openshift"

    * An image stream will be created as "hello:latest" that will track this image
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc get pods
NAME             READY     STATUS              RESTARTS   AGE
hello-1-deploy   1/1       Running             0          4s
hello-1-wm989    0/1       ContainerCreating   0          2s
[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-wm989   1/1       Running   0          6s
[student@workstation ~]$ 


[student@workstation ~]$ oc scale dc hello --replicas=5
deploymentconfig "hello" scaled
[student@workstation ~]$ oc get pods
NAME            READY     STATUS              RESTARTS   AGE
hello-1-55rg9   1/1       Running             0          3s
hello-1-drr5h   1/1       Running             0          3s
hello-1-f789j   0/1       ContainerCreating   0          3s
hello-1-nln2d   1/1       Running             0          3s
hello-1-wm989   1/1       Running             0          42s
[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-55rg9   1/1       Running   0          5s
hello-1-drr5h   1/1       Running   0          5s
hello-1-f789j   1/1       Running   0          5s
hello-1-nln2d   1/1       Running   0          5s
hello-1-wm989   1/1       Running   0          44s


```

2. 
```
[student@workstation ~]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps
[student@workstation ~]$ oc get node -L region -L zone
NAME                     STATUS    ROLES     AGE       VERSION             REGION    ZONE
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657             
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra     
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps      

```

3. 

```
[student@workstation ~]$ oc get dc hello -o yaml > dc.yml
[student@workstation ~]$ cat dc.yml
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-26T01:29:11Z
  generation: 3
  labels:
    app: hello
  name: hello
  namespace: schedule-control
  resourceVersion: "337468"
  selfLink: /apis/apps.openshift.io/v1/namespaces/schedule-control/deploymentconfigs/hello
  uid: 0abc3921-dffd-11e9-98ef-52540000fa0a
spec:
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    app: hello
    deploymentconfig: hello
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: hello
        deploymentconfig: hello
    spec:
      containers:
      - image: registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
        imagePullPolicy: Always
        name: hello
        ports:
        - containerPort: 8080
          protocol: TCP
        - containerPort: 8888
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - hello
      from:
        kind: ImageStreamTag
        name: hello:latest
        namespace: schedule-control
      lastTriggeredImage: registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    type: ImageChange
status:
  availableReplicas: 5
  conditions:
  - lastTransitionTime: 2019-09-26T01:29:13Z
    lastUpdateTime: 2019-09-26T01:29:17Z
    message: replication controller "hello-1" successfully rolled out
    reason: NewReplicationControllerAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: 2019-09-26T01:29:55Z
    lastUpdateTime: 2019-09-26T01:29:55Z
    message: Deployment config has minimum availability.
    status: "True"
    type: Available
  details:
    causes:
    - type: ConfigChange
    message: config change
  latestVersion: 1
  observedGeneration: 3
  readyReplicas: 5
  replicas: 5
  unavailableReplicas: 0
  updatedReplicas: 5


此処の部分を変えた。

spec:
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    app: hello
    deploymentconfig: hello
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: hello
        deploymentconfig: hello
    spec:
      containers:

ーーー

    spec:
      nodeSelector:
        region: apps
      containers:


[student@workstation ~]$ vi dc.yml 
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc apply -f dc.yml 
Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
deploymentconfig "hello" configured

tudent@workstation ~]$ oc get pod -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-2-2kdx2   1/1       Running   0          1m        10.129.0.126   node2.lab.example.com
hello-2-8nqw8   1/1       Running   0          1m        10.129.0.128   node2.lab.example.com
hello-2-8rrtp   1/1       Running   0          1m        10.129.0.129   node2.lab.example.com
hello-2-cr6kf   1/1       Running   0          1m        10.129.0.127   node2.lab.example.com
hello-2-fn87l   1/1       Running   0          1m        10.129.0.130   node2.lab.example.com


分散配置されてたものが、NODE2（REGION指定されたもの）に変わった｡\
```


4. 

```
[student@workstation ~]$ oc label node node1.lab.example.com region=apps --overwrite=true
node "node1.lab.example.com" labeled
[student@workstation ~]$ oc get nodes -o wide
NAME                     STATUS    ROLES     AGE       VERSION             EXTERNAL-IP   OS-IMAGE                                      KERNEL-VERSION          CONTAINER-RUNTIME
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   <none>        Red Hat Enterprise Linux Server 7.5 (Maipo)   3.10.0-862.el7.x86_64   docker://1.13.1
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   <none>        Red Hat Enterprise Linux Server 7.5 (Maipo)   3.10.0-862.el7.x86_64   docker://1.13.1
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   <none>        Red Hat Enterprise Linux Server 7.5 (Maipo)   3.10.0-862.el7.x86_64   docker://1.13.1
[student@workstation ~]$ oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
[student@workstation ~]$ oc get nodes -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps

```

5. 

```
[student@workstation ~]$ oc adm manage-node --schedulable=false node2.lab.example.com
NAME                    STATUS                     ROLES     AGE       VERSION
node2.lab.example.com   Ready,SchedulingDisabled   compute   1d        v1.9.1+a0ce1bc657

STATUSがSchedulingDisabledにかわった。

[student@workstation ~]$ oc adm drain node2.lab.example.com --delete-local-data
node "node2.lab.example.com" already cordoned
WARNING: Deleting pods with local storage: mysql-1-p9799
pod "hello-2-cr6kf" evicted
pod "hello-2-8nqw8" evicted
pod "router-1-g5vnp" evicted
pod "hello-2-8rrtp" evicted
pod "phpmyadmin-2-5lsgl" evicted
pod "mysql-1-p9799" evicted
pod "hello-2-2kdx2" evicted
pod "hello-2-fn87l" evicted
node "node2.lab.example.com" drained

[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP            NODE
hello-2-66gb4   1/1       Running   0          40s       10.128.0.69   node1.lab.example.com
hello-2-7q5sw   1/1       Running   0          40s       10.128.0.67   node1.lab.example.com
hello-2-d5n8g   1/1       Running   0          40s       10.128.0.71   node1.lab.example.com
hello-2-jvzmg   1/1       Running   0          40s       10.128.0.68   node1.lab.example.com
hello-2-ljc77   1/1       Running   0          40s       10.128.0.70   node1.lab.example.com

```

6. 

```
[student@workstation ~]$ oc adm manage-node --schedulable=true node2.lab.example.com
NAME                    STATUS    ROLES     AGE       VERSION
node2.lab.example.com   Ready     compute   1d        v1.9.1+a0ce1bc657
[student@workstation ~]$ oc get node
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
[student@workstation ~]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps

[student@workstation ~]$ oc label node node1.lab.example.com region=infra --overwrite=true
node "node1.lab.example.com" labeled
[student@workstation ~]$ 
[student@workstation ~]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   apps
[student@workstation ~]$ oc label node node2.lab.example.com region=infra --overwrite=true
node "node2.lab.example.com" labeled
[student@workstation ~]$ 
[student@workstation ~]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   infra


[student@workstation ~]$ oc delete project schedule-control
project "schedule-control" deleted

```
プラスα。
```

この状態でNODE1のPODころすとどうなるか？

[student@workstation ~]$ oc adm drain node1.lab.example.com
node "node1.lab.example.com" cordoned
error: unable to drain node "node1.lab.example.com", aborting command...

There are pending nodes to be drained:
 node1.lab.example.com
error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): apiserver-q8t9z; pods with local storage (use --delete-local-data to override): mysql-1-rhkpn

オプションつけると？

[student@workstation ~]$ oc adm drain node1.lab.example.com --delete-local-data
node "node1.lab.example.com" already cordoned
error: unable to drain node "node1.lab.example.com", aborting command...

There are pending nodes to be drained:
 node1.lab.example.com
error: DaemonSet-managed pods (use --ignore-daemonsets to ignore): apiserver-q8t9z

ふやせばいいか

[student@workstation ~]$ oc scale dc hello --replicas=6
deploymentconfig "hello" scaled
[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP            NODE
hello-2-66gb4   1/1       Running   0          6m        10.128.0.69   node1.lab.example.com
hello-2-7q5sw   1/1       Running   0          6m        10.128.0.67   node1.lab.example.com
hello-2-chjbp   0/1       Pending   0          3s        <none>        <none>
hello-2-d5n8g   1/1       Running   0          6m        10.128.0.71   node1.lab.example.com
hello-2-jvzmg   1/1       Running   0          6m        10.128.0.68   node1.lab.example.com
hello-2-ljc77   1/1       Running   0          6m        10.128.0.70   node1.lab.example.com
[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP            NODE
hello-2-66gb4   1/1       Running   0          6m        10.128.0.69   node1.lab.example.com
hello-2-7q5sw   1/1       Running   0          6m        10.128.0.67   node1.lab.example.com
hello-2-chjbp   0/1       Pending   0          6s        <none>        <none>
hello-2-d5n8g   1/1       Running   0          6m        10.128.0.71   node1.lab.example.com
hello-2-jvzmg   1/1       Running   0          6m        10.128.0.68   node1.lab.example.com
hello-2-ljc77   1/1       Running   0          6m        10.128.0.70   node1.lab.example.com


お、Pendingになった。
TroubleShooting!!

[student@workstation ~]$ oc get event
LAST SEEN   FIRST SEEN   COUNT     NAME                              KIND                    SUBOBJECT                     TYPE      REASON                        SOURCE                           MESSAGE
17m         17m          1         hello-1-55rg9.15c7d8d229e28a14    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-55rg9 to node1.lab.example.com
17m         17m          1         hello-1-55rg9.15c7d8d234f2fb21    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
17m         17m          1         hello-1-55rg9.15c7d8d2ab63bb4d    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-55rg9.15c7d8d2ac0b87ba    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-55rg9.15c7d8d2aeebce28    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
17m         17m          1         hello-1-55rg9.15c7d8d2b4244217    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
13m         13m          1         hello-1-55rg9.15c7d90a5ca966f3    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node1.lab.example.com   Killing container with id docker://hello:Need to kill Pod
18m         18m          1         hello-1-deploy.15c7d8c898e6c556   Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-deploy to node2.lab.example.com
18m         18m          1         hello-1-deploy.15c7d8c8a309804e   Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-4tcrn" 
18m         18m          1         hello-1-deploy.15c7d8c9016152d5   Pod                     spec.containers{deployment}   Normal    Pulled                        kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
18m         18m          1         hello-1-deploy.15c7d8c90339a686   Pod                     spec.containers{deployment}   Normal    Created                       kubelet, node2.lab.example.com   Created container
18m         18m          1         hello-1-deploy.15c7d8c908efe877   Pod                     spec.containers{deployment}   Normal    Started                       kubelet, node2.lab.example.com   Started container
18m         18m          1         hello-1-deploy.15c7d8ca00410e4e   Pod                     spec.containers{deployment}   Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod
17m         17m          1         hello-1-drr5h.15c7d8d22bcd6f63    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-drr5h to node2.lab.example.com
17m         17m          1         hello-1-drr5h.15c7d8d23a4f554c    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
17m         17m          1         hello-1-drr5h.15c7d8d298e17842    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-drr5h.15c7d8d299795450    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-drr5h.15c7d8d29b5666d4    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
17m         17m          1         hello-1-drr5h.15c7d8d2a0877ecf    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
13m         13m          1         hello-1-drr5h.15c7d90ad273adc4    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
17m         17m          1         hello-1-f789j.15c7d8d22a88fad6    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-f789j to node2.lab.example.com
17m         17m          1         hello-1-f789j.15c7d8d23a3e9e5f    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
17m         17m          1         hello-1-f789j.15c7d8d2b0a3177c    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-f789j.15c7d8d2b166823d    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-f789j.15c7d8d2b313b538    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
17m         17m          1         hello-1-f789j.15c7d8d2b855c4cd    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
13m         13m          1         hello-1-f789j.15c7d909e91bf8e0    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
17m         17m          1         hello-1-nln2d.15c7d8d22b37273c    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-nln2d to node1.lab.example.com
17m         17m          1         hello-1-nln2d.15c7d8d234fd88c5    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
17m         17m          1         hello-1-nln2d.15c7d8d299cbe74e    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-nln2d.15c7d8d29a8c0c6c    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
17m         17m          1         hello-1-nln2d.15c7d8d29d454a01    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
17m         17m          1         hello-1-nln2d.15c7d8d2a25b2f98    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
13m         13m          1         hello-1-nln2d.15c7d90a5c0a4455    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node1.lab.example.com   Killing container with id docker://hello:Need to kill Pod
18m         18m          1         hello-1-wm989.15c7d8c9127a22d3    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-1-wm989 to node2.lab.example.com
18m         18m          1         hello-1-wm989.15c7d8c91a8b5a12    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
18m         18m          1         hello-1-wm989.15c7d8c99f44f21b    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
18m         18m          1         hello-1-wm989.15c7d8c99fe8bcc2    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
18m         18m          1         hello-1-wm989.15c7d8c9a24ef012    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
18m         18m          1         hello-1-wm989.15c7d8c9a7854984    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
13m         13m          1         hello-1-wm989.15c7d90b198b3b1c    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
18m         18m          1         hello-1.15c7d8c912354e5f          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-1-wm989
17m         17m          1         hello-1.15c7d8d2299a5e53          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-1-55rg9
17m         17m          1         hello-1.15c7d8d22a1c6c4d          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-1-f789j
17m         17m          1         hello-1.15c7d8d22a823393          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-1-nln2d
17m         17m          1         hello-1.15c7d8d22b2e3688          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-1-drr5h
13m         13m          1         hello-1.15c7d909e4acc907          ReplicationController                                 Normal    SuccessfulDelete              replication-controller           Deleted pod: hello-1-f789j
13m         13m          1         hello-1.15c7d90a5864e6e8          ReplicationController                                 Normal    SuccessfulDelete              replication-controller           Deleted pod: hello-1-nln2d
13m         13m          1         hello-1.15c7d90a5870f414          ReplicationController                                 Normal    SuccessfulDelete              replication-controller           Deleted pod: hello-1-55rg9
13m         13m          1         hello-1.15c7d90acf9d1602          ReplicationController                                 Normal    SuccessfulDelete              replication-controller           Deleted pod: hello-1-drr5h
13m         13m          1         hello-1.15c7d90b16d93c6e          ReplicationController                                 Normal    SuccessfulDelete              replication-controller           Deleted pod: hello-1-wm989
13m         13m          1         hello-2-2kdx2.15c7d909a7369dc8    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-2kdx2 to node2.lab.example.com
13m         13m          1         hello-2-2kdx2.15c7d909b11569d8    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
13m         13m          1         hello-2-2kdx2.15c7d90a094b8809    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-2kdx2.15c7d90a0a038638    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-2kdx2.15c7d90a0b823651    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-2kdx2.15c7d90a105b11ab    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
8m          8m           1         hello-2-2kdx2.15c7d9569ec94834    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
8m          8m           1         hello-2-66gb4.15c7d956a199672c    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-66gb4 to node1.lab.example.com
8m          8m           1         hello-2-66gb4.15c7d956cc1801e2    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
8m          8m           1         hello-2-66gb4.15c7d957709cce39    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-66gb4.15c7d95892e6cc36    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-66gb4.15c7d958973df3f4    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
8m          8m           1         hello-2-66gb4.15c7d958ab293aa6    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
8m          8m           1         hello-2-7q5sw.15c7d956a05ec2c7    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-7q5sw to node1.lab.example.com
8m          8m           1         hello-2-7q5sw.15c7d956cc3d176b    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
8m          8m           1         hello-2-7q5sw.15c7d9574ce3be79    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-7q5sw.15c7d9574e2382e7    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-7q5sw.15c7d9575299015e    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
8m          8m           1         hello-2-7q5sw.15c7d9575d5391d0    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
13m         13m          1         hello-2-8nqw8.15c7d90a04a6fd3d    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-8nqw8 to node2.lab.example.com
13m         13m          1         hello-2-8nqw8.15c7d90a119898b2    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
13m         13m          1         hello-2-8nqw8.15c7d90a85f03985    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-8nqw8.15c7d90a86a3de7e    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-8nqw8.15c7d90a88456713    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-8nqw8.15c7d90a8e655c95    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
8m          8m           1         hello-2-8nqw8.15c7d9569da14f17    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
13m         13m          1         hello-2-8rrtp.15c7d90a7ba2d6b0    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-8rrtp to node2.lab.example.com
13m         13m          1         hello-2-8rrtp.15c7d90a894662cc    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
13m         13m          1         hello-2-8rrtp.15c7d90ae068aa93    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-8rrtp.15c7d90ae129c4d1    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-8rrtp.15c7d90ae29828b9    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-8rrtp.15c7d90ae768103c    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
8m          8m           1         hello-2-8rrtp.15c7d956a14cff6b    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
1s          2m           10        hello-2-chjbp.15c7d9adb442dc48    Pod                                                   Warning   FailedScheduling              default-scheduler                0/3 nodes are available: 1 NodeUnschedulable, 3 CheckServiceAffinity, 3 MatchNodeSelector.
13m         13m          1         hello-2-cr6kf.15c7d909a77192f8    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-cr6kf to node2.lab.example.com
13m         13m          1         hello-2-cr6kf.15c7d909b15b07b4    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
13m         13m          1         hello-2-cr6kf.15c7d90a24a329d4    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-cr6kf.15c7d90a254eb5f7    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-cr6kf.15c7d90a2782a056    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-cr6kf.15c7d90a2c28455e    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
8m          8m           1         hello-2-cr6kf.15c7d9569fa4a44c    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
8m          8m           1         hello-2-d5n8g.15c7d9569fc3dd43    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-d5n8g to node1.lab.example.com
8m          8m           1         hello-2-d5n8g.15c7d956cc31ea40    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
8m          8m           1         hello-2-d5n8g.15c7d9577c377750    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-d5n8g.15c7d9589f1d13f3    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-d5n8g.15c7d958a61eaf80    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
8m          8m           1         hello-2-d5n8g.15c7d958aff455c1    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
13m         13m          1         hello-2-deploy.15c7d908f733019d   Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-deploy to node2.lab.example.com
13m         13m          1         hello-2-deploy.15c7d90909c562a5   Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-4tcrn" 
13m         13m          1         hello-2-deploy.15c7d90959993244   Pod                     spec.containers{deployment}   Normal    Pulled                        kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
13m         13m          1         hello-2-deploy.15c7d9095bd72f2f   Pod                     spec.containers{deployment}   Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-deploy.15c7d909618df6bd   Pod                     spec.containers{deployment}   Normal    Started                       kubelet, node2.lab.example.com   Started container
13m         13m          1         hello-2-deploy.15c7d90bc6450ceb   Pod                     spec.containers{deployment}   Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod
13m         13m          1         hello-2-fn87l.15c7d90a7c555d65    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-fn87l to node2.lab.example.com
13m         13m          1         hello-2-fn87l.15c7d90a8a09efe8    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
13m         13m          1         hello-2-fn87l.15c7d90aec374bf0    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-fn87l.15c7d90aecd43e13    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
13m         13m          1         hello-2-fn87l.15c7d90aeed7208d    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node2.lab.example.com   Created container
13m         13m          1         hello-2-fn87l.15c7d90af38fe6ab    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node2.lab.example.com   Started container
8m          8m           1         hello-2-fn87l.15c7d956a1600b3f    Pod                     spec.containers{hello}        Normal    Killing                       kubelet, node2.lab.example.com   Killing container with id docker://hello:Need to kill Pod
8m          8m           1         hello-2-jvzmg.15c7d956a0ba7070    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-jvzmg to node1.lab.example.com
8m          8m           1         hello-2-jvzmg.15c7d956cc253e12    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
8m          8m           1         hello-2-jvzmg.15c7d9577ba1acac    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-jvzmg.15c7d9589b66afcf    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-jvzmg.15c7d958a137fc3a    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
8m          8m           1         hello-2-jvzmg.15c7d958acde2a80    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
8m          8m           1         hello-2-ljc77.15c7d9569b12fd2c    Pod                                                   Normal    Scheduled                     default-scheduler                Successfully assigned hello-2-ljc77 to node1.lab.example.com
8m          8m           1         hello-2-ljc77.15c7d956a856ecd4    Pod                                                   Normal    SuccessfulMountVolume         kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-n222d" 
8m          8m           1         hello-2-ljc77.15c7d95773a55bdc    Pod                     spec.containers{hello}        Normal    Pulling                       kubelet, node1.lab.example.com   pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-ljc77.15c7d95898d3b68c    Pod                     spec.containers{hello}        Normal    Pulled                        kubelet, node1.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
8m          8m           1         hello-2-ljc77.15c7d9589e64f8fa    Pod                     spec.containers{hello}        Normal    Created                       kubelet, node1.lab.example.com   Created container
8m          8m           1         hello-2-ljc77.15c7d958ae5ce424    Pod                     spec.containers{hello}        Normal    Started                       kubelet, node1.lab.example.com   Started container
13m         13m          1         hello-2.15c7d909a6df9cc7          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-2kdx2
13m         13m          1         hello-2.15c7d909a760423d          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-cr6kf
13m         13m          1         hello-2.15c7d90a045cb427          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-8nqw8
13m         13m          1         hello-2.15c7d90a7b6ba7a2          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-8rrtp
13m         13m          1         hello-2.15c7d90a7bec6550          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-fn87l
8m          8m           1         hello-2.15c7d9569bde2139          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-ljc77
8m          8m           1         hello-2.15c7d9569e6ce943          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-d5n8g
8m          8m           1         hello-2.15c7d9569f50470a          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-jvzmg
8m          8m           1         hello-2.15c7d9569f52f879          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           Created pod: hello-2-7q5sw
2m          8m           2         hello-2.15c7d956a4207e78          ReplicationController                                 Normal    SuccessfulCreate              replication-controller           (combined from similar events): Created pod: hello-2-chjbp
18m         18m          1         hello.15c7d8c896fd2389            DeploymentConfig                                      Normal    DeploymentCreated             deploymentconfig-controller      Created new replication controller "hello-1" for version 1
17m         17m          1         hello.15c7d8d22895825e            DeploymentConfig                                      Normal    ReplicationControllerScaled   deploymentconfig-controller      Scaled replication controller "hello-1" from 1 to 5
13m         13m          1         hello.15c7d908f57a7897            DeploymentConfig                                      Normal    DeploymentCreated             deploymentconfig-controller      Created new replication controller "hello-2" for version 2
2m          2m           1         hello.15c7d9adb2a479b5            DeploymentConfig                                      Normal    ReplicationControllerScaled   deploymentconfig-controller      Scaled replication controller "hello-2" from 5 to 6

こっちか

[student@workstation ~]$ oc describe pod hello-2-chjbp
Name:           hello-2-chjbp
Namespace:      schedule-control
Node:           <none>
Labels:         app=hello
                deployment=hello-2
                deploymentconfig=hello
Annotations:    openshift.io/deployment-config.latest-version=2
                openshift.io/deployment-config.name=hello
                openshift.io/deployment.name=hello-2
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Pending
IP:             
Controlled By:  ReplicationController/hello-2
Containers:
  hello:
    Image:        registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    Ports:        8080/TCP, 8888/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n222d (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  default-token-n222d:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-n222d
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
                 region=apps
Tolerations:     <none>
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  18s (x15 over 3m)  default-scheduler  0/3 nodes are available: 1 NodeUnschedulable, 3 CheckServiceAffinity, 3 MatchNodeSelector.



0/3 nodes are available ってでてるのが問題の原因ってかんじかしら。
Node-Selectors:  node-role.kubernetes.io/compute=true
                 region=apps
指定内容もここにかいてあるな。


[student@workstation ~]$ oc adm manage-node --schedulable=true node1.lab.example.com
NAME                    STATUS    ROLES     AGE       VERSION
node1.lab.example.com   Ready     compute   1d        v1.9.1+a0ce1bc657
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657


YAML形式とかで出したい時にはoc getを使う。oc describeだと怒られる。

[student@workstation ~]$ oc describe dc hello -o yaml
Error: unknown shorthand flag: 'o' in -o


Usage:
  oc describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE/NAME) [options]

Examples:
  # Provide details about the ruby-22-centos7 image repository
  oc describe imageRepository ruby-22-centos7
  
  # Provide details about the ruby-sample-build build configuration
  oc describe bc ruby-sample-build

Options:
      --all-namespaces=false: If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.
  -f, --filename=[]: Filename, directory, or URL to files containing the resource to describe
      --include-extended-apis=true: If true, include definitions of new APIs via calls to the API server. [default true]
      --include-uninitialized=false: If true, the kubectl command applies to uninitialized objects. If explicitly set to false, this flag overrides other flags that make the kubectl commands apply to uninitialized objects, e.g., "--all". Objects with empty metadata.initializers are regarded as initialized.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)
      --show-events=true: If true, display events related to the described object.

Use "oc options" for a list of global command-line options (applies to all commands).




[student@workstation ~]$ oc get dc hello -o yaml
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps.openshift.io/v1","kind":"DeploymentConfig","metadata":{"annotations":{"openshift.io/generated-by":"OpenShiftNewApp"},"creationTimestamp":"2019-09-26T01:29:11Z","generation":3,"labels":{"app":"hello"},"name":"hello","namespace":"schedule-control","resourceVersion":"337468","selfLink":"/apis/apps.openshift.io/v1/namespaces/schedule-control/deploymentconfigs/hello","uid":"0abc3921-dffd-11e9-98ef-52540000fa0a"},"spec":{"replicas":5,"revisionHistoryLimit":10,"selector":{"app":"hello","deploymentconfig":"hello"},"strategy":{"activeDeadlineSeconds":21600,"resources":{},"rollingParams":{"intervalSeconds":1,"maxSurge":"25%","maxUnavailable":"25%","timeoutSeconds":600,"updatePeriodSeconds":1},"type":"Rolling"},"template":{"metadata":{"annotations":{"openshift.io/generated-by":"OpenShiftNewApp"},"creationTimestamp":null,"labels":{"app":"hello","deploymentconfig":"hello"}},"spec":{"containers":[{"image":"registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e","imagePullPolicy":"Always","name":"hello","ports":[{"containerPort":8080,"protocol":"TCP"},{"containerPort":8888,"protocol":"TCP"}],"resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","nodeSelector":{"region":"apps"},"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}},"test":false,"triggers":[{"type":"ConfigChange"},{"imageChangeParams":{"automatic":true,"containerNames":["hello"],"from":{"kind":"ImageStreamTag","name":"hello:latest","namespace":"schedule-control"},"lastTriggeredImage":"registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"},"type":"ImageChange"}]},"status":{"availableReplicas":5,"conditions":[{"lastTransitionTime":"2019-09-26T01:29:13Z","lastUpdateTime":"2019-09-26T01:29:17Z","message":"replication controller \"hello-1\" successfully rolled out","reason":"NewReplicationControllerAvailable","status":"True","type":"Progressing"},{"lastTransitionTime":"2019-09-26T01:29:55Z","lastUpdateTime":"2019-09-26T01:29:55Z","message":"Deployment config has minimum availability.","status":"True","type":"Available"}],"details":{"causes":[{"type":"ConfigChange"}],"message":"config change"},"latestVersion":1,"observedGeneration":3,"readyReplicas":5,"replicas":5,"unavailableReplicas":0,"updatedReplicas":5}}
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-26T01:29:11Z
  generation: 5
  labels:
    app: hello
  name: hello
  namespace: schedule-control
  resourceVersion: "339867"
  selfLink: /apis/apps.openshift.io/v1/namespaces/schedule-control/deploymentconfigs/hello
  uid: 0abc3921-dffd-11e9-98ef-52540000fa0a
spec:
  replicas: 6
  revisionHistoryLimit: 10
  selector:
    app: hello
    deploymentconfig: hello
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: hello
        deploymentconfig: hello
    spec:
      containers:
      - image: registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
        imagePullPolicy: Always
        name: hello
        ports:
        - containerPort: 8080
          protocol: TCP
        - containerPort: 8888
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      nodeSelector:
        region: apps
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - hello
      from:
        kind: ImageStreamTag
        name: hello:latest
        namespace: schedule-control
      lastTriggeredImage: registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    type: ImageChange
status:
  availableReplicas: 5
  conditions:
  - lastTransitionTime: 2019-09-26T01:33:56Z
    lastUpdateTime: 2019-09-26T01:33:59Z
    message: replication controller "hello-2" successfully rolled out
    reason: NewReplicationControllerAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: 2019-09-26T01:39:30Z
    lastUpdateTime: 2019-09-26T01:39:30Z
    message: Deployment config has minimum availability.
    status: "True"
    type: Available
  details:
    causes:
    - type: ConfigChange
    message: config change
  latestVersion: 2
  observedGeneration: 5
  readyReplicas: 5
  replicas: 6
  unavailableReplicas: 1
  updatedReplicas: 6






もういちどかえてみた。

[student@workstation ~]$ oc apply -f dc-test.yml 
deploymentconfig "hello" configured
[student@workstation ~]$ 
[student@workstation ~]$ oc get pods
NAME             READY     STATUS              RESTARTS   AGE
hello-2-66gb4    1/1       Running             0          16m
hello-2-7q5sw    1/1       Running             0          16m
hello-2-d5n8g    1/1       Running             0          16m
hello-2-jvzmg    1/1       Running             0          16m
hello-2-ljc77    1/1       Running             0          16m
hello-3-269cs    1/1       Running             0          3s
hello-3-6fj9f    0/1       ContainerCreating   0          3s
hello-3-deploy   1/1       Running             0          6s
hello-3-pqc2m    0/1       ContainerCreating   0          1s
[student@workstation ~]$ oc get pods -o wide
NAME             READY     STATUS              RESTARTS   AGE       IP             NODE
hello-2-66gb4    0/1       Terminating         0          16m       10.128.0.69    node1.lab.example.com
hello-2-7q5sw    1/1       Terminating         0          16m       10.128.0.67    node1.lab.example.com
hello-2-d5n8g    0/1       Terminating         0          16m       10.128.0.71    node1.lab.example.com
hello-2-jvzmg    1/1       Terminating         0          16m       10.128.0.68    node1.lab.example.com
hello-3-269cs    1/1       Running             0          8s        10.129.0.134   node2.lab.example.com
hello-3-59zhz    0/1       ContainerCreating   0          2s        <none>         node1.lab.example.com
hello-3-6fj9f    1/1       Running             0          8s        10.129.0.133   node2.lab.example.com
hello-3-8fcb9    1/1       Running             0          4s        10.129.0.136   node2.lab.example.com
hello-3-bz58l    1/1       Running             0          4s        10.129.0.137   node2.lab.example.com
hello-3-deploy   1/1       Running             0          11s       10.129.0.132   node2.lab.example.com
hello-3-pqc2m    1/1       Running             0          6s        10.129.0.135   node2.lab.example.com


[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-3-269cs   1/1       Running   0          23s       10.129.0.134   node2.lab.example.com
hello-3-59zhz   1/1       Running   0          17s       10.128.0.74    node1.lab.example.com
hello-3-6fj9f   1/1       Running   0          23s       10.129.0.133   node2.lab.example.com
hello-3-8fcb9   1/1       Running   0          19s       10.129.0.136   node2.lab.example.com
hello-3-bz58l   1/1       Running   0          19s       10.129.0.137   node2.lab.example.com
hello-3-pqc2m   1/1       Running   0          21s       10.129.0.135   node2.lab.example.com


いけた！！

[student@workstation ~]$ diff dc.yml dc-test.yml

<   replicas: 5
---
>   replicas: 6
40,41d41
<       nodeSelector:
<         region: apps
54a55,56
>       nodeSelector:
>         region: infra 


```

-----------------------------------------------------------------

## Managing Images, Image Streams, and Templates(P251)

Image と Image Stream

TEXTにメモした内容をあとから文字に書き起こす。

<registry-address>/<user-name>/<image-name:tag>

is は、デフォルトではopenshift プロジェクトに登録されている。
oc get is -n openshift 
のように、内容を確認するにはプロジェクト名の指定が必要。

```
[student@workstation ~]$ oc get is -n openshift
NAME                                  DOCKER REPO                                                                      TAGS                         UPDATED
dotnet                                docker-registry.default.svc:5000/openshift/dotnet                                1.0,1.1,2.0                  
dotnet-runtime                        docker-registry.default.svc:5000/openshift/dotnet-runtime                        2.0                          
fis-java-openshift                    docker-registry.default.svc:5000/openshift/fis-java-openshift                    2.0,1.0                      45 hours ago
fis-karaf-openshift                   docker-registry.default.svc:5000/openshift/fis-karaf-openshift                   1.0,2.0                      
httpd                                 docker-registry.default.svc:5000/openshift/httpd                                 latest,2.4                   45 hours ago
jboss-amq-62                          docker-registry.default.svc:5000/openshift/jboss-amq-62                          1.1,1.2,1.3 + 4 more...      
jboss-amq-63                          docker-registry.default.svc:5000/openshift/jboss-amq-63                          1.0,1.1,1.2 + 1 more...      
jboss-datagrid65-client-openshift     docker-registry.default.svc:5000/openshift/jboss-datagrid65-client-openshift     1.0,1.1                      
jboss-datagrid65-openshift            docker-registry.default.svc:5000/openshift/jboss-datagrid65-openshift            1.2,1.3,1.4 + 2 more...      
jboss-datagrid71-client-openshift     docker-registry.default.svc:5000/openshift/jboss-datagrid71-client-openshift     1.0                          
jboss-datagrid71-openshift            docker-registry.default.svc:5000/openshift/jboss-datagrid71-openshift            1.0,1.1,1.2                  
jboss-datavirt63-driver-openshift     docker-registry.default.svc:5000/openshift/jboss-datavirt63-driver-openshift     1.0,1.1                      
jboss-datavirt63-openshift            docker-registry.default.svc:5000/openshift/jboss-datavirt63-openshift            1.4,1.0,1.1 + 2 more...      
jboss-decisionserver62-openshift      docker-registry.default.svc:5000/openshift/jboss-decisionserver62-openshift      1.2                          
jboss-decisionserver63-openshift      docker-registry.default.svc:5000/openshift/jboss-decisionserver63-openshift      1.3,1.4                      
jboss-decisionserver64-openshift      docker-registry.default.svc:5000/openshift/jboss-decisionserver64-openshift      1.0,1.1,1.2                  
jboss-eap64-openshift                 docker-registry.default.svc:5000/openshift/jboss-eap64-openshift                 1.3,1.4,1.5 + 4 more...      
jboss-eap70-openshift                 docker-registry.default.svc:5000/openshift/jboss-eap70-openshift                 1.6,1.7,1.3 + 2 more...      
jboss-eap71-openshift                 docker-registry.default.svc:5000/openshift/jboss-eap71-openshift                 1.0-TP,1.1                   
jboss-processserver63-openshift       docker-registry.default.svc:5000/openshift/jboss-processserver63-openshift       1.3,1.4                      
jboss-processserver64-openshift       docker-registry.default.svc:5000/openshift/jboss-processserver64-openshift       1.0,1.1,1.2                  
jboss-webserver30-tomcat7-openshift   docker-registry.default.svc:5000/openshift/jboss-webserver30-tomcat7-openshift   1.1,1.2,1.3                  
jboss-webserver30-tomcat8-openshift   docker-registry.default.svc:5000/openshift/jboss-webserver30-tomcat8-openshift   1.3,1.1,1.2                  
jboss-webserver31-tomcat7-openshift   docker-registry.default.svc:5000/openshift/jboss-webserver31-tomcat7-openshift   1.0,1.1                      
jboss-webserver31-tomcat8-openshift   docker-registry.default.svc:5000/openshift/jboss-webserver31-tomcat8-openshift   1.0,1.1                      
jenkins                               docker-registry.default.svc:5000/openshift/jenkins                               2,1                          
mariadb                               docker-registry.default.svc:5000/openshift/mariadb                               10.1,10.2                    
mongodb                               docker-registry.default.svc:5000/openshift/mongodb                               3.2,3.4,2.4 + 1 more...      45 hours ago
mysql                                 docker-registry.default.svc:5000/openshift/mysql                                 5.5,5.6,5.7 + 1 more...      45 hours ago
nginx                                 docker-registry.default.svc:5000/openshift/nginx                                 1.8,1.10,1.12                45 hours ago
nodejs                                docker-registry.default.svc:5000/openshift/nodejs                                6,4,8 + 1 more...            45 hours ago
perl                                  docker-registry.default.svc:5000/openshift/perl                                  5.16,5.20,5.24               
php                                   docker-registry.default.svc:5000/openshift/php                                   5.6,7.0,5.5 + 1 more...      45 hours ago
postgresql                            docker-registry.default.svc:5000/openshift/postgresql                            9.5,9.2,9.4 + 1 more...      45 hours ago
python                                docker-registry.default.svc:5000/openshift/python                                2.7,3.3,3.4 + 2 more...      
redhat-openjdk18-openshift            docker-registry.default.svc:5000/openshift/redhat-openjdk18-openshift            1.2,1.0,1.1                  45 hours ago
redhat-sso70-openshift                docker-registry.default.svc:5000/openshift/redhat-sso70-openshift                1.4,1.3                      
redhat-sso71-openshift                docker-registry.default.svc:5000/openshift/redhat-sso71-openshift                1.0,1.1,1.2 + 1 more...      
redis                                 docker-registry.default.svc:5000/openshift/redis                                 3.2                          
ruby                                  docker-registry.default.svc:5000/openshift/ruby                                  latest,2.4,2.0 + 2 more...   45 hours ago

```

```
[student@workstation ~]$ oc get is php -n openshift
NAME      DOCKER REPO                                      TAGS                      UPDATED
php       docker-registry.default.svc:5000/openshift/php   5.6,7.0,7.1 + 1 more...   45 hours ago
[student@workstation ~]$ oc get is php -n openshift -o wide
NAME      DOCKER REPO                                      TAGS                      UPDATED
php       docker-registry.default.svc:5000/openshift/php   5.6,7.0,5.5 + 1 more...   45 hours ago


[student@workstation ~]$ oc describe is php -n openshift
Name:			php
Namespace:		openshift
Created:		45 hours ago
Labels:			<none>
Annotations:		openshift.io/display-name=PHP
			openshift.io/image.dockerRepositoryCheck=2019-09-24T05:11:45Z
Docker Pull Spec:	docker-registry.default.svc:5000/openshift/php
Image Lookup:		local=false
Unique Images:		2
Tags:			5

7.1 (latest)
  tagged from registry.lab.example.com/rhscl/php-71-rhel7:latest

  Build and run PHP 7.1 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.1/README.md.
  Tags: builder, php
  Supports: php:7.1, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/rhscl/php-71-rhel7:latest" not found
      45 hours ago

7.0
  tagged from registry.lab.example.com/rhscl/php-70-rhel7:latest

  Build and run PHP 7.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.0/README.md.
  Tags: builder, php
  Supports: php:7.0, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd
      45 hours ago

5.6
  tagged from registry.lab.example.com/rhscl/php-56-rhel7:latest

  Build and run PHP 5.6 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.6/README.md.
  Tags: builder, php
  Supports: php:5.6, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-56-rhel7@sha256:920c2cf85b5da5d0701898f0ec9ee567473fa4b9af6f3ac5b2b3f863796bbd68
      45 hours ago

5.5
  tagged from registry.lab.example.com/openshift3/php-55-rhel7:latest

  Build and run PHP 5.5 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.5/README.md.
  Tags: hidden, builder, php
  Supports: php:5.5, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/openshift3/php-55-rhel7:latest" not found
      45 hours ago

```

oc import image 

あとからでてくる？

### Templates

Template resource

```
[student@workstation ~]$ oc get template
No resources found.
[student@workstation ~]$ oc get template -n openshift
NAME                                            DESCRIPTION                                                                        PARAMETERS        OBJECTS
3scale-gateway                                  3scale API Gateway                                                                 15 (6 blank)      2
amp-apicast-wildcard-router                                                                                                        3 (1 blank)       4
amp-pvc                                                                                                                            0 (all set)       4
amq62-basic                                     Application template for JBoss A-MQ brokers. These can be deployed as standal...   11 (4 blank)      6
amq62-persistent                                Application template for JBoss A-MQ brokers. These can be deployed as standal...   13 (4 blank)      8
amq62-persistent-ssl                            An example JBoss A-MQ application. For more information about using this temp...   18 (6 blank)      12
amq62-ssl                                       Application template for JBoss A-MQ brokers. These can be deployed as standal...   16 (6 blank)      10
amq63-basic                                     Application template for JBoss A-MQ brokers. These can be deployed as standal...   11 (4 blank)      6
amq63-persistent                                An example JBoss A-MQ application. For more information about using this temp...   13 (4 blank)      8
amq63-persistent-ssl                            An example JBoss A-MQ application. For more information about using this temp...   18 (6 blank)      12
amq63-ssl                                       An example JBoss A-MQ application. For more information about using this temp...   16 (6 blank)      10
cakephp-mysql-example                           An example CakePHP application with a MySQL database. For more information ab...   19 (4 blank)      8
cakephp-mysql-persistent                        An example CakePHP application with a MySQL database. For more information ab...   20 (4 blank)      9
dancer-mysql-example                            An example Dancer application with a MySQL database. For more information abo...   16 (5 blank)      8
dancer-mysql-persistent                         An example Dancer application with a MySQL database. For more information abo...   17 (5 blank)      9
datagrid65-basic                                Application template for JDG 6.5 applications.                                     14 (8 blank)      6
datagrid65-https                                Application template for JDG 6.5 applications.                                     23 (13 blank)     8
datagrid65-mysql                                Application template for JDG 6.5 and MySQL applications.                           36 (21 blank)     10
datagrid65-mysql-persistent                     An example JBoss Data Grid application with a MySQL database. For more inform...   37 (21 blank)     11
datagrid65-postgresql                           Application template for JDG 6.5 and PostgreSQL applications built using.          33 (18 blank)     10
datagrid65-postgresql-persistent                An example JBoss Data Grid application with a PostgreSQL database. For more i...   34 (18 blank)     11
datagrid71-basic                                An example JBoss Data Grid application. For more information about using this...   18 (12 blank)     6
datagrid71-https                                An example JBoss Data Grid application. For more information about using this...   27 (17 blank)     8
datagrid71-mysql                                An example JBoss Data Grid application with a MySQL database. For more inform...   40 (25 blank)     10
datagrid71-mysql-persistent                     An example JBoss Data Grid application with a MySQL database. For more inform...   41 (25 blank)     11
datagrid71-partition                            An example JBoss Data Grid application. For more information about using this...   20 (12 blank)     8
datagrid71-postgresql                           An example JBoss Data Grid application with a PostgreSQL database. For more i...   37 (22 blank)     10
datagrid71-postgresql-persistent                An example JBoss Data Grid application with a PostgreSQL database. For more i...   38 (22 blank)     11
datavirt63-basic-s2i                            Application template for JBoss Data Virtualization 6.3 services built using S2I.   19 (4 blank)      6
datavirt63-extensions-support-s2i               An example JBoss Data Virtualization application. For more information about...    34 (7 blank)      10
datavirt63-secure-s2i                           An example JBoss Data Virtualization application. For more information about...    50 (20 blank)     8
decisionserver64-amq-s2i                        An example BRMS decision server A-MQ application. For more information about...    30 (5 blank)      10
decisionserver64-basic-s2i                      Application template for Red Hat JBoss BRMS 6.4 decision server applications...    17 (5 blank)      5
decisionserver64-https-s2i                      An example BRMS decision server application. For more information about using...   24 (6 blank)      7
django-psql-example                             An example Django application with a PostgreSQL database. For more informatio...   17 (5 blank)      8
django-psql-persistent                          An example Django application with a PostgreSQL database. For more informatio...   18 (5 blank)      9
dotnet-example                                  An example .NET Core application.                                                  16 (5 blank)      5
dotnet-pgsql-persistent                         An example .NET Core application with a PostgreSQL database. For more informa...   23 (6 blank)      9
dotnet-runtime-example                          An example .NET Core Runtime example application.                                  17 (5 blank)      7
eap64-amq-persistent-s2i                        An example EAP 6 A-MQ application. For more information about using this temp...   34 (10 blank)     12
eap64-amq-s2i                                   An example EAP 6 A-MQ application. For more information about using this temp...   32 (10 blank)     11
eap64-basic-s2i                                 An example EAP 6 application. For more information about using this template,...   16 (5 blank)      6
eap64-https-s2i                                 An example EAP 6 application. For more information about using this template,...   26 (11 blank)     8
eap64-mongodb-persistent-s2i                    An example EAP 6 application with a MongoDB database. For more information ab...   39 (18 blank)     11
eap64-mongodb-s2i                               An example EAP 6 application with a MongoDB database. For more information ab...   38 (18 blank)     10
eap64-mysql-persistent-s2i                      An example EAP 6 application with a MySQL database. For more information abou...   40 (19 blank)     11
eap64-mysql-s2i                                 An example EAP 6 application with a MySQL database. For more information abou...   39 (19 blank)     10
eap64-postgresql-persistent-s2i                 An example EAP 6 application with a PostgreSQL database. For more information...   37 (16 blank)     11
eap64-postgresql-s2i                            An example EAP 6 application with a PostgreSQL database. For more information...   36 (16 blank)     10
eap64-sso-s2i                                   An example EAP 6 Single Sign-On application. For more information about using...   44 (19 blank)     8
eap64-third-party-db-s2i                        An example EAP 6 DB application. For more information about using this templa...   30 (7 blank)      8
eap64-tx-recovery-s2i                           An example EAP 6 application. For more information about using this template,...   18 (5 blank)      8
eap70-amq-persistent-s2i                        An example EAP 7 A-MQ application. For more information about using this temp...   34 (10 blank)     12
eap70-amq-s2i                                   An example EAP 7 A-MQ application. For more information about using this temp...   32 (10 blank)     11
eap70-basic-s2i                                 An example EAP 7 application. For more information about using this template,...   16 (5 blank)      6
eap70-https-s2i                                 An example EAP 7 application. For more information about using this template,...   26 (11 blank)     8
eap70-mongodb-persistent-s2i                    An example EAP 7 application with a MongoDB database. For more information ab...   39 (18 blank)     11
eap70-mongodb-s2i                               An example EAP 7 application with a MongoDB database. For more information ab...   38 (18 blank)     10
eap70-mysql-persistent-s2i                      An example EAP 7 application with a MySQL database. For more information abou...   40 (19 blank)     11
eap70-mysql-s2i                                 An example EAP 7 application with a MySQL database. For more information abou...   39 (19 blank)     10
eap70-postgresql-persistent-s2i                 An example EAP 7 application with a PostgreSQL database. For more information...   37 (16 blank)     11
eap70-postgresql-s2i                            An example EAP 7 application with a PostgreSQL database. For more information...   36 (16 blank)     10
eap70-sso-s2i                                   An example EAP 7 Single Sign-On application. For more information about using...   44 (19 blank)     8
eap70-third-party-db-s2i                        An example EAP 7 DB application. For more information about using this templa...   30 (7 blank)      8
eap70-tx-recovery-s2i                           An example EAP 7 application. For more information about using this template,...   18 (5 blank)      8
eap71-amq-persistent-s2i                        An example EAP 7 A-MQ application. For more information about using this temp...   34 (10 blank)     12
eap71-amq-s2i                                   An example EAP 7 A-MQ application. For more information about using this temp...   32 (10 blank)     11
eap71-basic-s2i                                 An example EAP 7 application. For more information about using this template,...   16 (5 blank)      6
eap71-https-s2i                                 An example EAP 7 application. For more information about using this template,...   26 (11 blank)     8
eap71-mongodb-persistent-s2i                    An example EAP 7 application with a MongoDB database. For more information ab...   39 (18 blank)     11
eap71-mongodb-s2i                               An example EAP 7 application with a MongoDB database. For more information ab...   38 (18 blank)     10
eap71-mysql-persistent-s2i                      An example EAP 7 application with a MySQL database. For more information abou...   40 (19 blank)     11
eap71-mysql-s2i                                 An example EAP 7 application with a MySQL database. For more information abou...   39 (19 blank)     10
eap71-postgresql-persistent-s2i                 An example EAP 7 application with a PostgreSQL database. For more information...   37 (16 blank)     11
eap71-postgresql-s2i                            An example EAP 7 application with a PostgreSQL database. For more information...   36 (16 blank)     10
eap71-sso-s2i                                   An example EAP 7 Single Sign-On application. For more information about using...   44 (19 blank)     8
eap71-third-party-db-s2i                        An example EAP 7 DB application. For more information about using this templa...   30 (7 blank)      8
eap71-tx-recovery-s2i                           An example EAP 7 application. For more information about using this template,...   18 (5 blank)      8
httpd-example                                   An example Apache HTTP Server (httpd) application that serves static content....   9 (3 blank)       5
jenkins-ephemeral                               Jenkins service, without persistent storage....                                    6 (all set)       6
jenkins-persistent                              Jenkins service, with persistent storage....                                       7 (all set)       7
jws31-tomcat7-basic-s2i                         Application template for JWS applications built using S2I.                         12 (3 blank)      5
jws31-tomcat7-https-s2i                         An example JBoss Web Server application configured for use with https. For mo...   17 (5 blank)      7
jws31-tomcat7-mongodb-persistent-s2i            An example JBoss Web Server application with a MongoDB database. For more inf...   30 (12 blank)     10
jws31-tomcat7-mongodb-s2i                       Application template for JWS MongoDB applications built using S2I.                 29 (12 blank)     9
jws31-tomcat7-mysql-persistent-s2i              An example JBoss Web Server application with a MySQL database. For more infor...   31 (13 blank)     10
jws31-tomcat7-mysql-s2i                         Application template for JWS MySQL applications built using S2I.                   30 (13 blank)     9
jws31-tomcat7-postgresql-persistent-s2i         An example JBoss Web Server application with a PostgreSQL database. For more...    28 (10 blank)     10
jws31-tomcat7-postgresql-s2i                    Application template for JWS PostgreSQL applications built using S2I.              27 (10 blank)     9
jws31-tomcat8-basic-s2i                         An example JBoss Web Server application. For more information about using thi...   12 (3 blank)      5
jws31-tomcat8-https-s2i                         An example JBoss Web Server application. For more information about using thi...   17 (5 blank)      7
jws31-tomcat8-mongodb-persistent-s2i            An example JBoss Web Server application with a MongoDB database. For more inf...   30 (12 blank)     10
jws31-tomcat8-mongodb-s2i                       Application template for JWS MongoDB applications built using S2I.                 29 (12 blank)     9
jws31-tomcat8-mysql-persistent-s2i              An example JBoss Web Server application with a MySQL database. For more infor...   31 (13 blank)     10
jws31-tomcat8-mysql-s2i                         Application template for JWS MySQL applications built using S2I.                   30 (13 blank)     9
jws31-tomcat8-postgresql-persistent-s2i         Application template for JWS PostgreSQL applications with persistent storage...    28 (10 blank)     10
jws31-tomcat8-postgresql-s2i                    Application template for JWS PostgreSQL applications built using S2I.              27 (10 blank)     9
mariadb-ephemeral                               MariaDB database service, without persistent storage. For more information ab...   8 (3 generated)   3
mariadb-persistent                              MariaDB database service, with persistent storage. For more information about...   9 (3 generated)   4
mongodb-ephemeral                               MongoDB database service, without persistent storage. For more information ab...   8 (3 generated)   3
mongodb-persistent                              MongoDB database service, with persistent storage. For more information about...   9 (3 generated)   4
mysql-ephemeral                                 MySQL database service, without persistent storage. For more information abou...   8 (3 generated)   3
mysql-persistent                                MySQL database service, with persistent storage. For more information about u...   9 (3 generated)   4
nginx-example                                   An example Nginx HTTP server and a reverse proxy (nginx) application that ser...   10 (3 blank)      5
nodejs-mongo-persistent                         An example Node.js application with a MongoDB database. For more information...    17 (4 blank)      9
nodejs-mongodb-example                          An example Node.js application with a MongoDB database. For more information...    16 (4 blank)      8
openjdk18-web-basic-s2i                         An example Java application using OpenJDK 8. For more information about using...   8 (1 blank)       5
postgresql-ephemeral                            PostgreSQL database service, without persistent storage. For more information...   7 (2 generated)   3
postgresql-persistent                           PostgreSQL database service, with persistent storage. For more information ab...   8 (2 generated)   4
processserver64-amq-mysql-persistent-s2i        An example BPM Suite application with A-MQ and a MySQL database. For more inf...   49 (13 blank)     14
processserver64-amq-mysql-s2i                   An example BPM Suite application with A-MQ and a MySQL database. For more inf...   47 (13 blank)     12
processserver64-amq-postgresql-persistent-s2i   An example BPM Suite application with A-MQ and a PostgreSQL database. For mor...   46 (10 blank)     14
processserver64-amq-postgresql-s2i              An example BPM Suite application with A-MQ and a PostgreSQL database. For mor...   44 (10 blank)     12
processserver64-basic-s2i                       An example BPM Suite application. For more information about using this templ...   18 (5 blank)      5
processserver64-mysql-persistent-s2i            An example BPM Suite application with a MySQL database. For more information...    40 (14 blank)     10
processserver64-mysql-s2i                       An example BPM Suite application with a MySQL database. For more information...    39 (14 blank)     9
processserver64-postgresql-persistent-s2i       An example BPM Suite application with a PostgreSQL database. For more informa...   37 (11 blank)     10
processserver64-postgresql-s2i                  An example BPM Suite application with a PostgreSQL database. For more informa...   36 (11 blank)     9
rails-pgsql-persistent                          An example Rails application with a PostgreSQL database. For more information...   21 (4 blank)      9
rails-postgresql-example                        An example Rails application with a PostgreSQL database. For more information...   20 (4 blank)      8
redis-ephemeral                                 Redis in-memory data structure store, without persistent storage. For more in...   5 (1 generated)   3
redis-persistent                                Redis in-memory data structure store, with persistent storage. For more infor...   6 (1 generated)   4
registry-console                                Template for deploying registry web console. Requires cluster-admin.               8 (3 blank)       4
s2i-karaf2-camel-amq                            Camel route using ActiveMQ in Karaf container. This quickstart shows how to u...   18 (4 blank)      3
s2i-karaf2-camel-log                            A simple Camel route in Karaf container. This quickstart shows a simple Apach...   15 (2 blank)      3
s2i-karaf2-camel-rest-sql                       Camel example using Rest DSL with SQL Database in Karaf container.  This exam...   18 (4 blank)      5
s2i-karaf2-cxf-rest                             REST example using CXF in Karaf container. This quickstart demonstrates how t...   14 (2 blank)      5
s2i-spring-boot-camel                           Spring-Boot and Camel QuickStart. This example demonstrates how you can use A...   12 (2 blank)      3
s2i-spring-boot-camel-amq                       Spring Boot, Camel and ActiveMQ QuickStart. This quickstart demonstrates how...    15 (4 blank)      3
s2i-spring-boot-camel-config                    Spring Boot and Camel using ConfigMaps and Secrets. This quickstart demonstra...   13 (2 blank)      3
s2i-spring-boot-camel-drools                    Spring-Boot, Camel and JBoss BRMS QuickStart. This example demonstrates how y...   15 (3 blank)      3
s2i-spring-boot-camel-infinispan                Spring Boot, Camel and JBoss Data Grid QuickStart. This quickstart demonstrat...   13 (2 blank)      3
s2i-spring-boot-camel-rest-sql                  Spring Boot, Camel REST DSL and MySQL QuickStart. This quickstart demonstrate...   17 (4 blank)      5
s2i-spring-boot-camel-teiid                     Spring-Boot, Camel and JBoss Data Virtualization QuickStart. This example dem...   16 (4 blank)      3
s2i-spring-boot-camel-xml                       Spring-Boot and Camel Xml QuickStart. This example demonstrates how you can u...   12 (2 blank)      3
s2i-spring-boot-cxf-jaxrs                       Spring-Boot and CXF JAXRS QuickStart. This example demonstrates how you can u...   13 (2 blank)      5
s2i-spring-boot-cxf-jaxws                       Spring-Boot and CXF JAXWS QuickStart. This example demonstrates how you can u...   13 (2 blank)      5
sso71-https                                     An example SSO 7 application. For more information about using this template,...   26 (15 blank)     6
sso71-mysql                                     An example SSO 7 application with a MySQL database. For more information abou...   36 (20 blank)     8
sso71-mysql-persistent                          An example SSO 7 application with a MySQL database. For more information abou...   37 (20 blank)     9
sso71-postgresql                                An example SSO 7 application with a PostgreSQL database. For more information...   33 (17 blank)     8
sso71-postgresql-persistent                     An example SSO 7 application with a PostgreSQL database. For more information...   34 (17 blank)     9
system                                                                                                                             20 (1 blank)      34

```

------------------------------------------------------------------

## Guide Exercise: Managing Image Streams(P256)

- pre

```
[student@workstation ~]$ lab schedule-is setup

Checking prerequisites for GE: Managing Image Streams

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for GE: Managing Image Streams

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS


```


1.

```
[student@workstation ~]$ oc new-project schedule-is
Now using project "schedule-is" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation ~]$ oc new-app --name=phpmyadmin --docker-image=registry.lab.example.com/phpmyadmin/phpmyadmin:4.7
--> Found Docker image f51fd61 (18 months old) from registry.lab.example.com for "registry.lab.example.com/phpmyadmin/phpmyadmin:4.7"

    * An image stream will be created as "phpmyadmin:4.7" that will track this image
    * This image will be deployed in deployment config "phpmyadmin"
    * Ports 80/tcp, 9000/tcp will be load balanced by service "phpmyadmin"
      * Other containers can access this service through the hostname "phpmyadmin"
    * WARNING: Image "registry.lab.example.com/phpmyadmin/phpmyadmin:4.7" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    imagestream "phpmyadmin" created
    deploymentconfig "phpmyadmin" created
    service "phpmyadmin" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/phpmyadmin' 
    Run 'oc status' to view your app.
[student@workstation ~]$ 

```
2. 

```
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * schedule-is
    secure-review

Using project "schedule-is".
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc project schedule-is
Already on project "schedule-is" on server "https://master.lab.example.com:443".

[student@workstation ~]$ oc create serviceaccount phpmyadmin-account
serviceaccount "phpmyadmin-account" created


[student@workstation ~]$ oc adm policy add-scc-to-
add-scc-to-group  add-scc-to-user   


[student@workstation ~]$ oc adm policy add-scc-to-user 
.ansible/       .config/        do280-ansible/  .local/         .pki/           Videos/
.bash_history   .dbus/          Documents/      .m2/            Public/         .viminfo
.bash_logout    dc-test.yml     Downloads/      .mozilla/       .rnd            
.bash_profile   dc.yml          .esd_auth       Music/          scaling.yml     
.bashrc         Desktop/        .ICEauthority   .mysql_history  .ssh/           
.cache/         DO280/          .kube/          Pictures/       Templates/      


[student@workstation ~]$ oc adm policy add-scc-to-user anyuid -z phpmyadmin-account
scc "anyuid" added to: ["system:serviceaccount:schedule-is:phpmyadmin-account"]


```


3.

```
[student@workstation ~]$ oc login -u developer
Logged into "https://master.lab.example.com:443" as "developer" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * schedule-is

Using project "schedule-is".
[student@workstation ~]$ more /home/student/DO280/labs/secure-review/patch-dc.sh 
#!/bin/bash

oc patch dc/phpmyadmin --patch \
'{"spec":{"template":{"spec":{"serviceAccountName": "phpmyadmin-account"}}}}'

[student@workstation ~]$ /home/student/DO280/labs/secure-review/patch-dc.sh 
deploymentconfig "phpmyadmin" patched

[student@workstation ~]$ oc get pods
NAME                 READY     STATUS        RESTARTS   AGE
phpmyadmin-1-ffjtx   0/1       Terminating   4          3m
phpmyadmin-2-vgsc7   1/1       Running       0          40s



```


4. 
```
[student@workstation ~]$ cd /home/student/DO280/labs/
\
[student@workstation labs]$ cd schedule-is/

[student@workstation schedule-is]$ ls -la
total 169576
drwxr-xr-x.  2 student student        69 Sep 26 11:47 .
drwxr-xr-x. 10 student student       176 Sep 26 11:47 ..
-rw-r--r--.  1 root    root    173638144 Sep 26 11:47 phpmyadmin-latest.tar
-rwxr-xr-x.  1 student student       463 Aug 17  2018 trust_internal_registry.sh


[student@workstation schedule-is]$ docker load -i phpmyadmin-latest.tar 
cd7100a72410: Loading layer [==================================================>] 4.403 MB/4.403 MB
f06b58790eeb: Loading layer [==================================================>] 2.873 MB/2.873 MB
730b09e0430c: Loading layer [==================================================>] 11.78 kB/11.78 kB
931398d7728c: Loading layer [==================================================>] 3.584 kB/3.584 kB
62cc7a6086ee: Loading layer [==================================================>] 12.17 MB/12.17 MB
81a0447a08a2: Loading layer [==================================================>] 4.096 kB/4.096 kB
e2e960fe72d5: Loading layer [==================================================>] 63.51 MB/63.51 MB
2a1618e7fd7b: Loading layer [==================================================>] 11.78 kB/11.78 kB
24b469d11dd9: Loading layer [==================================================>] 178.7 kB/178.7 kB
d2d690a99eef: Loading layer [==================================================>]  2.56 kB/2.56 kB
94902ed2d44a: Loading layer [==================================================>]  29.7 kB/29.7 kB
0fabe17015af: Loading layer [==================================================>] 46.35 MB/46.35 MB
269f8070596b: Loading layer [==================================================>] 8.461 MB/8.461 MB
d5b9d4ad6025: Loading layer [==================================================>] 35.53 MB/35.53 MB
8952a88ecadd: Loading layer [==================================================>] 15.36 kB/15.36 kB
a8454daaac38: Loading layer [==================================================>] 4.608 kB/4.608 kB
ecba9843268b: Loading layer [==================================================>]  2.56 kB/2.56 kB
Loaded image ID: sha256:93d0d7db5ce202f07f3cedd5c027e2f97874d0df904594f53d46024b021603ec
[student@workstation schedule-is]$ 

[student@workstation schedule-is]$ docker images
REPOSITORY                                      TAG                 IMAGE ID            CREATED             SIZE
node-hello                                      latest              194638dc4731        18 hours ago        495 MB
registry.lab.example.com/node-hello             latest              194638dc4731        18 hours ago        495 MB
node-hello                                      katest              1813a728f00e        18 hours ago        495 MB
<none>                                          <none>              93d0d7db5ce2        15 months ago       166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7   latest              fba56b5381b7        2 years ago         489 MB


[student@workstation schedule-is]$ docker tag 93d0d7db5ce2 docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin:4.7

[student@workstation schedule-is]$ docker images
REPOSITORY                                                            TAG                 IMAGE ID            CREATED             SIZE
node-hello                                                            latest              194638dc4731        18 hours ago        495 MB
registry.lab.example.com/node-hello                                   latest              194638dc4731        18 hours ago        495 MB
node-hello                                                            katest              1813a728f00e        18 hours ago        495 MB
docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin   4.7                 93d0d7db5ce2        15 months ago       166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7                         latest              fba56b5381b7        2 years ago         489 MB
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ TOKEN=$(oc whoami -t)
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ export | grep TOKEN
declare -x TOKEN="VzJe7y2kC1K8sXOj9OlUIedhLkUkNO1fiStP5NK9fQ4"
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ docker login -u developer -p ${TOKEN} docker-registry-default.app.lab.example.com
Error response from daemon: Get https://docker-registry-default.app.lab.example.com/v1/users/: dial tcp: lookup docker-registry-default.app.lab.example.com on 172.25.250.254:53: no such host


[student@workstation schedule-is]$ docker login -u developer -p ${TOKEN} docker-registry-default.apps.lab.example.com
Error response from daemon: Get https://docker-registry-default.apps.lab.example.com/v1/users/: x509: certificate signed by unknown authority



[student@workstation schedule-is]$ cat ./trust_internal_registry.sh 
#!/bin/sh


echo "Fetching the OpenShift internal registry certificate."
scp -q root@master.lab.example.com:/etc/origin/master/registry.crt .
echo "done."

echo -e "\nCopying certificate to the correct directory." 
sudo cp registry.crt /etc/pki/ca-trust/source/anchors/docker-registry-default.apps.lab.example.com.crt
echo "done."

sudo update-ca-trust
echo -e "\nSystem trust updated."

echo -e "\nRestarting docker."
sudo systemctl restart docker
echo "done."


[student@workstation schedule-is]$ ./trust_internal_registry.sh 
Fetching the OpenShift internal registry certificate.
done.

Copying certificate to the correct directory.
done.

System trust updated.

Restarting docker.
done.



[student@workstation schedule-is]$ docker login -u developer -p ${TOKEN} docker-registry-default.apps.lab.example.com
Login Succeeded
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ docker images
REPOSITORY                                                            TAG                 IMAGE ID            CREATED             SIZE
node-hello                                                            latest              194638dc4731        18 hours ago        495 MB
registry.lab.example.com/node-hello                                   latest              194638dc4731        18 hours ago        495 MB
node-hello                                                            katest              1813a728f00e        18 hours ago        495 MB
docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin   4.7                 93d0d7db5ce2        15 months ago       166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7                         latest              fba56b5381b7        2 years ago         489 MB
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ docker push docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin:4.7
The push refers to a repository [docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin]
ecba9843268b: Pushed 
a8454daaac38: Pushed 
8952a88ecadd: Pushed 
d5b9d4ad6025: Pushed 
269f8070596b: Pushed 
0fabe17015af: Pushed 
94902ed2d44a: Pushed 
d2d690a99eef: Pushed 
24b469d11dd9: Pushed 
2a1618e7fd7b: Pushed 
e2e960fe72d5: Pushed 
81a0447a08a2: Pushed 
62cc7a6086ee: Pushed 
931398d7728c: Pushed 
730b09e0430c: Pushed 
f06b58790eeb: Pushed 
cd7100a72410: Pushed 
4.7: digest: sha256:b003fa5555dcb0a305d26aec3935b3a1127179ea8ad9d57685df4e4eab912ca8 size: 3874
[student@workstation schedule-is]$ 



```

5. 

```
[student@workstation schedule-is]$ oc get pods
NAME                 READY     STATUS    RESTARTS   AGE
phpmyadmin-3-585v8   1/1       Running   0          35s

```

6.

```
[student@workstation schedule-is]$ oc delete project schedule-is 
project "schedule-is" deleted
[student@workstation schedule-is]$ oc whoami
developer
[student@workstation schedule-is]$ oc project hogehoge
Now using project "hogehoge" on server "https://master.lab.example.com:443".
[student@workstation schedule-is]$ 

```

-------------------------------------------------------------------

## Lab: Managing Application Deployments(P261)

- pre

```
[student@workstation schedule-is]$ lab manage-review setup

Checking prerequisites for Lab: Managing Application Deployments

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Overall setup status...........................................  SUCCESS


```

1.

```
[student@workstation schedule-is]$ oc label node node1.lab.example.com region=service --overwrite=true
node "node1.lab.example.com" labeled

[student@workstation schedule-is]$ oc label node node2.lab.example.com region=applications --overwrite=true
node "node2.lab.example.com" labeled

[student@workstation schedule-is]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   service
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   applications

[student@workstation schedule-is]$ oc label node node1.lab.example.com region=services --overwrite=true
node "node1.lab.example.com" labeled
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ 

[student@workstation schedule-is]$ oc get node -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   services
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   applications

```

2.

```
[student@workstation schedule-is]$ oc whoami
admin
[student@workstation schedule-is]$ oc new-project manage-review
Now using project "manage-review" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

```

3.

```
[student@workstation schedule-is]$ oc new-app http://registry.lab.example.com/version --name=version
error: the image match "php" for source repository "http://registry.lab.example.com/version" does not appear to be a source-to-image builder.

- to attempt to use this image as a source builder, pass "--strategy=source"
- to use it as a base image for a Docker build, pass "--strategy=docker"
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ oc new-app http://registry.lab.example.com/version --name=version --strategy=source
--> Found Docker image d1d1e48 (13 days old) from Docker Hub for "php"

    * An image stream will be created as "php:latest" that will track the source image
    * The source repository appears to match: php
    * A source build using source code from http://registry.lab.example.com/version will be created
      * The resulting image will be pushed to image stream "version:latest"
      * Every time "php:latest" changes a new build will be triggered
    * This image will be deployed in deployment config "version"
    * The image does not expose any ports - if you want to load balance or send traffic to this component
      you will need to create a service with 'expose dc/version --port=[port]' later
    * WARNING: Image "php" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    imagestream "php" created
    imagestream "version" created
    buildconfig "version" created
    deploymentconfig "version" created
--> Success
    Build scheduled, use 'oc logs -f bc/version' to track its progress.
    Run 'oc status' to view your app.

[student@workstation schedule-is]$ oc get pods
NAME              READY     STATUS    RESTARTS   AGE
version-1-build   0/1       Error     0          12s


[student@workstation schedule-is]$ oc scale dc/version --replicas=3
deploymentconfig "version" scaled
[student@workstation schedule-is]$ oc get dc/version -o yaml
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-26T03:48:02Z
  generation: 2
  labels:
    app: version
  name: version
  namespace: manage-review
  resourceVersion: "356159"
  selfLink: /apis/apps.openshift.io/v1/namespaces/manage-review/deploymentconfigs/version
  uid: 70a57d4d-e010-11e9-98ef-52540000fa0a
spec:
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    app: version
    deploymentconfig: version
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: version
        deploymentconfig: version
    spec:
      containers:
      - image: version:latest
        imagePullPolicy: Always
        name: version
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - version
      from:
        kind: ImageStreamTag
        name: version:latest
        namespace: manage-review
    type: ImageChange
status:
  availableReplicas: 0
  conditions:
  - lastTransitionTime: 2019-09-26T03:48:02Z
    lastUpdateTime: 2019-09-26T03:48:02Z
    message: Deployment config does not have minimum availability.
    status: "False"
    type: Available
  latestVersion: 0
  observedGeneration: 2
  replicas: 0
  unavailableReplicas: 0
  updatedReplicas: 0



やりなおし

[student@workstation schedule-is]$ oc new-app http://registry.lab.example.com/version -i php:7.0 --name=version
--> Found image c101534 (2 years old) in image stream "openshift/php" under tag "7.0" for "php:7.0"

    Apache 2.4 with PHP 7.0 
    ----------------------- 
    PHP 7.0 available as docker container is a base platform for building and running various PHP 7.0 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php70, rh-php70

    * The source repository appears to match: php
    * A source build using source code from http://registry.lab.example.com/version will be created
      * The resulting image will be pushed to image stream "version:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "version"
    * Port 8080/tcp will be load balanced by service "version"
      * Other containers can access this service through the hostname "version"

--> Creating resources ...
    imagestream "version" created
    buildconfig "version" created
    deploymentconfig "version" created
    service "version" created
--> Success
    Build scheduled, use 'oc logs -f bc/version' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/version' 
    Run 'oc status' to view your app.
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ oc get pods
NAME              READY     STATUS    RESTARTS   AGE
version-1-build   1/1       Running   0          19s
[student@workstation schedule-is]$ oc get pods
NAME               READY     STATUS              RESTARTS   AGE
version-1-build    0/1       Completed           0          22s
version-1-deploy   0/1       ContainerCreating   0          1s


[student@workstation schedule-is]$ oc scale dc/version --replicas=3
deploymentconfig "version" scaled
[student@workstation schedule-is]$ oc get pods
NAME              READY     STATUS      RESTARTS   AGE
version-1-9lkkb   1/1       Running     0          6s
version-1-build   0/1       Completed   0          50s
version-1-n8rbk   1/1       Running     0          6s
version-1-xhbk9   1/1       Running     0          27s


```

4.5.

```
[student@workstation schedule-is]$ oc apply -f dc-version.yml 
Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
deploymentconfig "version" configured
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ oc get pods
NAME               READY     STATUS        RESTARTS   AGE
version-1-build    0/1       Completed     0          3m
version-1-xhbk9    0/1       Terminating   0          3m
version-2-cvlwc    1/1       Running       0          8s
version-2-deploy   1/1       Running       0          14s
version-2-j9972    1/1       Running       0          11s
version-2-x86md    1/1       Running       0          5s
[student@workstation schedule-is]$ oc get pods -o wide
NAME              READY     STATUS        RESTARTS   AGE       IP             NODE
version-1-build   0/1       Completed     0          3m        10.129.0.144   node2.lab.example.com
version-1-xhbk9   0/1       Terminating   0          3m        10.129.0.146   node2.lab.example.com
version-2-cvlwc   1/1       Running       0          12s       10.129.0.151   node2.lab.example.com
version-2-j9972   1/1       Running       0          15s       10.129.0.150   node2.lab.example.com
version-2-x86md   1/1       Running       0          9s        10.129.0.152   node2.lab.example.com
```
6.

```
[student@workstation schedule-is]$ oc label node node1.lab.example.com region=applications --overwrite=true
node "node1.lab.example.com" labeled
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ oc get nodes -L region
NAME                     STATUS    ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   applications
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657   applications

```

7.

```
[student@workstation schedule-is]$ oc adm manage-node node2.lab.example.jp --schedulable=false
error: No nodes found

[student@workstation schedule-is]$ oc adm manage-node node2.lab.example.com --schedulable=false
NAME                    STATUS                     ROLES     AGE       VERSION
node2.lab.example.com   Ready,SchedulingDisabled   compute   1d        v1.9.1+a0ce1bc657

[student@workstation schedule-is]$ oc adm drain --help
Drain node in preparation for maintenance. 

The given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the
pods if the APIServer supports eviction (http://kubernetes.io/docs/admin/disruptions/). Otherwise,
it will use normal DELETE to delete the pods. The 'drain' evicts or deletes all pods except mirror
pods (which cannot be deleted through the API server).  If there are DaemonSet-managed pods, drain
will not proceed without --ignore-daemonsets, and regardless it will not delete any
DaemonSet-managed pods, because those pods would be immediately replaced by the DaemonSet
controller, which ignores unschedulable markings.  If there are any pods that are neither mirror
pods nor managed by ReplicationController, ReplicaSet, DaemonSet, StatefulSet or Job, then drain
will not delete any pods unless you use --force.  --force will also allow deletion to proceed if the
managing resource of one or more pods is missing. 

'drain' waits for graceful termination. You should not operate on the machine until the command
completes. 

When you are ready to put the node back into service, use kubectl uncordon, which will make the node
schedulable again. 

! http://kubernetes.io/images/docs/kubectl_drain.svg

Usage:
  oc adm drain NODE [options]

Examples:
  # Drain node "foo", even if there are pods not managed by a ReplicationController, ReplicaSet,
Job, DaemonSet or StatefulSet on it.
  $ oc adm drain foo --force
  
  # As above, but abort if there are pods not managed by a ReplicationController, ReplicaSet, Job,
DaemonSet or StatefulSet, and use a grace period of 15 minutes.
  $ oc adm drain foo --grace-period=900

Options:
      --delete-local-data=false: Continue even if there are pods using emptyDir (local data that
will be deleted when the node is drained).
      --dry-run=false: If true, only print the object that would be sent, without sending it.
      --force=false: Continue even if there are pods not managed by a ReplicationController,
ReplicaSet, Job, DaemonSet or StatefulSet.
      --grace-period=-1: Period of time in seconds given to each pod to terminate gracefully. If
negative, the default value specified in the pod will be used.
      --ignore-daemonsets=false: Ignore DaemonSet-managed pods.
      --pod-selector='': Label selector to filter pods on the node
  -l, --selector='': Selector (label query) to filter on
      --timeout=0s: The length of time to wait before giving up, zero means infinite

Use "oc adm options" for a list of global command-line options (applies to all commands).
[student@workstation schedule-is]$ 


[student@workstation schedule-is]$ oc adm drain node2.lab.example.com
node "node2.lab.example.com" already cordoned
error: unable to drain node "node2.lab.example.com", aborting command...

There are pending nodes to be drained:
 node2.lab.example.com
error: pods with local storage (use --delete-local-data to override): version-1-build

[student@workstation schedule-is]$ oc adm drain node2.lab.example.com --delete-local-data
node "node2.lab.example.com" already cordoned
WARNING: Deleting pods with local storage: version-1-build
pod "version-1-build" evicted
pod "version-2-cvlwc" evicted
pod "router-1-td587" evicted
pod "version-2-j9972" evicted
pod "version-2-x86md" evicted
node "node2.lab.example.com" drained
[student@workstation schedule-is]$ 



```

7.

```
[student@workstation schedule-is]$ oc get pods -o wide
NAME              READY     STATUS    RESTARTS   AGE       IP            NODE
version-2-28ngd   1/1       Running   0          35s       10.128.0.77   node1.lab.example.com
version-2-fkffh   1/1       Running   0          35s       10.128.0.78   node1.lab.example.com
version-2-jmnpb   1/1       Running   0          35s       10.128.0.76   node1.lab.example.com

```
8.

```
[student@workstation schedule-is]$ oc expose svc version --name=version.apps.lab.example.com
route "version.apps.lab.example.com" exposed
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ oc get route
NAME                           HOST/PORT                                                         PATH      SERVICES   PORT       TERMINATION   WILDCARD
version.apps.lab.example.com   version-apps-lab-example-com-manage-review.apps.lab.example.com             version    8080-tcp                 None

after Retry

[student@workstation schedule-is]$ oc get svc
NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
version   ClusterIP   172.30.230.191   <none>        8080/TCP   2m

[student@workstation schedule-is]$ oc expose svc version --hostname=version.apps.lab.example.com
route "version" exposed
[student@workstation schedule-is]$ oc vet route
Error: unknown command "vet" for "oc"

Did you mean this?
	get
	set
	ex

Run 'oc --help' for usage.
[student@workstation schedule-is]$ oc get route
NAME      HOST/PORT                      PATH      SERVICES   PORT       TERMINATION   WILDCARD
version   version.apps.lab.example.com             version    8080-tcp                 None


```

9.

```
[student@workstation schedule-is]$ oc get route
NAME      HOST/PORT                      PATH      SERVICES   PORT       TERMINATION   WILDCARD
version   version.apps.lab.example.com             version    8080-tcp                 None
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ curl http://version.apps.lab.example.com
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <p>Version v1</p> 
 </body>
</html>
[student@workstation schedule-is]$ 

```

10.

```
[student@workstation schedule-is]$ lab manage-review grade

Grading the student's work for Lab: Managing Application Deployments


Grading the lab.

Checking the manage-review project.............................  PASS
Check the labels from the node 1...............................  PASS
Check the labels from the node 2...............................  PASS
Checking the pod scale.........................................  PASS

Overall exercise grade.........................................  PASS

```

11.

```
[student@workstation schedule-is]$ oc label node node1.example.com region=infra --overwrite=true
Error from server (NotFound): nodes "node1.example.com" not found

[student@workstation schedule-is]$ oc label node node1.lab.example.com region=infra --overwrite=true
node "node1.lab.example.com" labeled

[student@workstation schedule-is]$ 

[student@workstation schedule-is]$ oc label node node2.lab.example.com region=infra --overwrite=true
node "node2.lab.example.com" labeled

[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ oc get nodes -L region
NAME                     STATUS                     ROLES     AGE       VERSION             REGION
master.lab.example.com   Ready                      master    1d        v1.9.1+a0ce1bc657   
node1.lab.example.com    Ready                      compute   1d        v1.9.1+a0ce1bc657   infra
node2.lab.example.com    Ready,SchedulingDisabled   compute   1d        v1.9.1+a0ce1bc657   infra

```

--------------------------------------------------------------------


# CHAPTER 8 INSTALLING AND CONFIGURING THE METRICS SUBSYSTEM(271)

3.11以降から、Metrics Subsystem はPrometheusがGA。
3.9はHawkular


InstallはAnsible!!!

```
[student@workstation schedule-is]$ cat /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml 
---
- import_playbook: ../init/main.yml
  vars:
    l_init_fact_hosts: "oo_masters_to_config"
    l_openshift_version_set_hosts: "oo_masters_to_config:!oo_first_master"
    l_openshift_version_check_hosts: "all:!all"
    l_sanity_check_hosts: "{{ groups['oo_masters_to_config'] }}"


- import_playbook: private/config.yml


```

## Guide Exercise: Installing the Metrics(P286)

- pre 

```
[student@workstation schedule-is]$ lab install-metrics setup

Checking prerequisites for GE: Installing the Metrics Subsystem

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS

Downloading files for GE: Installing the Metrics Subsystem

 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS


```



1.

```
[student@workstation schedule-is]$ docker-registry-cli registry.lab.example.com search metrics-cassandra ssl
available options:- 

-----------
1) Name: openshift3/ose-metrics-cassandra
Tags: latest	v3.9	

1 images found !
[student@workstation schedule-is]$ docker-registry-cli registry.lab.example.com search metrics-hawkular-metrics ssl
available options:- 

-----------
1) Name: openshift3/ose-metrics-hawkular-metrics
Tags: latest	v3.9	

1 images found !
[student@workstation schedule-is]$ docker-registry-cli registry.lab.example.com search metrics-heapster ssl
available options:- 

-----------
1) Name: openshift3/ose-metrics-heapster
Tags: latest	v3.9	

1 images found !
[student@workstation schedule-is]$
```

2.

```
[student@workstation schedule-is]$ docker-registry-cli registry.lab.example.com search ose-recycler ssl
available options:- 

-----------
1) Name: openshift3/ose-recycler
Tags: latest	v3.9	

1 images found !

```

3.

```
[root@services ~]# showmount -e
Export list for services.lab.example.com:
/var/export/dbvol                *
/var/export/review-dbvol         *
/exports/prometheus-alertbuffer  *
/exports/prometheus-alertmanager *
/exports/prometheus              *
/exports/etcd-vol2               *
/exports/logging-es-ops          *
/exports/logging-es              *
/exports/metrics                 *
/exports/registry                *
[root@services ~]# ls -alZ /exports/metrics/
drwxrwxrwx. nfsnobody nfsnobody unconfined_u:object_r:default_t:s0 .
drwxr-xr-x. root      root      unconfined_u:object_r:default_t:s0 ..

[root@services ~]# cat /etc/exports.d/openshift-ansible.exports 
"/exports/registry" *(rw,root_squash)
"/exports/metrics" *(rw,root_squash)
"/exports/logging-es" *(rw,root_squash)
"/exports/logging-es-ops" *(rw,root_squash)
"/exports/etcd-vol2" *(rw,root_squash,sync,no_wdelay)
"/exports/prometheus" *(rw,root_squash)
"/exports/prometheus-alertmanager" *(rw,root_squash)
"/exports/prometheus-alertbuffer" *(rw,root_squash)
[root@services ~]# 


```
4.

```
[student@workstation schedule-is]$ cat /home/student/DO280/labs/install-metrics/metrics-pv.yml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: metrics
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: /exports/metrics
    server: services.lab.example.com
  persistentVolumeReclaimPolicy: Recycle


[student@workstation schedule-is]$ oc create -f /home/student/DO280/labs/install-metrics/metrics-pv.yml 
persistentvolume "metrics" created


[student@workstation schedule-is]$ oc get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                   STORAGECLASS   REASON    AGE
etcd-vol2-volume   1G         RWO            Retain           Bound       openshift-ansible-service-broker/etcd                            1d
metrics            5Gi        RWO            Recycle          Available                                                                    3s
registry-volume    40Gi       RWX            Retain           Bound       default/registry-claim                                           1d


```
5.

```
[student@workstation install-metrics]$ cp ../../solutions/install-metrics/inventory .
[student@workstation install-metrics]$ more inventory 
[workstations]
workstation.lab.example.com

[nfs]
services.lab.example.com

[masters]
master.lab.example.com

[etcd]
master.lab.example.com

[nodes]
master.lab.example.com
node1.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"
node2.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"

[OSEv3:children]
masters
etcd
nodes
nfs

[nodes:vars]
registry_local=registry.lab.example.com
use_overlay2_driver=true
insecure_registry=false
run_docker_offline=true
docker_storage_device=/dev/vdb


[OSEv3:vars]
# General Variables
openshift_disable_check=disk_availability,docker_storage,memory_availability
openshift_deployment_type=openshift-enterprise
openshift_release=v3.9
openshift_image_tag=v3.9.14

# OpenShift Networking Variables
os_firewall_use_firewalld=true
openshift_master_api_port=443
openshift_master_console_port=443
openshift_master_default_subdomain=apps.lab.example.com

# Cluster Authentication Variables
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'H
TPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
openshift_master_htpasswd_users={'admin': '$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611', 'developer': '$apr1$4ZbKL26l
$3eKL/6AQM8O94lRwTAu611'}

# Need to enable NFS
openshift_enable_unsupported_configurations=true

# Registry Configuration Variables
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=40Gi

# etcd Configuration Variables
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/exports
openshift_hosted_etcd_storage_volume_name=etcd-vol2
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

# Modifications Needed for a Disconnected Install
oreg_url=registry.lab.example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
openshift_docker_additional_registries=registry.lab.example.com
openshift_docker_blocked_registries=registry.access.redhat.com,docker.io
openshift_web_console_prefix=registry.lab.example.com/openshift3/ose-
openshift_cockpit_deployer_prefix='registry.lab.example.com/openshift3/'
openshift_service_catalog_image_prefix=registry.lab.example.com/openshift3/ose-
template_service_broker_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_image_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_etcd_image_prefix=registry.lab.example.com/rhel7/

# Metrics Variables
# Append the variables to the [OSEv3:vars] group
openshift_metrics_install_metrics=True
openshift_metrics_image_prefix=registry.lab.example.com/openshift3/ose-
openshift_metrics_image_version=v3.9
openshift_metrics_heapster_requests_memory=300M
openshift_metrics_hawkular_requests_memory=750M
openshift_metrics_cassandra_requests_memory=750M
openshift_metrics_cassandra_storage_type=pv
openshift_metrics_cassandra_pvc_size=5Gi
openshift_metrics_cassandra_pvc_prefix=metrics
[student@workstation install-metrics]$ 


[student@workstation install-metrics]$ lab install-metrics grade
 · Detecting solution inventory................................  PASS
 · Detecting student inventory.................................  PASS

Comparing Entries in [OSEv3:children]

 · Checking masters............................................  PASS
 · Checking etcd...............................................  PASS
 · Checking nodes..............................................  PASS
 · Checking nfs................................................  PASS

Comparing Entries in [OSEv3:vars]

 · Checking openshift_disable_check............................  PASS
 · Checking openshift_deployment_type..........................  PASS
 · Checking openshift_release..................................  PASS
 · Checking openshift_image_tag................................  PASS
 · Checking os_firewall_use_firewalld..........................  PASS
 · Checking openshift_master_api_port..........................  PASS
 · Checking openshift_master_console_port......................  PASS
 · Checking openshift_master_default_subdomain.................  PASS
 · Checking openshift_master_identity_providers................  PASS
 · Skipping openshift_master_htpasswd_users....................  PASS
 · Checking openshift_enable_unsupported_configurations........  PASS
 · Checking openshift_hosted_registry_storage_kind.............  PASS
 · Checking openshift_hosted_registry_storage_access_mode......  PASS
 · Checking openshift_hosted_registry_storage_nfs_directo......  PASS
 · Checking openshift_hosted_registry_storage_nfs_options......  PASS
 · Checking openshift_hosted_registry_storage_volume_name......  PASS
 · Checking openshift_hosted_registry_storage_volume_size......  PASS
 · Checking openshift_hosted_etcd_storage_kind.................  PASS
 · Checking openshift_hosted_etcd_storage_nfs_options..........  PASS
 · Checking openshift_hosted_etcd_storage_nfs_directory........  PASS
 · Checking openshift_hosted_etcd_storage_volume_name..........  PASS
 · Checking openshift_hosted_etcd_storage_access_modes.........  PASS
 · Checking openshift_hosted_etcd_storage_volume_size..........  PASS
 · Checking openshift_hosted_etcd_storage_labels...............  PASS
 · Checking oreg_url...........................................  PASS
 · Checking openshift_examples_modify_imagestreams.............  PASS
 · Checking openshift_docker_additional_registries.............  PASS
 · Checking openshift_docker_blocked_registries................  PASS
 · Checking openshift_web_console_prefix.......................  PASS
 · Checking openshift_cockpit_deployer_prefix..................  PASS
 · Checking openshift_service_catalog_image_prefix.............  PASS
 · Checking template_service_broker_prefix.....................  PASS
 · Checking ansible_service_broker_image_prefix................  PASS
 · Checking ansible_service_broker_etcd_image_prefix...........  PASS
 · Checking openshift_metrics_install_metrics..................  PASS
 · Checking openshift_metrics_image_prefix.....................  PASS
 · Checking openshift_metrics_image_version....................  PASS
 · Checking openshift_metrics_heapster_requests_memory.........  PASS
 · Checking openshift_metrics_hawkular_requests_memory.........  PASS
 · Checking openshift_metrics_cassandra_requests_memory........  PASS
 · Checking openshift_metrics_cassandra_storage_type...........  PASS
 · Checking openshift_metrics_cassandra_pvc_size...............  PASS
 · Checking openshift_metrics_cassandra_pvc_prefix.............  PASS

Comparing Entries in [etcd]

 · Checking master.lab.example.com.............................  PASS

Comparing Entries in [masters]

 · Checking master.lab.example.com.............................  PASS

Comparing Entries in [nfs]

 · Checking services.lab.example.com...........................  PASS

Comparing Entries in [nodes]

 · Checking master.lab.example.com.............................  PASS
 · Checking node1.lab.example.com openshift_node_labels........  PASS
 · Checking node2.lab.example.com openshift_node_labels........  PASS

Overall inventory file check: .................................  PASS




[student@workstation install-metrics]$ ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-metrics/config.yml



PLAY RECAP ****************************************************************************************************
localhost                  : ok=12   changed=0    unreachable=0    failed=0   
master.lab.example.com     : ok=212  changed=47   unreachable=0    failed=0   
node1.lab.example.com      : ok=0    changed=0    unreachable=0    failed=0   
node2.lab.example.com      : ok=0    changed=0    unreachable=0    failed=0   
services.lab.example.com   : ok=1    changed=0    unreachable=0    failed=0   
workstation.lab.example.com : ok=4    changed=0    unreachable=0    failed=0   


INSTALLER STATUS **********************************************************************************************
Initialization             : Complete (0:00:12)
Metrics Install            : Complete (0:01:34)




```


6.


```
[student@workstation install-metrics]$ oc get pvc -n openshift-infra
NAME        STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
metrics-1   Bound     metrics   5Gi        RWO     

                      2m
[student@workstation install-metrics]$ oc get pod -n openshift-infra
NAME                         READY     STATUS    RESTARTS   AGE
hawkular-cassandra-1-fb65r   1/1       Running   0          2m
hawkular-metrics-c2dkq       1/1       Running   0          2m
heapster-p9bt8               1/1       Running   0          2m

```

8.

```
[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".
[student@workstation ~]$ oc new-project load
Now using project "load" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation ~]$ oc new-app --name=hello --docker-image=registry.lab.example.com/openshift/hello-openshift
--> Found Docker image 7af3297 (17 months old) from registry.lab.example.com for "registry.lab.example.com/openshift/hello-openshift"

    * An image stream will be created as "hello:latest" that will track this image
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation ~]$ 

[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-x2qdn   1/1       Running   0          23s
[student@workstation ~]$ oc scale --replicas=9 dc/hello
deploymentconfig "hello" scaled
[student@workstation ~]$ oc get pods
NAME            READY     STATUS              RESTARTS   AGE
hello-1-25mbp   1/1       Running             0          3s
hello-1-7w8wl   0/1       Pending             0          3s
hello-1-d4kvm   1/1       Running             0          3s
hello-1-hjrct   0/1       Pending             0          3s
hello-1-m5c2n   0/1       ContainerCreating   0          3s
hello-1-pdx55   0/1       Pending             0          3s
hello-1-rjhdg   0/1       ContainerCreating   0          3s
hello-1-x2qdn   1/1       Running             0          41s
hello-1-ztn4p   0/1       Pending             0          3s

9こあがらなかったので原因確認

[student@workstation ~]$ oc get nodes
NAME                     STATUS                     ROLES     AGE       VERSION
master.lab.example.com   Ready                      master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready                      compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready,SchedulingDisabled   compute   1d        v1.9.1+a0ce1bc657

あー、これだ。。。


[student@workstation ~]$ oc adm manage-node node2.lab.example.com --schedulable=true
NAME                    STATUS    ROLES     AGE       VERSION
node2.lab.example.com   Ready     compute   1d        v1.9.1+a0ce1bc657
[student@workstation ~]$ 
[student@workstation ~]$ 

修正！！


[student@workstation ~]$ oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    1d        v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   1d        v1.9.1+a0ce1bc657



[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-1-25mbp   1/1       Running   0          2m        10.128.0.100   node1.lab.example.com
hello-1-7w8wl   1/1       Running   0          2m        10.129.0.156   node2.lab.example.com
hello-1-d4kvm   1/1       Running   0          2m        10.128.0.99    node1.lab.example.com
hello-1-hjrct   1/1       Running   0          2m        10.129.0.155   node2.lab.example.com
hello-1-m5c2n   1/1       Running   0          2m        10.128.0.98    node1.lab.example.com
hello-1-pdx55   1/1       Running   0          2m        10.129.0.157   node2.lab.example.com
hello-1-rjhdg   1/1       Running   0          2m        10.128.0.101   node1.lab.example.com
hello-1-x2qdn   1/1       Running   0          3m        10.128.0.97    node1.lab.example.com
hello-1-ztn4p   1/1       Running   0          2m        10.129.0.154   node2.lab.example.com

いけたー

[student@workstation ~]$ oc expose svc hello
route "hello" exposed
[student@workstation ~]$ oc get route
NAME      HOST/PORT                         PATH      SERVICES   PORT       TERMINATION   WILDCARD
hello     hello-load.apps.lab.example.com             hello      8080-tcp                 None
[student@workstation ~]$ 

student@workstation ~]$ sudo yum install httpd-tools
Loaded plugins: langpacks, product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
rhel--server-dvd                                                                                                                                            | 4.3 kB  00:00:00     
rhel-7-server-ansible-24                                                                                                                                    | 2.9 kB  00:00:00     
rhel-7-server-common-rpms                                                                                                                                   | 2.9 kB  00:00:00     
rhel-7-server-datapath-rpms                                                                                                                                 | 2.9 kB  00:00:00     
rhel-7-server-extras-rpms                                                                                                                                   | 2.9 kB  00:00:00     
rhel-7-server-optional-rpms                                                                                                                                 | 2.9 kB  00:00:00     
rhel-7-server-ose-3.9-rpms                                                                                                                                  | 2.9 kB  00:00:00     
rhel-7-server-supplementary                                                                                                                                 | 2.9 kB  00:00:00     
rhel-7-server-updates                                                                                                                                       | 2.9 kB  00:00:00     
rhel-server-rhscl-7-rpms                                                                                                                                    | 2.9 kB  00:00:00     
Package httpd-tools-2.4.6-80.el7.x86_64 already installed and latest version
Nothing to do
[student@workstation ~]$ 
[student@workstation ~]$ 



```
9.
```
[student@workstation ~]$ oc adm top node --heapster-namespace=openshift-infra --heapster-scheme=https
NAME                     CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%   
master.lab.example.com   169m         8%        1369Mi          78%       
node1.lab.example.com    1829m        91%       4608Mi          59%       
node2.lab.example.com    867m         43%       2337Mi          30%       


```


10.

```
[student@workstation ~]$ cat ~/DO280/labs/install-metrics/node-metrics.sh 
#!/bin/bash

oc login -u admin -p redhat >/dev/null

TOKEN=$(oc whoami -t)
APIPROXY=https://master.lab.example.com:/api/v1/proxy/namespaces/openshift-infra/services
HEAPSTER=https:heapster:/api/v1/model
NODE=nodes/node1.lab.example.com
START=$(date -d '1 minute ago' -u '+%FT%TZ')

curl -kH "Authorization: Bearer $TOKEN" \
 -X GET $APIPROXY/$HEAPSTER/$NODE/metrics/memory/working_set?start=$START

curl -kH "Authorization: Bearer $TOKEN" \
 -X GET $APIPROXY/$HEAPSTER/$NODE/metrics/cpu/usage_rate?start=$START


[student@workstation ~]$ ~/DO280/labs/install-metrics/node-metrics.sh 
{
  "metrics": [
   {
    "timestamp": "2019-09-26T05:08:00Z",
    "value": 4827512832
   },
   {
    "timestamp": "2019-09-26T05:08:30Z",
    "value": 4822228992
   }
  ],
  "latestTimestamp": "2019-09-26T05:08:30Z"
 }{
  "metrics": [
   {
    "timestamp": "2019-09-26T05:08:00Z",
    "value": 1027
   },
   {
    "timestamp": "2019-09-26T05:08:30Z",
    "value": 203
   }
  ],
  "latestTimestamp": "2019-09-26T05:08:30Z"

```


11.


```
student@workstation ~]$ 
[student@workstation ~]$ oc delete project load
project "load" deleted

```

番外編
これを最初にやると、結構違うんじゃないか。。？

```
[student@workstation ~]$ docker-registry-cli registry.lab.example.com search php ssl
available options:- 

-----------
1) Name: phpmyadmin/phpmyadmin
Tags: 4.7	latest	
-----------
2) Name: rhscl/php-56-rhel7
Tags: latest	
-----------
3) Name: rhscl/php-70-rhel7
Tags: 7.0-5.12	latest	

3 images found !

```

```
[student@workstation ~]$ oc get is php -n openshift -o wide
NAME      DOCKER REPO                                      TAGS                      UPDATED
php       docker-registry.default.svc:5000/openshift/php   5.6,7.0,5.5 + 1 more...   2 days ago


[student@workstation secure-route]$ oc describe is php -n openshift

[student@workstation ~]$ oc describe is php -n openshift
Name:			php
Namespace:		openshift
Created:		2 days ago
Labels:			<none>
Annotations:		openshift.io/display-name=PHP
			openshift.io/image.dockerRepositoryCheck=2019-09-24T05:11:45Z
Docker Pull Spec:	docker-registry.default.svc:5000/openshift/php
Image Lookup:		local=false
Unique Images:		2
Tags:			5

7.1 (latest)
  tagged from registry.lab.example.com/rhscl/php-71-rhel7:latest

  Build and run PHP 7.1 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.1/README.md.
  Tags: builder, php
  Supports: php:7.1, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/rhscl/php-71-rhel7:latest" not found
      2 days ago

7.0
  tagged from registry.lab.example.com/rhscl/php-70-rhel7:latest

  Build and run PHP 7.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.0/README.md.
  Tags: builder, php
  Supports: php:7.0, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd
      2 days ago

5.6
  tagged from registry.lab.example.com/rhscl/php-56-rhel7:latest

  Build and run PHP 5.6 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.6/README.md.
  Tags: builder, php
  Supports: php:5.6, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-56-rhel7@sha256:920c2cf85b5da5d0701898f0ec9ee567473fa4b9af6f3ac5b2b3f863796bbd68
      2 days ago

5.5
  tagged from registry.lab.example.com/openshift3/php-55-rhel7:latest

  Build and run PHP 5.5 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.5/README.md.
  Tags: hidden, builder, php
  Supports: php:5.5, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/openshift3/php-55-rhel7:latest" not found
      2 days ago

```

---------------------------------------------------------------
# CHAPTER 9 MANAGING AND MONITORING OPENSHIFT CONTAINER PLATFORM(P299)

## Limiting Resource Usage(300)

Limit Range
requestされるリソース1件に対して許可できる範囲
MAX
MIN
DEFAULT

QUOTA
PROJECT全体の最大リソースサイズ。

request CPU1 mem512MB
Limit Range CPU1 mem512
QUOTA CPU4 mem4Gi
-> QUOTA CPU3 mem3.5Gi

QUOTAから減るのは予約値。


## Guided Exercise: Limiting Resource Usage(P307)

- pre 

```
[student@workstation ~]$ lab monitor-limit setup

Checking prerequisites for GE: Limiting Resource Usage

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS

Downloading files for GE: Limiting Resource Usage

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS


```

1.

```
[student@workstation ~]$ oc login -u admin -p redhat https://master.lab.example.com
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    manage-review
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review

Using project "default".

[student@workstation ~]$ oc whoami
admin


[student@workstation ~]$ oc describe node node1.lab.example.com | grep -A 4 Allocated
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests   Memory Limits
  ------------  ----------  ---------------   -------------
  300m (15%)    0 (0%)      3142177280 (38%)  8786870912 (108%)

[student@workstation ~]$ oc describe node node2.lab.example.com | grep -A 4 Allocated
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  100m (5%)     0 (0%)      256Mi (3%)       0 (0%)


[student@workstation ~]$ oc new-project resources
Now using project "resources" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


[student@workstation ~]$ oc new-app --name=hello --docker-image=repositry.lab.example.com/openshift/hello-openshift
W0926 14:52:58.288442   32487 dockerimagelookup.go:233] Docker registry lookup failed: Get https://repositry.lab.example.com/v2/: dial tcp: lookup repositry.lab.example.com on 172.25.250.10:53: no such host
error: unable to locate any local docker images with name "repositry.lab.example.com/openshift/hello-openshift"

The 'oc new-app' command will match arguments to the following types:

  1. Images tagged into image streams in the current project or the 'openshift' project
     - if you don't specify a tag, we'll add ':latest'
  2. Images in the Docker Hub, on remote registries, or on the local Docker engine
  3. Templates in the current project or the 'openshift' project
  4. Git repository URLs or local paths that point to Git repositories

--allow-missing-images can be used to point to an image that does not exist yet.

See 'oc new-app -h' for examples.

TYPO~!!

[student@workstation ~]$ oc new-app --name=hello --docker-image=registry.lab.example.com/openshift/hello-openshift
--> Found Docker image 7af3297 (17 months old) from registry.lab.example.com for "registry.lab.example.com/openshift/hello-openshift"

    * An image stream will be created as "hello:latest" that will track this image
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.


[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-1-qw5zm   1/1       Running   0          18s       10.129.0.159   node2.lab.example.com

[student@workstation ~]$ oc describe node node1.lab.example.com | grep -A 4 Allocated
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests   Memory Limits
  ------------  ----------  ---------------   -------------
  300m (15%)    0 (0%)      3142177280 (38%)  8786870912 (108%)

[student@workstation ~]$ oc describe pod hello
Name:           hello-1-qw5zm
Namespace:      resources
Node:           node2.lab.example.com/172.25.250.12
Start Time:     Thu, 26 Sep 2019 14:53:46 +0900
Labels:         app=hello
                deployment=hello-1
                deploymentconfig=hello
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=hello
                openshift.io/deployment.name=hello-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Running
IP:             10.129.0.159
Controlled By:  ReplicationController/hello-1
Containers:
  hello:
    Container ID:   docker://a1d1b2523daf7254dbd51542e0d83711a2fd3e83299a636459f33adc7a6ba30a
    Image:          registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    Image ID:       docker-pullable://registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    Ports:          8080/TCP, 8888/TCP
    State:          Running
      Started:      Thu, 26 Sep 2019 14:53:48 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jr6sc (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-jr6sc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-jr6sc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              2m    default-scheduler               Successfully assigned hello-1-qw5zm to node2.lab.example.com
  Normal  SuccessfulMountVolume  2m    kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-jr6sc"
  Normal  Pulling                2m    kubelet, node2.lab.example.com  pulling image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
  Normal  Pulled                 2m    kubelet, node2.lab.example.com  Successfully pulled image "registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e"
  Normal  Created                2m    kubelet, node2.lab.example.com  Created container
  Normal  Started                2m    kubelet, node2.lab.example.com  Started container


[student@workstation ~]$ oc delete all -l app=hello
deploymentconfig "hello" deleted
imagestream "hello" deleted
pod "hello-1-qw5zm" deleted
service "hello" deleted
[student@workstation ~]$ 


```


2.

```
[student@workstation ~]$ cat ~/DO280/labs/monitor-limit/limits.yml 
apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "project-limits"
spec:
  limits:
    - type: "Container"
      default:
        cpu: "250m"


[student@workstation ~]$ oc create -f  ~/DO280/labs/monitor-limit/limits.yml 
limitrange "project-limits" created


[student@workstation ~]$ oc describe limits
Name:       project-limits
Namespace:  resources
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    250m             250m           -


[student@workstation ~]$ cat ~/DO280/labs/monitor-limit/quota.yml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: project-quota
spec:
  hard:
    cpu: "900m"


[student@workstation ~]$ oc create -f  ~/DO280/labs/monitor-limit/quota.yml 
resourcequota "project-quota" created


[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         0     900m
[student@workstation ~]$ 

student@workstation ~]$ oc project
Using project "resources" on server "https://master.lab.example.com:443".

[student@workstation ~]$ oc adm policy add-role-to-user edit developer
role "edit" added: "developer"


```

3.

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-limit/add-pod.sh 
#!/bin/bash -x

oc login -u developer -p redhat >/dev/null

oc project resources

oc get limits

oc delete limits project-limits

oc get quota

oc delete quota project-quota

oc new-app --name=hello \
--docker-image=registry.lab.example.com/openshift/hello-openshift


ok="no"
while [ "$ok" != "yes" ]
do
  sleep 3
  oc get pod

  echo -n "Type 'yes' if the pod is ready and running..."
  read ok
done

oc describe quota



[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * resources

Using project "resources".


[student@workstation ~]$ oc project resources
Already on project "resources" on server "https://master.lab.example.com:443".
[student@workstation ~]$ 

[student@workstation ~]$ oc get limits
NAME             AGE
project-limits   3m
[student@workstation ~]$ 

[student@workstation ~]$ oc delete limits project-limits 
Error from server (Forbidden): limitranges "project-limits" is forbidden: User "developer" cannot delete limitranges in the namespace "resources": User "developer" cannot delete limitranges in project "resources"

[student@workstation ~]$ 

[student@workstation ~]$ oc get quota
NAME            AGE
project-quota   3m
[student@workstation ~]$ oc get quota -o wide
NAME            AGE
project-quota   3m
[student@workstation ~]$ oc delete quota project-quota
Error from server (Forbidden): resourcequotas "project-quota" is forbidden: User "developer" cannot delete resourcequotas in the namespace "resources": User "developer" cannot delete resourcequotas in project "resources"
[student@workstation ~]$ 


開発者権限ではQUOTA/LIMITの制限は変えられない！！

[student@workstation ~]$ oc get limitranges
NAME             AGE
project-limits   5m
[student@workstation ~]$ 

[student@workstation ~]$ oc new-app --name=hello --docker-image=registry.lab.example.com/openshift/hello-openshift
--> Found Docker image 7af3297 (17 months old) from registry.lab.example.com for "registry.lab.example.com/openshift/hello-openshift"

    * An image stream will be created as "hello:latest" that will track this image
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.


[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-1-kjdng   1/1       Running   0          16s       10.129.0.161   node2.lab.example.com
[student@workstation ~]$ 

[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         250m  900m
[student@workstation ~]$ 


```


4.

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-limit/check-nodes.sh 
#!/bin/bash -x

oc login -u admin -p redhat >/dev/null

oc get pod -o wide -n resources

# Check both nodes because the script doesn not know in which one
# the pod was allocated to run

oc describe node node1.lab.example.com \
    | grep -A 4 Allocated

oc describe node node2.lab.example.com \
    | grep -A 4 Allocated

# trick to get the hello pod name
pod=$(oc get pod -n resources -o name | grep -v hello | grep -v build | grep -v deploy)

oc describe pod ${pod} -n resources | grep -A 2 Requests

oc login -u developer -p redhat

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    manage-review
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * resources
    secure-review

Using project "resources".
[student@workstation ~]$ 


[student@workstation ~]$ oc get pod -o wide -n resources
NAME            READY     STATUS    RESTARTS   AGE       IP             NODE
hello-1-kjdng   1/1       Running   0          2m        10.129.0.161   node2.lab.example.com

[student@workstation ~]$ oc describe node node1.lab.example.com | grep -A 4 Allocated
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests   Memory Limits
  ------------  ----------  ---------------   -------------
  300m (15%)    0 (0%)      3142177280 (38%)  8786870912 (108%)


[student@workstation ~]$ oc describe node node2.lab.example.com | grep -A 4 Allocated
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  350m (17%)    250m (12%)  256Mi (3%)       0 (0%)



[student@workstation ~]$ oc describe pod hello-1-kjdng | grep -A2 Requests
    Requests:
      cpu:        250m
    Environment:  <none>
[student@workstation ~]$ 

[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * resources

Using project "resources".
[student@workstation ~]$ 
[student@workstation ~]$ oc whoami
developer
[student@workstation ~]$ 


```

5.

```

[student@workstation ~]$ cat ~/DO280/labs/monitor-limit/increase-bounded.sh 
#!/bin/bash -x

oc scale dc hello --replicas=2

ok="no"
while [ "$ok" != "yes" ]
do
  sleep 3
  oc get pod

  echo -n "Type 'yes' if two pods are ready and running..."
  read ok
done

oc describe quota
oc scale dc hello --replicas=4

ok="no"
while [ "$ok" != "yes" ]
do
  sleep 3
  oc get pod

  echo -n "Type 'yes' if three pods are ready and running."
  read ok
done

oc describe dc hello | grep Replicas
oc get events | grep -i error
oc scale dc hello --replicas=1

sleep 3

oc get pod


[student@workstation ~]$ oc scale dc hello --replicas=2
deploymentconfig "hello" scaled


[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-j8bvq   1/1       Running   0          2s
hello-1-kjdng   1/1       Running   0          5m

[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         500m  900m
[student@workstation ~]$ 


[student@workstation ~]$ oc scale dc hello --replicas=4
deploymentconfig "hello" scaled
[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-857w5   1/1       Running   0          4s
hello-1-j8bvq   1/1       Running   0          44s
hello-1-kjdng   1/1       Running   0          6m


[student@workstation ~]$ oc describe dc hello | grep Replicas
Replicas:	4
	Replicas:	3 current / 4 desired

[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         750m  900m


[student@workstation ~]$ oc get events | grep -i error
1m          1m           1         hello-1.15c7e839df4752bb          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-4485m" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839dfc85715          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-bwbk8" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839e05c6c20          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-44f6s" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839e0e8c249          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-xwvsx" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839e3844052          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-kbdbq" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839e8848687          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-2pkzd" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e839f250694b          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-xv7jk" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e83a059b9733          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-qrstd" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
1m          1m           1         hello-1.15c7e83a2bfe7760          ReplicationController                                 Warning   FailedCreate                  replication-controller           Error creating: pods "hello-1-4ffpv" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
5s          1m           6         hello-1.15c7e83a789a64bc          ReplicationController                                 Warning   FailedCreate                  replication-controller           (combined from similar events): Error creating: pods "hello-1-cdvxr" is forbidden: exceeded quota: project-quota, requested: cpu=250m, used: cpu=750m, limited: cpu=900m
[student@workstation ~]$ 


student@workstation ~]$ oc scale dc hello --replicas=1
deploymentconfig "hello" scaled

[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-kjdng   1/1       Running   0          7m



```

6.

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-limit/increase-unbounded.sh 
#!/bin/bash -x

oc set resources dc hello --requests=memory=256Mi

ok="no"
while [ "$ok" != "yes" ]
do
  sleep 3
  oc get pod

  echo -n "Type 'yes' if the pod is ready and running."
  read ok
done

# trick to get the hello pod name
pod=$(oc get pod -n resources -o name | grep -v hello | grep -v build | grep -v deploy)

oc describe pod ${pod} | grep -A 3 Requests
oc describe quota


[student@workstation ~]$ oc set resources dc hello --requests=memory=256Mi
deploymentconfig "hello" resource requirements updated

[student@workstation ~]$ oc get pods
NAME             READY     STATUS              RESTARTS   AGE
hello-1-kjdng    1/1       Running             0          9m
hello-2-deploy   1/1       Running             0          5s
hello-2-hmwbb    0/1       ContainerCreating   0          1s

[student@workstation ~]$ oc get pods
NAME             READY     STATUS        RESTARTS   AGE
hello-1-kjdng    0/1       Terminating   0          9m
hello-2-deploy   1/1       Running       0          8s
hello-2-hmwbb    1/1       Running       0          4s

[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-2-hmwbb   1/1       Running   0          7s


[student@workstation ~]$ oc describe pod hello-2-hmwbb | grep -A 3 Requests
    Requests:
      cpu:        250m
      memory:     256Mi
    Environment:  <none>


[student@workstation ~]$ oc describe quota
Name:       project-quota
Namespace:  resources
Resource    Used  Hard
--------    ----  ----
cpu         250m  900m
[student@workstation ~]$ 


```

7.

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-limit/increase-toomuch.sh 
#!/bin/bash -x

oc set resources dc hello --requests=memory=8Gi

ok="no"
while [ "$ok" != "yes" ]
do
  sleep 3
  oc get pod

  echo -n "Type 'yes' to proceed."
  read ok
done

oc get events | grep hello-3.*Failed
[student@workstation ~]$ 

[student@workstation ~]$ /home/student/DO280/labs/monitor-limit/increase-toomuch.sh 
+ oc set resources dc hello --requests=memory=8Gi
deploymentconfig "hello" resource requirements updated
+ ok=no
+ '[' no '!=' yes ']'
+ sleep 3
+ oc get pod
NAME             READY     STATUS    RESTARTS   AGE
hello-2-hmwbb    1/1       Running   0          19m
hello-3-deploy   1/1       Running   0          3s


[student@workstation ~]$ oc get pod
NAME             READY     STATUS    RESTARTS   AGE
hello-2-hmwbb    1/1       Running   0          20m
hello-3-deploy   1/1       Running   0          1m
hello-3-tkgzf    0/1       Pending   0          1m

[student@workstation ~]$ oc get pod
NAME             READY     STATUS    RESTARTS   AGE
hello-2-hmwbb    1/1       Running   0          21m
hello-3-deploy   1/1       Running   0          1m
hello-3-tkgzf    0/1       Pending   0          1m

[student@workstation ~]$ oc logs hello-3-deploy
--> Scaling up hello-3 from 0 to 1, scaling down hello-2 from 1 to 0 (keep 1 pods available, don't exceed 2 pods)
    Scaling hello-3 up to 1


[student@workstation ~]$ oc status
In project resources on server https://master.lab.example.com:443

svc/hello - 172.30.206.215 ports 8080, 8888
  dc/hello deploys istag/hello:latest 
    deployment #3 running for 2 minutes - 0/1 pods
    deployment #2 deployed 22 minutes ago - 1 pod
    deployment #1 deployed 31 minutes ago


3 infos identified, use 'oc status -v' to see details.




```

cleanup

```
[student@workstation ~]$ oc delete project resources
project "resources" deleted
[student@workstation ~]$ oc pr
```



## Upgrading the Openshift Container Platform（317）

基本的に段階アップグレード。一気にはできない。





## Monitoring Applications with Probes（327）

RCがいるのでPODが落ちたら再起動する。
ただ、見ているのはPODがいるかいないか。
アプリがバグでフリーズしていたりとか、そういう場合には再起動はされない。
なので、PROBEという仕組みを入れる。

Liveness Probe 健全性チェック

Readiness Probe 準備チェック


GUIで設定できるものは限られている。
細かいのはYAML。

注意点として、PROBEはコンテナ自身からのチェックであり、
外部からのチェックではない点は注意すること。


## Guided Excersice : Monitoring Applications with Probes(332)

あとでやる。

- pre

```
[student@workstation ~]$ lab probes setup

Checking prerequisites for GE: Monitoring Applications with Probes

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS

 Checking all OpenShift default pods are ready and running:

 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Overall setup status...........................................  SUCCESS



```

1.

```

[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".


[student@workstation ~]$ oc new-project probes
Now using project "probes" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


```

2.

```
[student@workstation ~]$ oc new-app --name=probes http://services.lab.example.com/node-hello
--> Found Docker image fba56b5 (2 years old) from registry.lab.example.com for "registry.lab.example.com/rhscl/nodejs-6-rhel7"

    Node.js 6 
    --------- 
    Node.js 6 available as docker container is a base platform for building and running various Node.js 6 applications and frameworks. Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices.

    Tags: builder, nodejs, nodejs6

    * An image stream will be created as "nodejs-6-rhel7:latest" that will track the source image
    * A Docker build using source code from http://services.lab.example.com/node-hello will be created
      * The resulting image will be pushed to image stream "probes:latest"
      * Every time "nodejs-6-rhel7:latest" changes a new build will be triggered
    * This image will be deployed in deployment config "probes"
    * Port 3000 will be load balanced by service "probes"
      * Other containers can access this service through the hostname "probes"

--> Creating resources ...
    imagestream "nodejs-6-rhel7" created
    imagestream "probes" created
    buildconfig "probes" created
    deploymentconfig "probes" created
    service "probes" created
--> Success
    Build scheduled, use 'oc logs -f bc/probes' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/probes' 
    Run 'oc status' to view your app.
[student@workstation ~]$ 

[student@workstation ~]$ oc status
In project probes on server https://master.lab.example.com:443

svc/probes - 172.30.205.212:3000
  dc/probes deploys istag/probes:latest <-
    bc/probes docker builds http://services.lab.example.com/node-hello on istag/nodejs-6-rhel7:latest 
      build #1 running for 15 seconds - aaf02db: Establish remote repository (root <root@services.lab.example.com>)
    deployment #1 waiting on image or update


2 infos identified, use 'oc status -v' to see details.
[student@workstation ~]$ oc get pods
NAME             READY     STATUS    RESTARTS   AGE
probes-1-build   1/1       Running   0          22s

[student@workstation ~]$ oc get route
No resources found.

[student@workstation ~]$ oc get svc
NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
probes    ClusterIP   172.30.205.212   <none>        3000/TCP   27s

[student@workstation ~]$ oc get pods -w
NAME             READY     STATUS      RESTARTS   AGE
probes-1-8xsj2   1/1       Running     0          24s
probes-1-build   0/1       Completed   0          48s


```

3.

```
[student@workstation ~]$ oc expose svc probes --hostname=probe.apps.lab.example.com
route "probes" exposed

[student@workstation ~]$ oc get routes
NAME      HOST/PORT                    PATH      SERVICES   PORT       TERMINATION   WILDCARD
probes    probe.apps.lab.example.com             probes     3000-tcp                 None

```

4.

```
student@workstation ~]$ curl http://probe.apps.lab.example.com
Hi! I am running on host -> probes-1-8xsj2


```

5.

```
[student@workstation ~]$ curl http://probe.apps.lab.example.com/health
OK

[student@workstation ~]$ curl http://probe.apps.lab.example.com/ready
READY
[student@workstation ~]$ 

```

10.

```
[student@workstation ~]$ oc get events --sort-by='.metadata.creationTimestamp' | grep 'probe failed'
7s          1m           6         probes-3-f8xtt.15c7ead266880ab3    Pod                     spec.containers{probes}                  Warning   Unhealthy               kubelet, node2.lab.example.com   Liveness probe failed: HTTP probe failed with statuscode: 404

```

12.

```
[student@workstation ~]$ oc get events --sort-by='.metadata.creationTimestamp' 
LAST SEEN   FIRST SEEN   COUNT     NAME                               KIND                    SUBOBJECT                                TYPE      REASON                  SOURCE                           MESSAGE
10m         10m          1         probes-1-build.15c7ea76f6dfa308    Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-1-build to node2.lab.example.com
10m         10m          1         probes-1-build.15c7ea77035cf84d    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "docker-socket" 
10m         10m          1         probes-1-build.15c7ea77043d258f    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "builder-dockercfg-vqgkq-push" 
10m         10m          1         probes-1-build.15c7ea770431a562    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "builder-token-8hqml" 
10m         10m          1         probes-1-build.15c7ea77037763ab    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "buildworkdir" 
10m         10m          1         probes-1-build.15c7ea770360cf8f    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "crio-socket" 
9m          9m           1         probes-1-build.15c7ea7776c7021b    Pod                     spec.initContainers{git-clone}           Normal    Pulling                 kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/openshift3/ose-docker-builder:v3.9.14"
9m          9m           1         probes-1-build.15c7ea7782a5d27e    Pod                     spec.initContainers{git-clone}           Normal    Started                 kubelet, node2.lab.example.com   Started container
9m          9m           1         probes-1-build.15c7ea777ce5b415    Pod                     spec.initContainers{git-clone}           Normal    Created                 kubelet, node2.lab.example.com   Created container
9m          9m           1         probes-1-build.15c7ea777b5b817f    Pod                     spec.initContainers{git-clone}           Normal    Pulled                  kubelet, node2.lab.example.com   Successfully pulled image "registry.lab.example.com/openshift3/ose-docker-builder:v3.9.14"
9m          9m           1         probes-1-build.15c7ea77b8c6c6ff    Pod                     spec.initContainers{manage-dockerfile}   Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-docker-builder:v3.9.14" already present on machine
9m          9m           1         probes-1-build.15c7ea77ba694798    Pod                     spec.initContainers{manage-dockerfile}   Normal    Created                 kubelet, node2.lab.example.com   Created container
9m          9m           1         probes-1-build.15c7ea77c08d86dd    Pod                     spec.initContainers{manage-dockerfile}   Normal    Started                 kubelet, node2.lab.example.com   Started container
9m          9m           1         probes-1-build.15c7ea77fd6c5d58    Pod                     spec.containers{docker-build}            Normal    Started                 kubelet, node2.lab.example.com   Started container
9m          9m           1         probes-1.15c7ea7822275e8c          Build                                                            Normal    BuildStarted            build-controller                 Build probes/probes-1 is now running
9m          9m           1         probes-1-build.15c7ea77f7a52156    Pod                     spec.containers{docker-build}            Normal    Created                 kubelet, node2.lab.example.com   Created container
9m          9m           1         probes-1-build.15c7ea77f54c5164    Pod                     spec.containers{docker-build}            Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-docker-builder:v3.9.14" already present on machine
9m          9m           1         probes-1.15c7ea7c1be099a3          Build                                                            Normal    BuildCompleted          build-controller                 Build probes/probes-1 completed successfully
9m          9m           1         probes-1-deploy.15c7ea7bf9393d24   Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-1-deploy to node2.lab.example.com
9m          9m           1         probes.15c7ea7bf7a3364a            DeploymentConfig                                                 Normal    DeploymentCreated       deploymentconfig-controller      Created new replication controller "probes-1" for version 1
9m          9m           1         probes-1-deploy.15c7ea7c07fe99f5   Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-hlv5b" 
9m          9m           1         probes-1-deploy.15c7ea7c7239da53   Pod                     spec.containers{deployment}              Normal    Created                 kubelet, node2.lab.example.com   Created container
9m          9m           1         probes-1-deploy.15c7ea7c790acc41   Pod                     spec.containers{deployment}              Normal    Started                 kubelet, node2.lab.example.com   Started container
9m          9m           1         probes-1.15c7ea7c84499235          ReplicationController                                            Normal    SuccessfulCreate        replication-controller           Created pod: probes-1-8xsj2
9m          9m           1         probes-1-8xsj2.15c7ea7c927b7a1f    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-bqg59" 
9m          9m           1         probes-1-deploy.15c7ea7c6f427298   Pod                     spec.containers{deployment}              Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
9m          9m           1         probes-1-8xsj2.15c7ea7c84be788a    Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-1-8xsj2 to node2.lab.example.com
9m          9m           1         probes-1-8xsj2.15c7ea7d0522e6c0    Pod                     spec.containers{probes}                  Normal    Started                 kubelet, node2.lab.example.com   Started container
9m          9m           1         probes-1-8xsj2.15c7ea7cfe1f6547    Pod                     spec.containers{probes}                  Normal    Created                 kubelet, node2.lab.example.com   Created container
9m          9m           1         probes-1-8xsj2.15c7ea7cfa7ef4ec    Pod                     spec.containers{probes}                  Normal    Pulled                  kubelet, node2.lab.example.com   Successfully pulled image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
9m          9m           1         probes-1-8xsj2.15c7ea7cf800371e    Pod                     spec.containers{probes}                  Normal    Pulling                 kubelet, node2.lab.example.com   pulling image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
9m          9m           1         probes-1-deploy.15c7ea7d4a2a93b0   Pod                     spec.containers{deployment}              Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod
5m          5m           1         probes-2-deploy.15c7eab7170b05c6   Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-hlv5b" 
5m          5m           1         probes-2-deploy.15c7eab70a284d53   Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-2-deploy to node2.lab.example.com
5m          5m           1         probes.15c7eab708784f1a            DeploymentConfig                                                 Normal    DeploymentCreated       deploymentconfig-controller      Created new replication controller "probes-2" for version 2
5m          5m           1         probes-2-deploy.15c7eab77d35635d   Pod                     spec.containers{deployment}              Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
5m          5m           1         probes-2-deploy.15c7eab77f2e5ba4   Pod                     spec.containers{deployment}              Normal    Created                 kubelet, node2.lab.example.com   Created container
5m          5m           1         probes-2-deploy.15c7eab78500447e   Pod                     spec.containers{deployment}              Normal    Started                 kubelet, node2.lab.example.com   Started container
5m          5m           1         probes-2.15c7eab7cb717c37          ReplicationController                                            Normal    SuccessfulCreate        replication-controller           Created pod: probes-2-w68s4
5m          5m           1         probes-2-w68s4.15c7eab7cc2f5273    Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-2-w68s4 to node1.lab.example.com
5m          5m           1         probes-2-w68s4.15c7eab7d5e9d7bf    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-bqg59" 
5m          5m           1         probes-2-w68s4.15c7eab84edda7cf    Pod                     spec.containers{probes}                  Normal    Pulling                 kubelet, node1.lab.example.com   pulling image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
5m          5m           1         probes-2-w68s4.15c7eab8993439c3    Pod                     spec.containers{probes}                  Normal    Pulled                  kubelet, node1.lab.example.com   Successfully pulled image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
5m          5m           1         probes-2-w68s4.15c7eab89b5e3b46    Pod                     spec.containers{probes}                  Normal    Created                 kubelet, node1.lab.example.com   Created container
5m          5m           1         probes-2-w68s4.15c7eab8a169ce2a    Pod                     spec.containers{probes}                  Normal    Started                 kubelet, node1.lab.example.com   Started container
5m          5m           1         probes-1.15c7eab9ea735e17          ReplicationController                                            Normal    SuccessfulDelete        replication-controller           Deleted pod: probes-1-8xsj2
5m          5m           1         probes-2-deploy.15c7eaba581274c7   Pod                     spec.containers{deployment}              Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod
4m          4m           1         probes-1-8xsj2.15c7eac0e9cf8152    Pod                     spec.containers{probes}                  Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://probes:Need to kill Pod
3m          3m           1         probes-3-deploy.15c7eacf54fde925   Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-3-deploy to node2.lab.example.com
3m          3m           1         probes-3-deploy.15c7eacf6641656d   Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-hlv5b" 
3m          3m           1         probes.15c7eacf5341b7b7            DeploymentConfig                                                 Normal    DeploymentCreated       deploymentconfig-controller      Created new replication controller "probes-3" for version 3
3m          3m           1         probes-3-deploy.15c7eacfb57e48db   Pod                     spec.containers{deployment}              Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
3m          3m           1         probes-3-deploy.15c7eacfbdc0ca89   Pod                     spec.containers{deployment}              Normal    Started                 kubelet, node2.lab.example.com   Started container
3m          3m           1         probes-3-deploy.15c7eacfb78e79ed   Pod                     spec.containers{deployment}              Normal    Created                 kubelet, node2.lab.example.com   Created container
3m          3m           1         probes-3.15c7ead002d7a29e          ReplicationController                                            Normal    SuccessfulCreate        replication-controller           Created pod: probes-3-f8xtt
3m          3m           1         probes-3-f8xtt.15c7ead00dc734c9    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-bqg59" 
3m          3m           1         probes-3-f8xtt.15c7ead002ff55a9    Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-3-f8xtt to node2.lab.example.com
58s         3m           4         probes-3-f8xtt.15c7ead092117193    Pod                     spec.containers{probes}                  Normal    Pulling                 kubelet, node2.lab.example.com   pulling image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
58s         3m           4         probes-3-f8xtt.15c7ead0953a45b6    Pod                     spec.containers{probes}                  Normal    Pulled                  kubelet, node2.lab.example.com   Successfully pulled image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
1m          3m           3         probes-3-f8xtt.15c7ead097b6755d    Pod                     spec.containers{probes}                  Normal    Created                 kubelet, node2.lab.example.com   Created container
1m          3m           3         probes-3-f8xtt.15c7ead09cf8ee33    Pod                     spec.containers{probes}                  Normal    Started                 kubelet, node2.lab.example.com   Started container
1m          3m           7         probes-3-f8xtt.15c7ead266880ab3    Pod                     spec.containers{probes}                  Warning   Unhealthy               kubelet, node2.lab.example.com   Liveness probe failed: HTTP probe failed with statuscode: 404
3m          3m           1         probes-2.15c7ead25ca09245          ReplicationController                                            Normal    SuccessfulDelete        replication-controller           Deleted pod: probes-2-w68s4
3m          3m           1         probes-3-deploy.15c7ead2afab3207   Pod                     spec.containers{deployment}              Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod
2m          2m           1         probes-2-w68s4.15c7ead95cac5acd    Pod                     spec.containers{probes}                  Normal    Killing                 kubelet, node1.lab.example.com   Killing container with id docker://probes:Need to kill Pod
58s         2m           3         probes-3-f8xtt.15c7eade20a927a7    Pod                     spec.containers{probes}                  Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://probes:Container failed liveness probe.. Container will be killed and recreated.
29s         29s          1         probes.15c7eafc033f0e5d            DeploymentConfig                                                 Normal    DeploymentCreated       deploymentconfig-controller      Created new replication controller "probes-4" for version 4
29s         29s          1         probes-4-deploy.15c7eafc048e7c73   Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-4-deploy to node2.lab.example.com
29s         29s          1         probes-4-deploy.15c7eafc1449dc61   Pod                                                              Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-hlv5b" 
28s         28s          1         probes-4-deploy.15c7eafc62a619c9   Pod                     spec.containers{deployment}              Normal    Created                 kubelet, node2.lab.example.com   Created container
28s         28s          1         probes-4-deploy.15c7eafc68ce7871   Pod                     spec.containers{deployment}              Normal    Started                 kubelet, node2.lab.example.com   Started container
28s         28s          1         probes-4-deploy.15c7eafc60ac1d45   Pod                     spec.containers{deployment}              Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
26s         26s          1         probes-4-sjf82.15c7eafcb902db60    Pod                                                              Normal    SuccessfulMountVolume   kubelet, node1.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-bqg59" 
26s         26s          1         probes-4.15c7eafcaf0c6553          ReplicationController                                            Normal    SuccessfulCreate        replication-controller           Created pod: probes-4-sjf82
26s         26s          1         probes-4-sjf82.15c7eafcaf4d6965    Pod                                                              Normal    Scheduled               default-scheduler                Successfully assigned probes-4-sjf82 to node1.lab.example.com
24s         24s          1         probes-4-sjf82.15c7eafd3990b4dd    Pod                     spec.containers{probes}                  Normal    Pulling                 kubelet, node1.lab.example.com   pulling image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
24s         24s          1         probes-4-sjf82.15c7eafd3f2c8c4e    Pod                     spec.containers{probes}                  Normal    Created                 kubelet, node1.lab.example.com   Created container
24s         24s          1         probes-4-sjf82.15c7eafd45e439ec    Pod                     spec.containers{probes}                  Normal    Started                 kubelet, node1.lab.example.com   Started container
24s         24s          1         probes-4-sjf82.15c7eafd3d40e17c    Pod                     spec.containers{probes}                  Normal    Pulled                  kubelet, node1.lab.example.com   Successfully pulled image "docker-registry.default.svc:5000/probes/probes@sha256:3fe1daba7e3e4fac4f4e5d7e88ee599cf1761fc79c675ba8d88b9b02a390bc2b"
17s         17s          1         probes-3.15c7eafeccca8c8e          ReplicationController                                            Normal    SuccessfulDelete        replication-controller           Deleted pod: probes-3-f8xtt
16s         16s          1         probes-4-deploy.15c7eaff3009a3b2   Pod                     spec.containers{deployment}              Normal    Killing                 kubelet, node2.lab.example.com   Killing container with id docker://deployment:Need to kill Pod

```

cleanup

```
[student@workstation ~]$ oc delete project probes
project "probes" deleted

```

## Monitoring Resources with the Web Console(340)

Web画面の使い方。


演習は任意。1533


## Lab (356)

- pre

```
[student@workstation ~]$ lab review-monitor setup

Checking prerequisites for Lab: Managing and Monitoring OpenShift Container Platform

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS

 Checking all OpenShift default pods are ready and running:

 · Checking pod hawkular-cassandra.............................  SUCCESS
 · Checking pod hawkular-metrics...............................  SUCCESS
 · Checking pod heapster.......................................  SUCCESS

Downloading files for Lab: Managing and Monitoring OpenShift Container Platform

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS


```

1.

```
[student@workstation ~]$ oc whoami
developer
[student@workstation ~]$ oc login -u developer -p redhat

Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".


[student@workstation ~]$ oc new-project load-review
Now using project "load-review" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


[student@workstation ~]$ oc get svc
No resources found.
[student@workstation ~]$ 

```


2.

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-review/limits.yml 
apiVersion: "v1"
kind: "LimitRange"
metadata:
  name: "review-limits"
spec:
  limits:
    - type: "Container"
      max:
        memory: "300Mi"
      default:
        memory: "200Mi"

[student@workstation ~]$ oc get limitRange
No resources found.


[student@workstation ~]$ oc create -f /home/student/DO280/labs/monitor-review/limits.yml
Error from server (Forbidden): error when creating "/home/student/DO280/labs/monitor-review/limits.yml": limitranges is forbidden: User "developer" cannot create limitranges in the namespace "load-review": User "developer" cannot create limitranges in project "load-review"

[student@workstation ~]$ oc project load-review
Already on project "load-review" on server "https://master.lab.example.com:443".

[student@workstation ~]$ 
[student@workstation ~]$ 

[student@workstation ~]$ oc create -f /home/student/DO280/labs/monitor-review/limits.yml
limitrange "review-limits" created
[student@workstation ~]$ 

[student@workstation ~]$ oc get limitRange
NAME            AGE
review-limits   6s

[student@workstation ~]$ oc whoami
admin

```
3.

```
[student@workstation ~]$ oc login -u developer -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * load-review

Using project "load-review".

[student@workstation ~]$ oc whoami
developer

[student@workstation ~]$ oc new-app --name=load --docker-image=services.lab.example.com/node-hello
W0926 16:20:55.971582    1270 dockerimagelookup.go:233] Docker registry lookup failed: Get https://services.lab.example.com/v2/: x509: certificate is valid for registry.lab.example.com, not services.lab.example.com
error: only a partial match was found for "services.lab.example.com/node-hello": "node-hello:latest"

The argument "services.lab.example.com/node-hello" only partially matched the following Docker image, OpenShift image stream, or template:

* Docker image "node-hello:latest", 194638d, from services.lab.example.com, 471.786mb, author username "username@example.com"
  Use --docker-image="node-hello:latest" to specify this image or template



[student@workstation ~]$ oc new-app --name=load http://services.lab.example.com/node-hello
--> Found Docker image fba56b5 (2 years old) from registry.lab.example.com for "registry.lab.example.com/rhscl/nodejs-6-rhel7"

    Node.js 6 
    --------- 
    Node.js 6 available as docker container is a base platform for building and running various Node.js 6 applications and frameworks. Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices.

    Tags: builder, nodejs, nodejs6

    * An image stream will be created as "nodejs-6-rhel7:latest" that will track the source image
    * A Docker build using source code from http://services.lab.example.com/node-hello will be created
      * The resulting image will be pushed to image stream "load:latest"
      * Every time "nodejs-6-rhel7:latest" changes a new build will be triggered
    * This image will be deployed in deployment config "load"
    * Port 3000 will be load balanced by service "load"
      * Other containers can access this service through the hostname "load"

--> Creating resources ...
    imagestream "nodejs-6-rhel7" created
    imagestream "load" created
    buildconfig "load" created
    deploymentconfig "load" created
    service "load" created
--> Success
    Build scheduled, use 'oc logs -f bc/load' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/load' 
    Run 'oc status' to view your app.
[student@workstation ~]$ 



```

4.

```
[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          33s
load-1-lv7jv   1/1       Running     0          9s


[student@workstation ~]$ oc describe pods load-1-lv7jv
Name:           load-1-lv7jv
Namespace:      load-review
Node:           node2.lab.example.com/172.25.250.12
Start Time:     Thu, 26 Sep 2019 16:21:50 +0900
Labels:         app=load
                deployment=load-1
                deploymentconfig=load
Annotations:    kubernetes.io/limit-ranger=LimitRanger plugin set: memory request for container load; memory limit for container load
                openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=load
                openshift.io/deployment.name=load-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Running
IP:             10.129.0.174
Controlled By:  ReplicationController/load-1
Containers:
  load:
    Container ID:   docker://60affc2364a7587637fbc3e16f221b49bf1fc29da045805e194d56a36f27f60d
    Image:          docker-registry.default.svc:5000/load-review/load@sha256:cc954beeb767bcb19e3433371e7004206154ce3959098d6eb7670f9f59295776
    Image ID:       docker-pullable://docker-registry.default.svc:5000/load-review/load@sha256:cc954beeb767bcb19e3433371e7004206154ce3959098d6eb7670f9f59295776
    Port:           3000/TCP
    State:          Running
      Started:      Thu, 26 Sep 2019 16:21:52 +0900
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  200Mi
    Requests:
      memory:     200Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k5pmn (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-k5pmn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-k5pmn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              47s   default-scheduler               Successfully assigned load-1-lv7jv to node2.lab.example.com
  Normal  SuccessfulMountVolume  47s   kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-k5pmn"
  Normal  Pulling                45s   kubelet, node2.lab.example.com  pulling image "docker-registry.default.svc:5000/load-review/load@sha256:cc954beeb767bcb19e3433371e7004206154ce3959098d6eb7670f9f59295776"
  Normal  Pulled                 45s   kubelet, node2.lab.example.com  Successfully pulled image "docker-registry.default.svc:5000/load-review/load@sha256:cc954beeb767bcb19e3433371e7004206154ce3959098d6eb7670f9f59295776"
  Normal  Created                45s   kubelet, node2.lab.example.com  Created container
  Normal  Started                45s   kubelet, node2.lab.example.com  Started container


```

5.

```
[student@workstation ~]$ oc set resources dc load --requests=memory=350Mi
deploymentconfig "load" resource requirements updated

[student@workstation ~]$ oc get events | grep Warning
1m          1m           1         load-2.15c7ec474cfb84bc          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-h4zhm" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec474dc0266d          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-fgrdh" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec474df57ad2          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-2nv9b" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec474f57e51d          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-srlns" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec4751ee2de1          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-6s2gl" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec4756e47298          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-twtv6" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec4760a00078          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-bsb4v" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec4773e05243          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-wwtjt" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
1m          1m           1         load-2.15c7ec479a3887d4          ReplicationController                                            Warning   FailedCreate            replication-controller           Error creating: Pod "load-2-m7sdf" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit
24s         1m           5         load-2.15c7ec47e6dbaa4f          ReplicationController                                            Warning   FailedCreate            replication-controller           (combined from similar events): Error creating: Pod "load-2-q5d5t" is invalid: spec.containers[0].resources.requests: Invalid value: "350Mi": must be less than or equal to memory limit

-> これが原因で2が失敗していた。

student@workstation ~]$ oc set resources dc load --requests=memory=200Mi
deploymentconfig "load" resource requirements updated


[student@workstation ~]$ oc get pods
NAME            READY     STATUS        RESTARTS   AGE
load-1-build    0/1       Completed     0          7m
load-1-lv7jv    1/1       Running       0          6m
load-2-deploy   0/1       Terminating   0          2m
load-3-deploy   1/1       Running       0          2s
[student@workstation ~]$ oc get pods
NAME           READY     STATUS        RESTARTS   AGE
load-1-build   0/1       Completed     0          7m
load-1-lv7jv   1/1       Terminating   0          7m
load-3-c9bzq   1/1       Running       0          11s
[student@workstation ~]$ oc get pods
NAME           READY     STATUS        RESTARTS   AGE
load-1-build   0/1       Completed     0          7m
load-1-lv7jv   1/1       Terminating   0          7m
load-3-c9bzq   1/1       Running       0          14s


[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          8m
load-3-c9bzq   1/1       Running     0          1m


```


6. 

```
[student@workstation ~]$ cat /home/student/DO280/labs/monitor-review/quotas.yml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: review-quotas
spec:
  hard:
    requests.memory: "600Mi"

[student@workstation ~]$ oc login -u admin
Logged into "https://master.lab.example.com:443" as "admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
  * load-review
    logging
    manage-review
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review

Using project "load-review".
[student@workstation ~]$ oc create -f /home/student/DO280/labs/monitor-review/quotas.yml 
resourcequota "review-quotas" created
[student@workstation ~]$ 

[student@workstation ~]$ oc get quota
NAME            AGE
review-quotas   12s

[student@workstation ~]$ oc describe quota
Name:            review-quotas
Namespace:       load-review
Resource         Used   Hard
--------         ----   ----
requests.memory  200Mi  600Mi


```
7.

```
[student@workstation ~]$ oc login -u developer
Logged into "https://master.lab.example.com:443" as "developer" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * load-review

Using project "load-review".
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc whoami
developer

[student@workstation ~]$ oc scale dc/load --replicas=2
deploymentconfig "load" scaled
[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          12m
load-3-c9bzq   1/1       Running     0          5m
load-3-k2jcv   1/1       Running     0          3s

[student@workstation ~]$ oc describe quota
Name:            review-quotas
Namespace:       load-review
Resource         Used   Hard
--------         ----   ----
requests.memory  400Mi  600Mi


[student@workstation ~]$ oc scale dc/load --replicas=3
deploymentconfig "load" scaled

[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          13m
load-3-6w8x7   1/1       Running     0          2s
load-3-c9bzq   1/1       Running     0          5m
load-3-k2jcv   1/1       Running     0          36s

[student@workstation ~]$ oc describe quota
Name:            review-quotas
Namespace:       load-review
Resource         Used   Hard
--------         ----   ----
requests.memory  600Mi  600Mi


[student@workstation ~]$ oc scale dc/load --replicas=4
deploymentconfig "load" scaled
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          13m
load-3-6w8x7   1/1       Running     0          26s
load-3-c9bzq   1/1       Running     0          6m
load-3-k2jcv   1/1       Running     0          1m
[student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          13m
load-3-6w8x7   1/1       Running     0          29s
load-3-c9bzq   1/1       Running     0          6m
load-3-k2jcv   1/1       Running     0          1m
[student@workstation ~]$ oc describe quota
Name:            review-quotas
Namespace:       load-review
Resource         Used   Hard
--------         ----   ----
requests.memory  600Mi  600Mi



[student@workstation ~]$ oc get events | grep Warning | tail
40s         40s          1         load-3.15c7ecbc4b270271          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-flmcw" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc4b9e063f          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-g5zqj" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc4bff1492          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-fdv4d" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc4d7095ae          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-scjjd" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc500854e0          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-5nprq" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc54fc780b          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-gqdzk" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
40s         40s          1         load-3.15c7ecbc5ebddc2d          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-6fztv" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
39s         39s          1         load-3.15c7ecbc7202a670          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-js625" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
39s         39s          1         load-3.15c7ecbc985e5b50          ReplicationController                                            Warning   FailedCreate                     replication-controller           Error creating: pods "load-3-rmjl9" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi
19s         37s          4         load-3.15c7ecbce4e41471          ReplicationController                                            Warning   FailedCreate                     replication-controller           (combined from similar events): Error creating: pods "load-3-wqsrq" is forbidden: exceeded quota: review-quotas, requested: requests.memory=200Mi, used: requests.memory=600Mi, limited: requests.memory=600Mi

student@workstation ~]$ oc get pods
NAME           READY     STATUS      RESTARTS   AGE
load-1-build   0/1       Completed   0          15m
load-3-c9bzq   1/1       Running     0          7m


```
8.

```
[student@workstation ~]$ oc expose svc load
route "load" exposed
[student@workstation ~]$ oc get route
NAME      HOST/PORT                               PATH      SERVICES   PORT       TERMINATION   WILDCARD
load      load-load-review.apps.lab.example.com             load       3000-tcp                 None
[student@workstation ~]$ 


```

11.

```
[student@workstation ~]$ lab review-monitor grade

Grading the student's work for Lab: Managing and Monitoring OpenShift Container Platform

 · Ensuring load-review is created.............................  PASS
 · Ensuring limits for load-review is created..................  FAIL
 · Reviewing limits for load-review............................  FAIL
 · Ensuring application load is created........................  FAIL
 · Checking events for limits violation........................  FAIL
 · Checking the DC to make sure limit is set to 200 Mi.........  FAIL
 · Ensuring quota for load-review is existing..................  FAIL
 · Reviewing quotas for load-review............................  FAIL
 · Checking events for quota violation.........................  FAIL
 · Ensuring route is exposed...................................  FAIL

Reviewing Liveness Probe

 · Ensuring Liveness probe is created..........................  FAIL
 · Checking failureThreshold...................................  FAIL
 · Checking Type...............................................  FAIL
 · Checking Path...............................................  FAIL
 · Checking Port...............................................  FAIL
 · Checking Initial Delay......................................  FAIL
 · Checking Timeout............................................  FAIL

Overall exercise grade.........................................  FAIL


...なんで。。。？
最初のoc describe limits がなかったからか。。？

```

cleanup

```
[student@workstation ~]$ oc delete project load-review
project "load-review" deleted


```


-----------------------------------------

## Lab: Deploy an Application

- pre

```
[student@workstation ~]$ lab review-deploy setup

Checking prerequisites for Lab: Deploying an Application

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for Lab: Deploying an Application

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.

Overall setup status...........................................  SUCCESS


[student@workstation ~]$ cd /home/student/DO280/labs/review-deploy/
[student@workstation review-deploy]$ ll
total 12
-rwxr-xr-x. 1 student student 779 Aug 14  2018 config-nfs.sh
-rwxr-xr-x. 1 student student 258 Aug 14  2018 fix-quotas.sh
-rwxr-xr-x. 1 student student 126 Aug 31  2017 set-quotas.sh
drwxr-xr-x. 6 student student 118 Jul 26  2017 todoapi
drwxr-xr-x. 4 student student 101 Aug 14  2018 todoui

[student@workstation review-deploy]$ ls todoapi/
app.js  controllers  models  openshift  package.json  README.md  sql


[student@workstation review-deploy]$ ls todoui
build.sh  conf  Dockerfile  README.md  src  training.repo


[student@workstation review-deploy]$ cat config-nfs.sh 
#!/bin/bash

# Variable declarations
export_dir="/var/export/dbvol"
export_file="/etc/exports.d/dbvol.exports"
master_hostname="services"

  if [[ $(hostname -s) != ${master_hostname} && ${UID} -ne "0" ]]; then
    echo "This script must be run on the ${master_hostname} host as root."
    exit 1
  fi

  if [ -d ${export_dir} ]; then
    echo "Export directory ${export_dir} already exists."
  else
    mkdir -p ${export_dir}
    chown nfsnobody:nfsnobody /var/export/dbvol
    chmod 700 /var/export/dbvol
    echo "Export directory ${export_dir} created."
  fi

  if [ -f ${export_file} ]; then
    echo "Export file ${export_file} already exists."
  else
    echo "/var/export/dbvol *(rw,async,all_squash)" > /etc/exports.d/dbvol.exports
    exportfs -a
    showmount -e
  fi

[student@workstation review-deploy]$ cat fix-quotas.sh 
#!/bin/bash
  
echo "Changing quotas for the todoapp project..."
oc login -u admin -p redhat https://master.lab.example.com:443 --insecure-skip-tls-verify=true
oc project todoapp
oc patch resourcequota/todoapp-quota --patch '{"spec":{"hard":{"pods":"10"}}}'
[student@workstation review-deploy]$ 

[student@workstation review-deploy]$ cat set-quotas.sh 
#!/bin/bash
  
echo "Setting quotas on the todoapp project..."
oc project todoapp
oc create quota todoapp-quota --hard=pods=1
[student@workstation review-deploy]$ 



```

1.

```
[student@workstation review-deploy]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authentication system:authentication:oauth
error: unable to find target [system:authentication system:authentication:oauth]

[student@workstation review-deploy]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated system:authenticated:oauth
cluster role "self-provisioner" removed: ["system:authenticated" "system:authenticated:oauth"]
[student@workstation review-deploy]$ 

```

2.

```
[student@workstation review-deploy]$ oc new-project todoapp
Now using project "todoapp" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

[student@workstation review-deploy]$ oc adm policy add-role-to-user edit developer
role "edit" added: "developer"
[student@workstation review-deploy]

[student@workstation review-deploy]$ ./set-quotas.sh 
Setting quotas on the todoapp project...
Already on project "todoapp" on server "https://master.lab.example.com:443".
resourcequota "todoapp-quota" created


[student@workstation review-deploy]$ oc describe quota
Name:       todoapp-quota
Namespace:  todoapp
Resource    Used  Hard
--------    ----  ----
pods        0     1
[student@workstation review-deploy]$ 


```


3.

```
[student@workstation review-deploy]$ oc new-app --name=hello http://services.lab.example.com/php-helloworld
error: the image match "php" for source repository "http://services.lab.example.com/php-helloworld" does not appear to be a source-to-image builder.

- to attempt to use this image as a source builder, pass "--strategy=source"
- to use it as a base image for a Docker build, pass "--strategy=docker"

[student@workstation review-deploy]$ oc new-app --name=hello http://services.lab.example.com/php-helloworld --strategy=source
--> Found Docker image d1d1e48 (13 days old) from Docker Hub for "php"

    * An image stream will be created as "php:latest" that will track the source image
    * The source repository appears to match: php
    * A source build using source code from http://services.lab.example.com/php-helloworld will be created
      * The resulting image will be pushed to image stream "hello:latest"
      * Every time "php:latest" changes a new build will be triggered
    * This image will be deployed in deployment config "hello"
    * The image does not expose any ports - if you want to load balance or send traffic to this component
      you will need to create a service with 'expose dc/hello --port=[port]' later
    * WARNING: Image "php" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    imagestream "php" created
    imagestream "hello" created
    buildconfig "hello" created
    deploymentconfig "hello" created
--> Success
    Build scheduled, use 'oc logs -f bc/hello' to track its progress.
    Run 'oc status' to view your app.

[student@workstation review-deploy]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-build   0/1       Error     0          24s
[student@workstation review-deploy]$ 

[student@workstation review-deploy]$ oc get events
LAST SEEN   FIRST SEEN   COUNT     NAME                             KIND          SUBOBJECT                                TYPE      REASON                         SOURCE                           MESSAGE
53s         53s          1         hello-1-build.15c7ee78cf12b1e3   Pod                                                    Normal    Scheduled                      default-scheduler                Successfully assigned hello-1-build to node2.lab.example.com
52s         52s          1         hello-1-build.15c7ee78dc10a61b   Pod                                                    Normal    SuccessfulMountVolume          kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "crio-socket" 
52s         52s          1         hello-1-build.15c7ee78dc20b3b9   Pod                                                    Normal    SuccessfulMountVolume          kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "docker-socket" 
52s         52s          1         hello-1-build.15c7ee78dc2d8c68   Pod                                                    Normal    SuccessfulMountVolume          kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "buildworkdir" 
52s         52s          1         hello-1-build.15c7ee78dd34e24e   Pod                                                    Normal    SuccessfulMountVolume          kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "builder-dockercfg-d678n-push" 
52s         52s          1         hello-1-build.15c7ee78dd52942e   Pod                                                    Normal    SuccessfulMountVolume          kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "builder-token-hwwm5" 
51s         51s          1         hello-1-build.15c7ee794e9b396d   Pod           spec.initContainers{git-clone}           Normal    Pulled                         kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14" already present on machine
51s         51s          1         hello-1-build.15c7ee795047b5db   Pod           spec.initContainers{git-clone}           Normal    Created                        kubelet, node2.lab.example.com   Created container
50s         50s          1         hello-1-build.15c7ee7956af4199   Pod           spec.initContainers{git-clone}           Normal    Started                        kubelet, node2.lab.example.com   Started container
49s         49s          1         hello-1-build.15c7ee7992e102bf   Pod           spec.initContainers{manage-dockerfile}   Normal    Pulled                         kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14" already present on machine
49s         49s          1         hello-1-build.15c7ee799523dd6c   Pod           spec.initContainers{manage-dockerfile}   Normal    Created                        kubelet, node2.lab.example.com   Created container
49s         49s          1         hello-1-build.15c7ee799a5a33f3   Pod           spec.initContainers{manage-dockerfile}   Normal    Started                        kubelet, node2.lab.example.com   Started container
48s         48s          1         hello-1-build.15c7ee79cf8a167b   Pod           spec.containers{sti-build}               Normal    Pulled                         kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14" already present on machine
48s         48s          1         hello-1-build.15c7ee79d1813eb4   Pod           spec.containers{sti-build}               Normal    Created                        kubelet, node2.lab.example.com   Created container
48s         48s          1         hello-1-build.15c7ee79d7419fcf   Pod           spec.containers{sti-build}               Normal    Started                        kubelet, node2.lab.example.com   Started container
48s         48s          1         hello-1.15c7ee79e11de5c9         Build                                                  Normal    BuildFailed                    build-controller                 Build todoapp/hello-1 failed
53s         54s          9         hello.15c7ee78748c296b           BuildConfig                                            Warning   BuildConfigInstantiateFailed   buildconfig-controller           error instantiating Build from BuildConfig todoapp/hello (0): Error resolving ImageStreamTag php:latest in namespace todoapp: unable to find latest tagged image
[student@workstation review-deploy]$ 



yarinaoshi



[student@workstation review-deploy]$ oc new-app php:5.6~http://services.lab.example.com/php-helloworld --name hello
--> Found image 520f0e9 (17 months old) in image stream "openshift/php" under tag "5.6" for "php:5.6"

    Apache 2.4 with PHP 5.6 
    ----------------------- 
    PHP 5.6 available as container is a base platform for building and running various PHP 5.6 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php56, rh-php56

    * A source build using source code from http://services.lab.example.com/php-helloworld will be created
      * The resulting image will be pushed to image stream "hello:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    buildconfig "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Build scheduled, use 'oc logs -f bc/hello' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation review-deploy]$ oc get pods
NAME            READY     STATUS     RESTARTS   AGE
hello-1-build   0/1       Init:1/2   0          4s
[student@workstation review-deploy]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-build   1/1       Running   0          7s
[student@workstation review-deploy]$ oc get svc
NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
hello     ClusterIP   172.30.220.184   <none>        8080/TCP,8443/TCP   10s
[student@workstation review-deploy]$ oc get route
No resources found.
[student@workstation review-deploy]$ oc get pods -o w
error: output format "w" not recognized
[student@workstation review-deploy]$ oc get pods -o wide
NAME             READY     STATUS      RESTARTS   AGE       IP             NODE
hello-1-build    0/1       Completed   0          27s       10.129.0.182   node2.lab.example.com
hello-1-deploy   1/1       Running     0          7s        10.129.0.183   node2.lab.example.com



```


4.

```
[student@workstation review-deploy]$ oc describe dc/hello
Name:		hello
Namespace:	todoapp
Created:	3 minutes ago
Labels:		app=hello
Annotations:	openshift.io/generated-by=OpenShiftNewApp
Latest Version:	1
Selector:	app=hello,deploymentconfig=hello
Replicas:	1
Triggers:	Config, Image(hello@latest, auto=true)
Strategy:	Rolling
Template:
Pod Template:
  Labels:	app=hello
		deploymentconfig=hello
  Annotations:	openshift.io/generated-by=OpenShiftNewApp
  Containers:
   hello:
    Image:		docker-registry.default.svc:5000/todoapp/hello@sha256:bf0011d5ce0c86b7cda7bd4e19ae21200b5940edac01cb020be182b994d80a93
    Ports:		8080/TCP, 8443/TCP
    Environment:	<none>
    Mounts:		<none>
  Volumes:		<none>

Deployment #1 (latest):
	Name:		hello-1
	Created:	2 minutes ago
	Status:		Running
	Replicas:	0 current / 1 desired
	Selector:	app=hello,deployment=hello-1,deploymentconfig=hello
	Labels:		app=hello,openshift.io/deployment-config.name=hello
	Pods Status:	0 Running / 0 Waiting / 0 Succeeded / 0 Failed

Events:
  Type		Reason			Age		From				Message
  ----		------			----		----				-------
  Warning	FailedCreate		2m (x5 over 2m)	deployer-controller		Error creating deployer pod: pods "hello-1-deploy" is forbidden: exceeded quota: todoapp-quota, requested: pods=1, used: pods=1, limited: pods=1
  Normal	DeploymentCreated	2m		deploymentconfig-controller	Created new replication controller "hello-1" for version 1


[student@workstation review-deploy]$ oc describe quota
Name:       todoapp-quota
Namespace:  todoapp
Resource    Used  Hard
--------    ----  ----
pods        1     1




```


4.

```
[student@workstation review-deploy]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    manage-review
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review
  * todoapp

Using project "todoapp".
[student@workstation review-deploy]$ ls
config-nfs.sh  fix-quotas.sh  set-quotas.sh  todoapi  todoui
[student@workstation review-deploy]$ more fix-quotas.sh 
#!/bin/bash
  
echo "Changing quotas for the todoapp project..."
oc login -u admin -p redhat https://master.lab.example.com:443 --insecure-skip-tls-verify=true
oc project todoapp
oc patch resourcequota/todoapp-quota --patch '{"spec":{"hard":{"pods":"10"}}}'
[student@workstation review-deploy]$ ./fix-quotas.sh 
Changing quotas for the todoapp project...
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    manage-review
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review
  * todoapp

Using project "todoapp".
Already on project "todoapp" on server "https://master.lab.example.com:443".
resourcequota "todoapp-quota" patched
[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ oc login -u developer -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    hogehoge
  * todoapp

Using project "todoapp".
[student@workstation review-deploy]$ 


[student@workstation review-deploy]$ oc rollout latest dc/hello
deploymentconfig "hello" rolled out
[student@workstation review-deploy]$ 

[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ oc get pods
NAME            READY     STATUS      RESTARTS   AGE
hello-1-build   0/1       Completed   0          7m
hello-2-dwt6j   1/1       Running     0          10s


[student@workstation review-deploy]$ oc describe quota
Name:       todoapp-quota
Namespace:  todoapp
Resource    Used  Hard
--------    ----  ----
pods        1     10
[student@workstation review-deploy]$ 

[student@workstation review-deploy]$ oc delete --all -l app=hello
error: setting 'all' parameter but found a non empty selector. 


[student@workstation review-deploy]$ oc delete all -l app=hello
deploymentconfig "hello" deleted
buildconfig "hello" deleted
imagestream "hello" deleted
pod "hello-2-dwt6j" deleted
service "hello" deleted




```

6. 

```
[student@workstation review-deploy]$ more /home/student/DO280/labs/review-deploy/todoapi/openshift/mysql-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
 name: <name of pv>
spec:
 capacity:
  storage: <storage capacity>
 accessModes:
  - <access mode>
 nfs:
  path: <NFS share path>
  server: <NFS server FQDN>

[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ oc whoami
admin
[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ 
[student@workstation review-deploy]$ oc create -f mysql-pv.yaml 
persistentvolume "mysql-pv" created
[student@workstation review-deploy]$ oc get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                   STORAGECLASS   REASON    AGE
etcd-vol2-volume   1G         RWO            Retain           Bound       openshift-ansible-service-broker/etcd                            2d
metrics            5Gi        RWO            Recycle          Bound       openshift-infra/metrics-1                                        3h
mysql-pv           2Gi        RWX            Retain           Available                                                                    5s
registry-volume    40Gi       RWX            Retain           Bound       default/registry-claim                                           2d

[student@workstation review-deploy]$ cat mysql-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
 name: mysql-pv
spec:
 capacity:
  storage: 2Gi
 accessModes:
  - ReadWriteMany
 nfs:
  path: /var/export/dbvol
  server: services


```

7.

```
[student@workstation review-deploy]$ cat /home/student/DO280/labs/review-deploy/todoapi/openshift/nodejs-mysql-template.yaml 
apiVersion: v1
kind: Template
labels:
  template: nodejs-mysql-persistent
message: |-
  The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

metadata:
  annotations:
    description: An example Node.js application with a MySQL database.
    iconClass: icon-nodejs
    openshift.io/display-name: Node.js + MySQL (Persistent)
    tags: quickstart,nodejs
    template.openshift.io/documentation-url: https://github.com/openshift/nodejs-ex
    template.openshift.io/long-description: This template defines resources needed
      to develop a NodeJS application, including a build configuration, application
      deployment configuration, and database deployment configuration.
    template.openshift.io/provider-display-name: Red Hat Training
    template.openshift.io/support-url: https://access.redhat.com
  creationTimestamp: null
  name: nodejs-mysql-persistent
objects:
- apiVersion: v1
  kind: Secret
  metadata:
    name: ${NAME}
  stringData:
    database-root-password: ${DATABASE_ROOT_PASSWORD}
    database-password: ${DATABASE_PASSWORD}
    database-user: ${DATABASE_USER}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      description: Exposes and load balances the application pods
      service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
        "kind": "Service"}]'
    name: ${NAME}
  spec:
    ports:
    - name: web
      port: 8080
      targetPort: 8080
    selector:
      name: ${NAME}
- apiVersion: v1
  kind: Route
  metadata:
    name: ${NAME}
  spec:
    host: ${APPLICATION_DOMAIN}
    to:
      kind: Service
      name: ${NAME}
- apiVersion: v1
  kind: ImageStream
  metadata:
    annotations:
      description: Keeps track of changes in the application image
    name: ${NAME}
- apiVersion: v1
  kind: BuildConfig
  metadata:
    annotations:
      description: Defines how to build the application
    name: ${NAME}
  spec:
    output:
      to:
        kind: ImageStreamTag
        name: ${NAME}:latest
    source:
      contextDir: ${CONTEXT_DIR}
      git:
        ref: ${SOURCE_REPOSITORY_REF}
        uri: ${SOURCE_REPOSITORY_URL}
      type: Git
    strategy:
      sourceStrategy:
        env:
        - name: NPM_MIRROR
          value: ${NPM_MIRROR}
        from:
          kind: ImageStreamTag
          name: nodejs:4
          namespace: ${NAMESPACE}
      type: Source
    triggers:
    - type: ImageChange
    - type: ConfigChange
    - github:
        secret: ${GITHUB_WEBHOOK_SECRET}
      type: GitHub
    - generic:
        secret: ${GENERIC_WEBHOOK_SECRET}
      type: Generic
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the application server
    name: ${NAME}
  spec:
    replicas: 1
    selector:
      name: ${NAME}
    strategy:
      type: Rolling
    template:
      metadata:
        labels:
          name: ${NAME}
        name: ${NAME}
      spec:
        containers:
        - env:
          - name: DATABASE_SERVICE_NAME
            value: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: MYSQL_DATABASE
            value: ${DATABASE_NAME}
          image: ' '
          name: nodejs-mysql-persistent
          ports:
          - containerPort: 8080
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - nodejs-mysql-persistent
        from:
          kind: ImageStreamTag
          name: ${NAME}:latest
      type: ImageChange
    - type: ConfigChange
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: ${DATABASE_SERVICE_NAME}-pvc
  spec:
    accessModes:
    - ReadWriteMany
    resources:
      requests:
        storage: ${VOLUME_CAPACITY}
    selector:
      name: mysql-pv
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      description: Exposes the database server
    name: ${DATABASE_SERVICE_NAME}
  spec:
    ports:
    - name: mysql
      port: 3306
      targetPort: 3306
    selector:
      name: ${DATABASE_SERVICE_NAME}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the database
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          name: ${DATABASE_SERVICE_NAME}
        name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: MYSQL_DATABASE
            value: ${DATABASE_NAME}
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-root-password
                name: ${NAME}
          image: ' '
          name: mysql
          ports:
          - containerPort: 3306
          resources:
            limits:
              memory: ${MEMORY_MYSQL_LIMIT}
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: ${DATABASE_SERVICE_NAME}-data
        volumes:
        - name: ${DATABASE_SERVICE_NAME}-data
          persistentVolumeClaim:
            claimName: ${DATABASE_SERVICE_NAME}-pvc
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - mysql
        from:
          kind: ImageStreamTag
          name: mysql:5.6 
          namespace: ${NAMESPACE}
      type: ImageChange
    - type: ConfigChange
parameters:
- description: The name assigned to all of the frontend objects defined in this template.
  displayName: Name
  name: NAME
  required: true
  value: nodejs-mysql-persistent
- description: The OpenShift Namespace where the ImageStream resides.
  displayName: Namespace
  name: NAMESPACE
  required: true
  value: openshift
- description: Maximum amount of memory the Node.js container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 512Mi
- description: Maximum amount of memory the MySQL container can use.
  displayName: Memory Limit (MySQL)
  name: MEMORY_MYSQL_LIMIT
  required: true
  value: 512Mi
- description: Volume space available for data, e.g. 512Mi, 2Gi
  displayName: Volume Capacity
  name: VOLUME_CAPACITY
  required: true
  value: 1Gi
- description: The URL of the repository with your application source code.
  displayName: Git Repository URL
  name: SOURCE_REPOSITORY_URL
  required: true
  value: https://github.com/openshift/nodejs-ex.git
- description: Set this to a branch name, tag or other ref of your repository if you
    are not using the default branch.
  displayName: Git Reference
  name: SOURCE_REPOSITORY_REF
- description: Set this to the relative path to your project if it is not in the root
    of your repository.
  displayName: Context Directory
  name: CONTEXT_DIR
- description: The exposed hostname that will route to the Node.js service, if left
    blank a value will be defaulted.
  displayName: Application Hostname
  name: APPLICATION_DOMAIN
- description: A secret string used to configure the GitHub webhook.
  displayName: GitHub Webhook Secret
  from: '[a-zA-Z0-9]{40}'
  generate: expression
  name: GITHUB_WEBHOOK_SECRET
- description: A secret string used to configure the Generic webhook.
  displayName: Generic Webhook Secret
  from: '[a-zA-Z0-9]{40}'
  generate: expression
  name: GENERIC_WEBHOOK_SECRET
- displayName: Database Service Name
  name: DATABASE_SERVICE_NAME
  required: true
  value: mysql
- description: Username for MySQL user that will be used for accessing the database.
  displayName: MySQL Username
  from: user[A-Z0-9]{3}
  generate: expression
  name: DATABASE_USER
- description: Password for the MySQL user.
  displayName: MySQL Password
  from: '[a-zA-Z0-9]{16}'
  generate: expression
  name: DATABASE_PASSWORD
- displayName: Database Name
  name: DATABASE_NAME
  required: true
  value: sampledb
- description: Password for the MySQL database root user.
  displayName: Database Administrator Password
  from: '[a-zA-Z0-9]{16}'
  generate: expression
  name: DATABASE_ROOT_PASSWORD
- description: The custom NPM mirror URL
  displayName: Custom NPM Mirror URL
  name: NPM_MIRROR
  value: http://services.lab.example.com:8081/nexus/content/groups/nodejs/
[student@workstation review-deploy]$ 

[student@workstation review-deploy]$ oc create -f /home/student/DO280/labs/review-deploy/todoapi/openshift/nodejs-mysql-template.yaml 
template "nodejs-mysql-persistent" created



```

8.

```
[student@workstation todoui]$ docker images
REPOSITORY                                                            TAG                 IMAGE ID            CREATED             SIZE
node-hello                                                            latest              194638dc4731        23 hours ago        495 MB
registry.lab.example.com/node-hello                                   latest              194638dc4731        23 hours ago        495 MB
node-hello                                                            katest              1813a728f00e        23 hours ago        495 MB
docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin   4.7                 93d0d7db5ce2        15 months ago       166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7                         latest              fba56b5381b7        2 years ago         489 MB
[student@workstation todoui]$ docker build .
Sending build context to Docker daemon 648.2 kB
Step 1/11 : FROM rhel7:7.5
Trying to pull repository registry.lab.example.com/rhel7 ... 
7.5: Pulling from registry.lab.example.com/rhel7
378837c0e24a: Pull complete 
e17262bc2341: Pull complete 
Digest: sha256:9999e735605c73f0f48ea80239b4f6d20034bdab0ead6c8bf982a83769dfe7be
Status: Downloaded newer image for registry.lab.example.com/rhel7:7.5
 ---> 4bbd153adf84
Step 2/11 : MAINTAINER Red Hat Training <training@redhat.com>
 ---> Running in 4830495f20f1
 ---> 683edb93e654
Removing intermediate container 4830495f20f1
Step 3/11 : ENV HOME /var/www/html
 ---> Running in 04a883fc03d6
 ---> a4fc1e427f37
Removing intermediate container 04a883fc03d6
Step 4/11 : ADD training.repo /etc/yum.repos.d/training.repo
 ---> 32995002c444
Removing intermediate container 3c62934db8e3
Step 5/11 : RUN yum downgrade -y krb5-libs libstdc++ libcom_err &&     yum install -y --setopt=tsflags=nodocs     httpd     openssl-devel     procps-ng     which &&     yum clean all -y &&     rm -rf /var/cache/yum
 ---> Running in 3cc7673b730a

Loaded plugins: ovl, product-id, search-disabled-repos, subscription-manager
This system is not receiving updates. You can use subscription-manager on the host to register and assign subscriptions.
Resolving Dependencies
--> Running transaction check
---> Package krb5-libs.x86_64 0:1.15.1-18.el7 will be a downgrade
---> Package krb5-libs.x86_64 0:1.15.1-19.el7 will be erased
---> Package libcom_err.x86_64 0:1.42.9-11.el7 will be a downgrade
---> Package libcom_err.x86_64 0:1.42.9-12.el7_5 will be erased
---> Package libstdc++.x86_64 0:4.8.5-28.el7 will be a downgrade
---> Package libstdc++.x86_64 0:4.8.5-28.el7_5.1 will be erased
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package          Arch         Version             Repository              Size
================================================================================
Downgrading:
 krb5-libs        x86_64       1.15.1-18.el7       rhel--server-dvd       748 k
 libcom_err       x86_64       1.42.9-11.el7       rhel--server-dvd        41 k
 libstdc++        x86_64       4.8.5-28.el7        rhel--server-dvd       303 k

Transaction Summary
================================================================================
Downgrade  3 Packages

Total download size: 1.1 M
Downloading packages:
--------------------------------------------------------------------------------
Total                                              8.0 MB/s | 1.1 MB  00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libcom_err-1.42.9-11.el7.x86_64                              1/6 
  Installing : krb5-libs-1.15.1-18.el7.x86_64                               2/6 
  Installing : libstdc++-4.8.5-28.el7.x86_64                                3/6 
  Cleanup    : krb5-libs-1.15.1-19.el7.x86_64                               4/6 
  Cleanup    : libcom_err-1.42.9-12.el7_5.x86_64                            5/6 
  Cleanup    : libstdc++-4.8.5-28.el7_5.1.x86_64                            6/6 
  Verifying  : libstdc++-4.8.5-28.el7.x86_64                                1/6 
  Verifying  : krb5-libs-1.15.1-18.el7.x86_64                               2/6 
  Verifying  : libcom_err-1.42.9-11.el7.x86_64                              3/6 
  Verifying  : krb5-libs-1.15.1-19.el7.x86_64                               4/6 
  Verifying  : libstdc++-4.8.5-28.el7_5.1.x86_64                            5/6 
  Verifying  : libcom_err-1.42.9-12.el7_5.x86_64                            6/6 

Removed:
  krb5-libs.x86_64 0:1.15.1-19.el7       libcom_err.x86_64 0:1.42.9-12.el7_5   
  libstdc++.x86_64 0:4.8.5-28.el7_5.1   

Installed:
  krb5-libs.x86_64 0:1.15.1-18.el7       libcom_err.x86_64 0:1.42.9-11.el7      
  libstdc++.x86_64 0:4.8.5-28.el7       

Complete!
Loaded plugins: ovl, product-id, search-disabled-repos, subscription-manager
This system is not receiving updates. You can use subscription-manager on the host to register and assign subscriptions.
Package matching procps-ng-3.3.10-17.el7.x86_64 already installed. Checking for update.
Package which-2.20-7.el7.x86_64 already installed and latest version
Resolving Dependencies
--> Running transaction check
---> Package httpd.x86_64 0:2.4.6-80.el7 will be installed
--> Processing Dependency: httpd-tools = 2.4.6-80.el7 for package: httpd-2.4.6-80.el7.x86_64
--> Processing Dependency: system-logos >= 7.92.1-1 for package: httpd-2.4.6-80.el7.x86_64
--> Processing Dependency: /etc/mime.types for package: httpd-2.4.6-80.el7.x86_64
--> Processing Dependency: libapr-1.so.0()(64bit) for package: httpd-2.4.6-80.el7.x86_64
--> Processing Dependency: libaprutil-1.so.0()(64bit) for package: httpd-2.4.6-80.el7.x86_64
---> Package openssl-devel.x86_64 1:1.0.2k-12.el7 will be installed
--> Processing Dependency: krb5-devel(x86-64) for package: 1:openssl-devel-1.0.2k-12.el7.x86_64
--> Processing Dependency: zlib-devel(x86-64) for package: 1:openssl-devel-1.0.2k-12.el7.x86_64
--> Running transaction check
---> Package apr.x86_64 0:1.4.8-3.el7_4.1 will be installed
---> Package apr-util.x86_64 0:1.5.2-6.el7 will be installed
---> Package httpd-tools.x86_64 0:2.4.6-80.el7 will be installed
---> Package krb5-devel.x86_64 0:1.15.1-19.el7 will be installed
--> Processing Dependency: libkadm5(x86-64) = 1.15.1-19.el7 for package: krb5-devel-1.15.1-19.el7.x86_64
--> Processing Dependency: krb5-libs(x86-64) = 1.15.1-19.el7 for package: krb5-devel-1.15.1-19.el7.x86_64
--> Processing Dependency: libverto-devel for package: krb5-devel-1.15.1-19.el7.x86_64
--> Processing Dependency: libselinux-devel for package: krb5-devel-1.15.1-19.el7.x86_64
--> Processing Dependency: libcom_err-devel for package: krb5-devel-1.15.1-19.el7.x86_64
--> Processing Dependency: keyutils-libs-devel for package: krb5-devel-1.15.1-19.el7.x86_64
---> Package mailcap.noarch 0:2.1.41-2.el7 will be installed
---> Package redhat-logos.noarch 0:70.0.3-6.el7 will be installed
---> Package zlib-devel.x86_64 0:1.2.7-17.el7 will be installed
--> Running transaction check
---> Package keyutils-libs-devel.x86_64 0:1.5.8-3.el7 will be installed
---> Package krb5-libs.x86_64 0:1.15.1-18.el7 will be updated
---> Package krb5-libs.x86_64 0:1.15.1-19.el7 will be an update
---> Package libcom_err-devel.x86_64 0:1.42.9-11.el7 will be installed
---> Package libkadm5.x86_64 0:1.15.1-19.el7 will be installed
---> Package libselinux-devel.x86_64 0:2.5-12.el7 will be installed
--> Processing Dependency: libsepol-devel(x86-64) >= 2.5-6 for package: libselinux-devel-2.5-12.el7.x86_64
--> Processing Dependency: pkgconfig(libpcre) for package: libselinux-devel-2.5-12.el7.x86_64
--> Processing Dependency: pkgconfig(libsepol) for package: libselinux-devel-2.5-12.el7.x86_64
---> Package libverto-devel.x86_64 0:0.2.5-4.el7 will be installed
--> Running transaction check
---> Package libsepol-devel.x86_64 0:2.5-8.1.el7 will be installed
---> Package pcre-devel.x86_64 0:8.32-17.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package               Arch     Version           Repository               Size
================================================================================
Installing:
 httpd                 x86_64   2.4.6-80.el7      rhel--server-dvd        1.2 M
 openssl-devel         x86_64   1:1.0.2k-12.el7   rhel--server-dvd        1.5 M
Installing for dependencies:
 apr                   x86_64   1.4.8-3.el7_4.1   rhel--server-dvd        103 k
 apr-util              x86_64   1.5.2-6.el7       rhel--server-dvd         92 k
 httpd-tools           x86_64   2.4.6-80.el7      rhel--server-dvd         89 k
 keyutils-libs-devel   x86_64   1.5.8-3.el7       rhel--server-dvd         37 k
 krb5-devel            x86_64   1.15.1-19.el7     rhel-7-server-updates   269 k
 libcom_err-devel      x86_64   1.42.9-11.el7     rhel--server-dvd         31 k
 libkadm5              x86_64   1.15.1-19.el7     rhel-7-server-updates   175 k
 libselinux-devel      x86_64   2.5-12.el7        rhel--server-dvd        186 k
 libsepol-devel        x86_64   2.5-8.1.el7       rhel--server-dvd         77 k
 libverto-devel        x86_64   0.2.5-4.el7       rhel--server-dvd         12 k
 mailcap               noarch   2.1.41-2.el7      rhel--server-dvd         31 k
 pcre-devel            x86_64   8.32-17.el7       rhel--server-dvd        480 k
 redhat-logos          noarch   70.0.3-6.el7      rhel--server-dvd         13 M
 zlib-devel            x86_64   1.2.7-17.el7      rhel--server-dvd         50 k
Updating for dependencies:
 krb5-libs             x86_64   1.15.1-19.el7     rhel-7-server-updates   748 k

Transaction Summary
================================================================================
Install  2 Packages (+14 Dependent packages)
Upgrade             (  1 Dependent package)

Total download size: 18 M
Downloading packages:
Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
--------------------------------------------------------------------------------
Total                                               36 MB/s |  18 MB  00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : apr-1.4.8-3.el7_4.1.x86_64                                  1/18 
  Installing : apr-util-1.5.2-6.el7.x86_64                                 2/18 
  Updating   : krb5-libs-1.15.1-19.el7.x86_64                              3/18 
  Installing : libkadm5-1.15.1-19.el7.x86_64                               4/18 
  Installing : httpd-tools-2.4.6-80.el7.x86_64                             5/18 
  Installing : libcom_err-devel-1.42.9-11.el7.x86_64                       6/18 
  Installing : libsepol-devel-2.5-8.1.el7.x86_64                           7/18 
  Installing : pcre-devel-8.32-17.el7.x86_64                               8/18 
  Installing : libselinux-devel-2.5-12.el7.x86_64                          9/18 
  Installing : keyutils-libs-devel-1.5.8-3.el7.x86_64                     10/18 
  Installing : mailcap-2.1.41-2.el7.noarch                                11/18 
  Installing : redhat-logos-70.0.3-6.el7.noarch                           12/18 
  Installing : libverto-devel-0.2.5-4.el7.x86_64                          13/18 
  Installing : krb5-devel-1.15.1-19.el7.x86_64                            14/18 
  Installing : zlib-devel-1.2.7-17.el7.x86_64                             15/18 
  Installing : 1:openssl-devel-1.0.2k-12.el7.x86_64                       16/18 
  Installing : httpd-2.4.6-80.el7.x86_64                                  17/18 
  Cleanup    : krb5-libs-1.15.1-18.el7.x86_64                             18/18 
  Verifying  : krb5-devel-1.15.1-19.el7.x86_64                             1/18 
  Verifying  : zlib-devel-1.2.7-17.el7.x86_64                              2/18 
  Verifying  : 1:openssl-devel-1.0.2k-12.el7.x86_64                        3/18 
  Verifying  : libverto-devel-0.2.5-4.el7.x86_64                           4/18 
  Verifying  : redhat-logos-70.0.3-6.el7.noarch                            5/18 
  Verifying  : libselinux-devel-2.5-12.el7.x86_64                          6/18 
  Verifying  : mailcap-2.1.41-2.el7.noarch                                 7/18 
  Verifying  : apr-util-1.5.2-6.el7.x86_64                                 8/18 
  Verifying  : keyutils-libs-devel-1.5.8-3.el7.x86_64                      9/18 
  Verifying  : pcre-devel-8.32-17.el7.x86_64                              10/18 
  Verifying  : libkadm5-1.15.1-19.el7.x86_64                              11/18 
  Verifying  : krb5-libs-1.15.1-19.el7.x86_64                             12/18 
  Verifying  : httpd-2.4.6-80.el7.x86_64                                  13/18 
  Verifying  : libsepol-devel-2.5-8.1.el7.x86_64                          14/18 
  Verifying  : apr-1.4.8-3.el7_4.1.x86_64                                 15/18 
  Verifying  : libcom_err-devel-1.42.9-11.el7.x86_64                      16/18 
  Verifying  : httpd-tools-2.4.6-80.el7.x86_64                            17/18 
  Verifying  : krb5-libs-1.15.1-18.el7.x86_64                             18/18 

Installed:
  httpd.x86_64 0:2.4.6-80.el7        openssl-devel.x86_64 1:1.0.2k-12.el7       

Dependency Installed:
  apr.x86_64 0:1.4.8-3.el7_4.1         apr-util.x86_64 0:1.5.2-6.el7            
  httpd-tools.x86_64 0:2.4.6-80.el7    keyutils-libs-devel.x86_64 0:1.5.8-3.el7 
  krb5-devel.x86_64 0:1.15.1-19.el7    libcom_err-devel.x86_64 0:1.42.9-11.el7  
  libkadm5.x86_64 0:1.15.1-19.el7      libselinux-devel.x86_64 0:2.5-12.el7     
  libsepol-devel.x86_64 0:2.5-8.1.el7  libverto-devel.x86_64 0:0.2.5-4.el7      
  mailcap.noarch 0:2.1.41-2.el7        pcre-devel.x86_64 0:8.32-17.el7          
  redhat-logos.noarch 0:70.0.3-6.el7   zlib-devel.x86_64 0:1.2.7-17.el7         

Dependency Updated:
  krb5-libs.x86_64 0:1.15.1-19.el7                                              

Complete!
Loaded plugins: ovl, product-id, search-disabled-repos, subscription-manager
This system is not receiving updates. You can use subscription-manager on the host to register and assign subscriptions.
Cleaning repos: rhel--server-dvd rhel-7-server-ansible-24
              : rhel-7-server-common-rpms rhel-7-server-datapath-rpms
              : rhel-7-server-extras-rpms rhel-7-server-optional-rpms
              : rhel-7-server-ose-3.9-rpms rhel-7-server-supplementary
              : rhel-7-server-updates rhel-server-rhscl-7-rpms
Cleaning up everything
Maybe you want: rm -rf /var/cache/yum, to also free up space taken by orphaned data from disabled or removed repos
 ---> 3d249de04d08
Removing intermediate container 3cc7673b730a
Step 6/11 : COPY conf/httpd.conf /etc/httpd/conf/httpd.conf
 ---> 8f1a954b54d9
Removing intermediate container 98b92c2b49ec
Step 7/11 : COPY src/ ${HOME}/
 ---> 53c13a572a35
Removing intermediate container e77f63461eb5
Step 8/11 : EXPOSE 8080
 ---> Running in 44ba3a957133
 ---> 36ae6be752ff
Removing intermediate container 44ba3a957133
Step 9/11 : RUN rm -rf /run/httpd && mkdir /run/httpd && chmod -R a+rwx /run/httpd
 ---> Running in ca5f2a112c61

 ---> cb9d1ec141b9
Removing intermediate container ca5f2a112c61
Step 10/11 : USER 1001
 ---> Running in a8915735449d
 ---> af518e1ff3aa
Removing intermediate container a8915735449d
Step 11/11 : CMD /usr/sbin/apachectl -DFOREGROUND
 ---> Running in 41d4c82d75b2
 ---> 0dc84d10590f
Removing intermediate container 41d4c82d75b2
Successfully built 0dc84d10590f


[student@workstation todoui]$ docker images
REPOSITORY                                                            TAG                 IMAGE ID            CREATED             SIZE
<none>                                                                <none>              0dc84d10590f        16 seconds ago      239 MB
node-hello                                                            latest              194638dc4731        23 hours ago        495 MB
registry.lab.example.com/node-hello                                   latest              194638dc4731        23 hours ago        495 MB
node-hello                                                            katest              1813a728f00e        23 hours ago        495 MB
registry.lab.example.com/rhel7                                        7.5                 4bbd153adf84        14 months ago       201 MB
docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin   4.7                 93d0d7db5ce2        15 months ago       166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7                         latest              fba56b5381b7        2 years ago         489 MB
[student@workstation todoui]$ 


student@workstation todoui]$ docker tag 0dc84d10590f registry.lab.example.com/todoui:latest
[student@workstation todoui]$ docker images
REPOSITORY                                                            TAG                 IMAGE ID            CREATED              SIZE
registory.lab.example.com/todoui                                      latest              0dc84d10590f        About a minute ago   239 MB
registry.lab.example.com/todoui                                       latest              0dc84d10590f        About a minute ago   239 MB
node-hello                                                            latest              194638dc4731        23 hours ago         495 MB
registry.lab.example.com/node-hello                                   latest              194638dc4731        23 hours ago         495 MB
node-hello                                                            katest              1813a728f00e        23 hours ago         495 MB
registry.lab.example.com/rhel7                                        7.5                 4bbd153adf84        14 months ago        201 MB
docker-registry-default.apps.lab.example.com/schedule-is/phpmyadmin   4.7                 93d0d7db5ce2        15 months ago        166 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7                         latest              fba56b5381b7        2 years ago          489 MB


[student@workstation todoui]$ docker push registry.lab.example.com/todoui 
The push refers to a repository [registry.lab.example.com/todoui]
21954a05a7d5: Pushed 
406d11308e48: Pushed 
adcf9bf6b3b4: Pushed 
8b5b28f3072e: Pushed 
06e5a4f445f5: Pushed 
00860a9b126f: Mounted from rhel7 
366de6e5861a: Mounted from rhel7 
latest: digest: sha256:1772c32e673a9323754530305046768b1125e29f1b8c87ed0aa934db06d99303 size: 1781
[student@workstation todoui]$ 


[student@workstation todoui]$ oc import-image todoui --from=registry.lab.example.com/todoui --confirm -n todoapp
The import completed successfully.

Name:			todoui
Namespace:		todoapp
Created:		Less than a second ago
Labels:			<none>
Annotations:		openshift.io/image.dockerRepositoryCheck=2019-09-26T08:42:21Z
Docker Pull Spec:	docker-registry.default.svc:5000/todoapp/todoui
Image Lookup:		local=false
Unique Images:		1
Tags:			1

latest
  tagged from registry.lab.example.com/todoui

  * registry.lab.example.com/todoui@sha256:1772c32e673a9323754530305046768b1125e29f1b8c87ed0aa934db06d99303
      Less than a second ago

Image Name:	todoui:latest
Docker Image:	registry.lab.example.com/todoui@sha256:1772c32e673a9323754530305046768b1125e29f1b8c87ed0aa934db06d99303
Name:		sha256:1772c32e673a9323754530305046768b1125e29f1b8c87ed0aa934db06d99303
Created:	Less than a second ago
Annotations:	image.openshift.io/dockerLayersOrder=ascending
Image Size:	96.2MB (first layer 74.92MB, last binary layer 151B)
Image Created:	4 minutes ago
Author:		Red Hat Training <training@redhat.com>
Arch:		amd64
Command:	/bin/sh -c /usr/sbin/apachectl -DFOREGROUND
Working Dir:	<none>
User:		1001
Exposes Ports:	8080/tcp
Docker Labels:	architecture=x86_64
		authoritative-source-url=registry.access.redhat.com
		build-date=2018-08-01T14:14:43.466377
		com.redhat.build-host=osbs-cpt-011.ocp.osbs.upshift.eng.rdu2.redhat.com
		com.redhat.component=rhel-server-container
		description=The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications. This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.
		distribution-scope=public
		io.k8s.description=The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications. This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.
		io.k8s.display-name=Red Hat Enterprise Linux 7
		io.openshift.expose-services=
		io.openshift.tags=base rhel7
		maintainer=Red Hat, Inc.
		name=rhel7
		release=409.1533127727
		summary=Provides the latest release of Red Hat Enterprise Linux 7 in a fully featured and supported base image.
		url=https://access.redhat.com/containers/#/registry.access.redhat.com/rhel7/images/7.5-409.1533127727
		usage=This image is very generic and does not serve a single use case. Use it as a base to build your own images.
		vcs-ref=b8a2783c87bd09059fb8ba8a00817734bcb48ac3
		vcs-type=git
		vendor=Red Hat, Inc.
		version=7.5
Environment:	PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		container=oci
		HOME=/var/www/html


[student@workstation todoui]$ oc get is -n todoapp | grep todoui
todoui    docker-registry.default.svc:5000/todoapp/todoui   latest    44 seconds ago


[student@workstation todoui]$ oc describe is todoui
Name:			todoui
Namespace:		todoapp
Created:		About a minute ago
Labels:			<none>
Annotations:		openshift.io/image.dockerRepositoryCheck=2019-09-26T08:42:21Z
Docker Pull Spec:	docker-registry.default.svc:5000/todoapp/todoui
Image Lookup:		local=false
Unique Images:		1
Tags:			1

latest
  tagged from registry.lab.example.com/todoui

  * registry.lab.example.com/todoui@sha256:1772c32e673a9323754530305046768b1125e29f1b8c87ed0aa934db06d99303
      About a minute ago
[student@workstation todoui]$ 


```

10.

```
[student@workstation todoui]$ oc get pods
NAME              READY     STATUS      RESTARTS   AGE
mysql-1-r7hh2     1/1       Running     0          1m
todoapi-1-build   0/1       Completed   0          1m
todoapi-1-nkxsw   1/1       Running     0          18s
[student@workstation todoui]$ oc port-forward mysql-1-r7hh2 3306:3306
Forwarding from 127.0.0.1:3306 -> 3306
Handling connection for 3306
Handling connection for 3306

```
11.

```
[student@workstation todoui]$ curl http://todoapi.apps.lab.example.com/todo/api/host
{"ip":"10.129.0.190","hostname":"todoapi-1-nkxsw"}[student@workstation todoui]$ 
```

12.

```
[student@workstation todoui]$ curl http://todoapi.apps.lab.example.com/todo/api/items
{"currentPage":1,"list":[{"id":1,"description":"Pick up newspaper","done":false},{"id":2,"description":"Buy groceries","done":true}],"pageSize":10,"sortDirections":"asc","sortFields":"id","totalResults":2}[student@workstation todoui]$ 

```

13.

```
[student@workstation todoui]$ oc new-app --name=todoui -i todoui
--> Found image 0dc84d1 (16 minutes old) in image stream "todoapp/todoui" under tag "latest" for "todoui"

    Red Hat Enterprise Linux 7 
    -------------------------- 
    The Red Hat Enterprise Linux Base image is designed to be a fully supported foundation for your containerized applications. This base image provides your operations and application teams with the packages, language runtimes and tools necessary to run, maintain, and troubleshoot all of your applications. This image is maintained by Red Hat and updated regularly. It is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. When used as the source for all of your containers, only one copy will ever be downloaded and cached in your production environment. Use this image just like you would a regular Red Hat Enterprise Linux distribution. Tools like yum, gzip, and bash are provided by default. For further information on how this image was built look at the /root/anacanda-ks.cfg file.

    Tags: base rhel7

    * This image will be deployed in deployment config "todoui"
    * Port 8080/tcp will be load balanced by service "todoui"
      * Other containers can access this service through the hostname "todoui"

--> Creating resources ...
    deploymentconfig "todoui" created
    service "todoui" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/todoui' 
    Run 'oc status' to view your app.
[student@workstation todoui]$ 


[student@workstation todoui]$ oc get pods
NAME              READY     STATUS      RESTARTS   AGE
mysql-1-r7hh2     1/1       Running     0          4m
todoapi-1-build   0/1       Completed   0          4m
todoapi-1-nkxsw   1/1       Running     0          4m
todoui-1-gw6gv    1/1       Running     0          10s
[student@workstation todoui]$ 


```


12.

```
[student@workstation todoui]$ oc expose svc todoui --hostname=todo.apps.lab.example.com
route "todoui" exposed

[student@workstation todoui]$ oc get route
NAME      HOST/PORT                      PATH      SERVICES   PORT       TERMINATION   WILDCARD
todoapi   todoapi.apps.lab.example.com             todoapi    <all>                    None
todoui    todo.apps.lab.example.com                todoui     8080-tcp                 None
[student@workstation todoui]$ 

```


14.

```
[student@workstation todoui]$ curl http://todo.apps.lab.example.com
<!DOCTYPE html>

<html ng-app="items">
<head>
    
```

evaluation

```
[student@workstation todoui]$ lab review-deploy grade

Grading the student's work for Lab: Deploying an Application

 · Check if the mysql pod is in Running state..................  PASS
 · Check if the todoui pod is in Running state.................  PASS
 · Check if the todoapi pod is in Running state................  PASS
 . Checking if the todoapi REST interface can be invoked successfully  PASS
 . Checking if resource quotas were set correctly..............  PASS
 . Checking if the nodejs-mysql-persistent template was imported correctly  FAIL
 . Checking if the todoui image stream exists..................  PASS
 . Checking if the todoui route can be invoked successfully....  PASS

Overall exercise grade.........................................  FAIL

[student@workstation todoui]$ 

```
