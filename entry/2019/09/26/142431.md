---
title: DO280一日目
date: 2019-09-26T05:24:31.000Z
id: "26006613439826420"
draft: true
---
# 最初の立ち上げ

- RHELアカウントに対しての終了完了後の表示名、送付先メールアドレスの入力
  - 作業完了は0952

- DO180でDockerコマンドの紹介などをしている。
- DO280がその後継。OpenShiftのAdministrator向け。
- その後、EX280の試験がある。
  - RHELの試験は実技試験。なので選択式とかではない。
  - 演習はテキストの末尾の範囲内。繰り返しやること。ホームページ上の出題範囲も確認をしておくこと。
  - リカバリを実施できるかも重要なので、それを気にしながら取り組むこと。
- 別系統でDO288という講座が存在。Developer むけで、開発者がどのようにしてOCPを利用していくか、という内容。
より開発者寄り。→これも試験があり、これはEX288。

- ガイド演習、まとめ演習がある。
  - ガイド演習はコマンドの入力
  - まとめ演習は問題文だけが存在する。

- 7割くらいが合格点。点数配分は公開されていない。

- 仮想マシンの立ち上げの実施（10:05）
  - マシンに問題があったら、Manage VMs。ただし、resetは最終手段。OCPインストールとかから全部消える。
  - インストールスクリプトもあるが、10ー15分かかるので多用しないほうがいい。

- 操作端末側の設定変更の案内
  - 日本語設定の案内
  - Windows+Spaceで日本語変換。
  - 仮想環境は少しメニューが異なる。そちらはRHEL7。
  - Blank ScreenはNeverにしておいたほうがいいかも。
  - 試験環境ともあわせるため、キーボードは英語キーボード。ご注意ください。

- インターネットは自由に使っていい。セキュリティは最低限なので自己責任で。
  - データは全部消えるので管理は自分でやってね。


---------------------------------------------------------------------
# 1章 OCPの概要説明

docker + K8s/etcdなどの構成がスタンダードだが、
セキュリティ、ユーザ認証、イメージ管理、ネットワーキングなど使いにくい点が多い。

そのようないろいろな問題を解決するうえで、OCPを搭載して、K8Sに不足している機能などを補っている。
WebConsoleやRHELイメージレジストリ、ネットワークプラグインなど。
→PaaS環境を構築するためのソフトウェアである。

最新版は4.1。この場合、コンテナプラットフォームがdockerではなくpodmanになっている。
ただし、サブコマンドは全て同じ。

4.0台から中身はだいぶ変わって入るが、使い方は同じ。
今回のコースはdockerベースの3.9。
英語環境では4.0でもリリースされてはいるが、演習は同じ。
コマンドがdockerからpodmanにかわっただけ。
DO280コースは4.0は鋭意作成中。4.2にあわせてリリースされるかもしれない。

Open Container Initiativeの説明。
いろいろなメーカーが出しているコンテナを一括してK8Sで使えるように、という取組。
CRIというものが定義されており、K8S側はそれに合わせてコントロール。
dockerでないものでも同じようにコントロール。なので4からpodmanにかわった。docker互換なのでやれることは変わらない。

RHELはCoreOS買収したこともあり、rocketというものも動かせるように？よくわからん。

## OCP機能の紹介
- Self-service platform:
  - S2Iビルドという機能で、ボタン一つでサービスを提供する環境を構築可能。
- Multilingual support:
  - コンテナイメージの管理が一番大変。セキュリティホールがあったら更新していかないといけない。
    コンテナイメージの担保を誰がするの？→RHELのレジストリが管理しますよ、というのがこれ。
- Automation:
  - 自動化。様々なコンポーネントが各種自動化されている。
    人の手を介在することなく、スケールアウト、スケールアップダウンなどが可能。
    最終的にはビルドすればビルドフックして自動Deployなども可能。DevOpsを推進するツールとしても活用可能。
- User Interfaces
  - IDEとの統合も可能。Eclipseとのプラグインなどもある。
- Scalability and High Availability
  - 全てのコンポーネントは原則二重化することが可能。
- Container portability:
  - 他社のコンテナイメージも普通に動かすことが出来る。多少のてこ入れが必要になるものもある。。
- Open Source:
  - no vendor lockin.
- Security:
  - SELINUXロールベース。
  - 認証機能は外部。OAuthとかGitなどなど。
- Dynamic Strage Manaement
  - コンテナストレージは再起動で消える。外部ストレージに保存して永続化させる機能が提供されている、K8S標準機能。
- Choice of cloud:
  - どこの環境でも動作可能。
   - 4.台があやしいところがあり、ベースがCoreOSになっている点もある。
   - 4.xはCloudの方が簡単。
    - ベアメタルだと4.xは面倒。いろいろと必要なツールだったり、踏み台からの作業にしないといけなかったりなど。
      - 4.2あたりでこの課題を解消しようとしており、年末くらいにリリースされる予定。
- Enterprise Grade:
  - RedHatがサポートしている点が大きい。イメージも含めてサポートしていくのが特徴。
- Log Aggregation and Metrics:
  - ログ収集やメトリクスなどの機能。一応オプション。インストール時のオプションで選択可能。

QUIZはやってもいいが、試験にはあまり関係ない（実技だから）なので、あんまり。

## OpenShift Container Platform Architecture

RHEL -> Docker(podman) -> K8s(オーケストレーション、) + etcd(key value store) 
  etcdに入ったデータをもとにK8Sはコンテナをコントロール。
  etcdの中身はjson/yaml.
  
コレだけでやろうと思うと大変。yamlかいてkubectlコマンドうつとか、リソース登録だけでも大変。
OCPでは安定版でリリースしている。少しバージョンが古くなる。

RHELのコンテナレジストリ、ほぼ毎日のように更新がされている。
RHELが毎回ソースからやっているので、誰かが組み立てた怪しいイメージとかは使っていない。

K8Sネイティブ環境との大きな違いは、ビルドがあること。
K8Sはあらかじめイメージを作っておくスタイル。
OCPは、OCP環境内でビルドが可能。ソースコードさえ用意しておけば、一括でCI／CDが可能。

## Master and Nodes

テキストの構造がわかりにくいので、板書する。-> 実体はノートに書いたものを参照。

Online Documentに、3.9とそれ以降(3.10)のコンポーネントの作りが大きく異なる。
構造が変わると設定ファイルの位置とかも変わってくる。講座としては大きく影響は無いが。

以下URLなどからたどれる先を参照。
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/

4.1など、いつもみているグラフが少し深くもぐらないと見れなかったり、少し使いにくいかも。慣れもある。
今回は3.9ベースの3台構成。
今回は、レジストリは自身の端末上に建てたものを使う。（ずれがでるといけないから）
本来はインターネット上からもってくる。

本当のレジストリをみておくと、ここ。(docker hub)
https://hub.docker.com/

docker pull mysql 
何も指定しないとlatestとして最新のものを持ってくる。
その他、イメージの軌道方法などもかいてある。このあたりはDO180の講座内容。

RH社のレジストリはここ。RHCC（コンテナカタログ）
https://access.redhat.com/containers/

DockerHUB以上に、いろいろな物が全て公開されている。
Subscriveが必要だが、その分安全に利用可能。

本来はこの2種類が標準だが、今回は内部で建てたレジストリからコンテナイメージを取得する形で実施する。
http://services.lab.example.com/

## Oc new app でアプリケーションをDeployしたときの基本的な流れ

-1. 管理者が以下コマンドを入力し、masterにコントロール
 oc newapp <git-url>
-2. Projectが作成され、bc/dcが作成される。
-3. bcにPodが作成され、git-urlからソースがCloneされる。
-4. ソースからアプリケーションを判定し、該当のアプリケーションのベースイメージをリポジトリからPull
-5. S2Iビルドの場合、それらをマージして新しいイメージを作り、内部レジストリに対しPush
-6. 内部レジストリに対し、新しいイメージが蓄えられる。（ビルドしたイメージを自己管理するため）
-7. 新しいイメージとともに、isというリソースが造られ、新しく出来たイメージに対してのポインタの役割をしている。
  isを消すことで、イメージも消してしまう。ポインタではあるが、イメージそのものと考えていい。
  OCPでは、isを管理することでビルドしたイメージを特定する。

-8. dcはisが更新されたかどうかを監視している。イメージが変わったかどうか確認し、
 変わった場合にはdcの内容に合わせてDeployが始まる。（Deploy Podが生成される。)

 -> rc(replication controller - PODの起動の管理 Podの数の管理など。) 
  -> isを通じてイメージがプルされる。（Nodeを特定し、その中のローカルキャッシュにイメージをプル)
-9. コンテナが起動される。
-10. PODへのアクセス口として、svc(service)というリソースが造られる。
 ここにCluster IPというIPを作られる。これがPODへのアクセスをラウンドロビンで分散してばら撒く。

rc/pod/svcとかはK8Sネイティブの機能。

-11. このままだとIPアドレスベースのアドレスになっているので使い辛い。
 OCPは、この点をサポートするため、routeリソースを作り、アドレスを生成する。(xxx.apps.lab.example.com)
 起動するPOD毎に固有のリクエストアドレスを作ることも可能。

-12. routeで作成されたアドレスを、外部のrouter(HAPROXY)に紐づける。

-13.ユーザはDNSでアプリケーション名を指定-> DNSがノード1のアドレスを返す。
 -> routerがrouteから埋め込まれた設定値に従ってPODをバランシング
 -> routeリソースはここからEnd Point のIPアドレスを取得
 -> Podにroute経由でアクセスさせる。(svcは経由はしない)

細かい所はこの後の章で個別で解説していくため、全体感を掴んでもらえれば。


---------------------------------------------------------------------
# 2章 OCPのインストール

## 前準備とかの説明

基本的には3.台はrpm＋Ansibleを使ったインストールなる。マシン台数が多いこともあって手動だと厳しい。
4.台はUPI／IPIという形式のインストール方法を選択する。
Installer Provisioned Infrastructure -> Cloud環境用
User Provisioned Infrastructure - > ベアメタル用
詳細はRHエンジニアブログを参照。
https://rheb.hatenablog.com/entry/openshift41-baremetal-upi

IPIはいろいろ勝手にやってくれるので非常に簡単。

今回は3.x台なので初期環境のセットアップにはAnsibleを使う。
教材にのっているAnsibleのPlaybookとかinventoryとかの説明を実施。
OCPでは、インストール用Playbookは用意されているので、必要なのはInventoryと環境変数の値くらい。

インストール前にansible pingモジュールで疎通確認を実施。
digによる名前解決。

RHELのリポジトリをsubscription-managerで有効化する必要がある。
このほか、wget/bindutilsとか。
あとは、docker/docker stroge設定などが必要。
-> これらを一台ずつやるの大変なので、Ansibleを使って作る。
 prepare_install.ymlというのがあるが、これはこの環境だけのもの。
 今回はオレオレ証明書なので、そのあたりのフォロー用の設定などを入れたりする。

ページ27でここまで。
まだインストール前環境のお話。OCPはインストールされていない。
他にもDNS整えたりとかは必要。

## OCPのインストール
P28ー
labコマンドはこの演習専用のコマンド。OCPとか全く関係ないので注意。
```
[student@workstation install-prepare]$ sudo yum install ansible

[student@workstation install-prepare]$ ansible --version
ansible 2.4.3.0
  config file = /home/student/DO280/labs/install-prepare/ansible.cfg
  configured module search path = [u'/home/student/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python2.7/site-packages/ansible
  executable location = /usr/bin/ansible
  python version = 2.7.5 (default, Feb 20 2018, 09:19:12) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]

[student@workstation install-prepare]$ cat inventory 
[workstations]
workstation.lab.example.com

[nfs]
services.lab.example.com

[masters]
master.lab.example.com

[etcd]
master.lab.example.com

[nodes]
master.lab.example.com
node1.lab.example.com
node2.lab.example.com

[OSEv3:children]
masters
etcd
nodes
nfs

#Variables needed by the prepare_install.yml playbook.
[nodes:vars]
registry_local=registry.lab.example.com
use_overlay2_driver=true
insecure_registry=false
run_docker_offline=true
docker_storage_device=/dev/vdb

[student@workstation install-prepare]$ cat prepare_install.yml 
---
- name: "Host Preparation: Docker tasks"
  hosts: nodes
  roles:
    - docker-storage
    - docker-registry-cert
    - openshift-node

  #Tasks below were not handled by the roles above.
  tasks:
    - name: Student Account - Docker Access
      user:
        name: student
        groups: docker
        append: yes

...

PLAY RECAP *********************************************************************
master.lab.example.com     : ok=22   changed=4    unreachable=0    failed=0   
node1.lab.example.com      : ok=22   changed=6    unreachable=0    failed=0   
node2.lab.example.com      : ok=22   changed=4    unreachable=0    failed=0   


[student@workstation install-prepare]$ for vm in master node1 node2; do
> echo -e "\n$vm"
> ssh $vm sudo systemctl status docker | head -n 3
> done

master
Warning: Permanently added 'master' (ECDSA) to the list of known hosts.
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2019-09-24 13:14:28 JST; 2min 56s ago

node1
Warning: Permanently added 'node1' (ECDSA) to the list of known hosts.
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2019-09-24 13:14:28 JST; 2min 57s ago

node2
Warning: Permanently added 'node2' (ECDSA) to the list of known hosts.
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2019-09-24 13:14:28 JST; 2min 57s ago

[student@workstation install-prepare]$ for vm in master node1 node2; do echo -e "\n$vm : lvs"; ssh $vm sudo lvs | echo -e "\n$vm : df -h" ; ssh $vm sudo df -h | grep vg-docker; done

master : lvs

master : df -h
/dev/mapper/docker--vg-docker--pool   20G   33M   20G   1% /var/lib/docker

node1 : lvs

node1 : df -h
/dev/mapper/docker--vg-docker--pool   20G   33M   20G   1% /var/lib/docker

node2 : lvs

node2 : df -h
/dev/mapper/docker--vg-docker--pool   20G   33M   20G   1% /var/lib/docker

[student@workstation install-prepare]$ for vm in master node1 node2; do echo -e "\n$vm"; ssh $vm rpm -qa atomic-openshift-utils; done

master
atomic-openshift-utils-3.9.14-1.git.3.c62bc34.el7.noarch

node1
atomic-openshift-utils-3.9.14-1.git.3.c62bc34.el7.noarch

node2
atomic-openshift-utils-3.9.14-1.git.3.c62bc34.el7.noarch


```

今回はmasterではPODは動かさない。

レジストリも、今回はクラスルームのイメージからプルする。
原則、OpenShiftの管理環境はインターネット接続は必須。
オフラインもお勧めはしないが、できるはできる。
RedHatが提供しているSateliteServerというプロダクトでイメージをプルする形に出来れば、まあ。
ただ、やっぱりインターネットには出れる様にした方が良いです。

↑P36

### Configuring the OpenShift Installation Version


### Configuring Authentication

ユーザ認証などの話
デフォルトではDenyAllPasswordIdentityProviderで誰もログインできなくなってる。
今回はHTPasswdPasswordIdentityProvidorを使う。
今回はユーザ情報が/etc/origin/master/htpasswdに保存される。

[student@workstation install-prepare]$ htpasswd -nb admin redhat
admin:$apr1$CfeQSw34$RCx/Ie7ZZe0srWxpyz8qC.

### Configureing Network Requirements

テキストはapps.mycluster.comになっているが、今回はapps.labs.examples.comとかちょっとちがう。
iptablesが基本だったが、firewalldも使えるようになった。
OpenShiftは空けたり閉めたりをガッツリやるので、それをやるのをどちらでやるか、という選択項目。

### Configuring Persistent Storage
外部ストレージに永続ストレージを用意しておかないと、せっかくDeployしても消える。
本番環境ではNFSはサポート外。ほかはiSCSI/GlusterFS/Ceph/などなど

細かい話は後の永続化の章で解説する。

ETCDが死ぬとK8S Clusterはしぬ。大事。

### Configuring Disconnected OpenShift Cluster
内部レジストリなどからもってくるための設定。本来の環境であればいらない。

### Configuring Node Labels
Inventoryファイルに対してのラベリングの実施
ゾーンやリージョンなどがある。この単語は予約されているので注意。
gpu trueっていうのは、AI計算などをするときに配置できるマシンを識別できるタグを付与するため。

あらかじめ、どんなゾーンにどんなマシンを立てるかというのは決めておく必要がある。
今回は3台構成、全てインフラリージョン。


master nodeやNodeには、必要なラベルがある。
Router Nodeなどを動かすリージョンも本来は分けるべき。

興味のある人はこのあたりをみてみるといいのでは。
いろいろな構成例などが掲載されていたりする。
https://blog.openshift.com/

これらを仕掛けて、インストールを実施する。

### Guide Exercise: Installing Red Hat Openshift Container Platform

```
[student@workstation install-prepare]$ lab install-run setup

Setting up workstation for lab work:


Downloading files for GE: Running the Installer

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.

Downloading additional artifacts for the lab:

 · Downloading Ansible artifacts...............................  SUCCESS
 · Install 'crudini' if necessary..............................  SUCCESS

Setup successful.

[student@workstation install-prepare]$ cd /home/student/DO280/labs/install-run/

[student@workstation install-run]$ ll
total 24
lrwxrwxrwx. 1 student student  43 Aug  7  2018 ansible.cfg -> /home/student/do280-ansible/ansible.cfg.lab
-rw-r--r--. 1 student student 317 Aug  7  2018 authentication_vars.txt
-rw-r--r--. 1 student student 667 Aug  7  2018 disconnected_vars.txt
-rw-r--r--. 1 student student 172 Aug  7  2018 general_vars.txt
-rw-r--r--. 1 student student 478 Aug  7  2018 inventory.initial
-rw-r--r--. 1 student student 129 Aug  7  2018 networking_vars.txt
-rw-r--r--. 1 student student 845 Aug  7  2018 persistence_vars.txt


[student@workstation install-run]$ sudo yum install atomic-openshift-utils

試験はインストールは無い。

[student@workstation install-run]$ cat general_vars.txt 
#General Cluster Variables
openshift_deployment_type=
openshift_release=
openshift_image_tag=
openshift_disable_check=disk_availability,docker_storage,memory_availability

[student@workstation install-run]$ 
[student@workstation install-run]$ 

今回は、あらかじめ設定されている回答をコピーしていく。

[student@workstation install-run]$ cp ../../solutions/install-run/general_vars.txt .
[student@workstation install-run]$ 
[student@workstation install-run]$ cat general_vars.txt 
#General Variables
openshift_deployment_type=openshift-enterprise
openshift_release=v3.9
openshift_image_tag=v3.9.14
openshift_disable_check=disk_availability,docker_storage,memory_availability

[student@workstation install-run]$ cp ../../solutions/install-run/authentication_vars.txt .
[student@workstation install-run]$ cat authentication_vars.txt 
#Cluster Authentication Variables
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
openshift_master_htpasswd_users={'admin': '$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611', 'developer': '$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611'}



[student@workstation install-run]$ cp ../../solutions/install-run/networking_vars.txt .
[student@workstation install-run]$ cat networking_vars.txt 
#OpenShift Networking Variables
os_firewall_use_firewalld=true
openshift_master_api_port=443
openshift_master_console_port=443
openshift_master_default_subdomain=apps.lab.example.com
openshift_set_node_ip=true


[student@workstation install-run]$ cp ../../solutions/install-run/persistence_vars.txt .
[student@workstation install-run]$ cat persistence_vars.txt 
#NFS is an unsupported configuration
openshift_enable_unsupported_configurations=true

#Registry Configuration Variables
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=40Gi

#etcd Configuration Variables
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_nfs_directory=/exports
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_volume_name=etcd-vol2
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}


[student@workstation install-run]$ cp ../../solutions/install-run/disconnected_vars.txt .
[student@workstation install-run]$ 
[student@workstation install-run]$ 
[student@workstation install-run]$ cat disconnected_vars.txt 
#Modifications Needed for a Disconnected Install
oreg_url=registry.lab.example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
openshift_docker_additional_registries=registry.lab.example.com
openshift_docker_blocked_registries=registry.access.redhat.com,docker.io
openshift_web_console_prefix=registry.lab.example.com/openshift3/ose-
openshift_cockpit_deployer_prefix='registry.lab.example.com/openshift3/'
openshift_service_catalog_image_prefix=registry.lab.example.com/openshift3/ose-
template_service_broker_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_image_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_etcd_image_prefix=registry.lab.example.com/rhel7/

```

inventoryファイルの修正
```
[nodes]
master.lab.example.com
node1.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"
node2.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"


[student@workstation install-run]$ cat inventory
[workstations]
workstation.lab.example.com

[nfs]
services.lab.example.com

[masters]
master.lab.example.com

[etcd]
master.lab.example.com

[nodes]
master.lab.example.com 
node1.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"
node2.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"

[OSEv3:children]
masters
etcd
nodes
nfs

#Variables needed by classroom host preparation playbooks.
[nodes:vars]
registry_local=registry.lab.example.com
use_overlay2_driver=true
insecure_registry=false
run_docker_offline=true
docker_storage_device=/dev/vdb


[OSEv3:vars]

[student@workstation install-run]$ cat general_vars.txt networking_vars.txt authentication_vars.txt persistence_vars.txt disconnected_vars.txt >> inventory


[student@workstation install-run]$ cat inventory
[workstations]
workstation.lab.example.com

[nfs]
services.lab.example.com

[masters]
master.lab.example.com

[etcd]
master.lab.example.com

[nodes]
master.lab.example.com 
node1.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"
node2.lab.example.com openshift_node_labels="{'region':'infra', 'node-role.kubernetes.io/compute':'true'}"

[OSEv3:children]
masters
etcd
nodes
nfs

#Variables needed by classroom host preparation playbooks.
[nodes:vars]
registry_local=registry.lab.example.com
use_overlay2_driver=true
insecure_registry=false
run_docker_offline=true
docker_storage_device=/dev/vdb


[OSEv3:vars]
#General Variables
openshift_deployment_type=openshift-enterprise
openshift_release=v3.9
openshift_image_tag=v3.9.14
openshift_disable_check=disk_availability,docker_storage,memory_availability

#OpenShift Networking Variables
os_firewall_use_firewalld=true
openshift_master_api_port=443
openshift_master_console_port=443
openshift_master_default_subdomain=apps.lab.example.com
openshift_set_node_ip=true

#Cluster Authentication Variables
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
openshift_master_htpasswd_users={'admin': '$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611', 'developer': '$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611'}

#NFS is an unsupported configuration
openshift_enable_unsupported_configurations=true

#Registry Configuration Variables
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=40Gi

#etcd Configuration Variables
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_nfs_directory=/exports
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_volume_name=etcd-vol2
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}

#Modifications Needed for a Disconnected Install
oreg_url=registry.lab.example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
openshift_docker_additional_registries=registry.lab.example.com
openshift_docker_blocked_registries=registry.access.redhat.com,docker.io
openshift_web_console_prefix=registry.lab.example.com/openshift3/ose-
openshift_cockpit_deployer_prefix='registry.lab.example.com/openshift3/'
openshift_service_catalog_image_prefix=registry.lab.example.com/openshift3/ose-
template_service_broker_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_image_prefix=registry.lab.example.com/openshift3/ose-
ansible_service_broker_etcd_image_prefix=registry.lab.example.com/rhel7/

```

チェック用コマンドの実行
```

[student@workstation install-run]$ lab install-run grade

Checking the OpenShift Advanced Installation method inventory file

 · Detecting solution inventory................................  PASS
 · Detecting student inventory.................................  PASS

Comparing Entries in [OSEv3:children]

 · Checking masters............................................  PASS
 · Checking etcd...............................................  PASS
 · Checking nodes..............................................  PASS
 · Checking nfs................................................  PASS

Comparing Entries in [OSEv3:vars]

 · Checking openshift_disable_check............................  PASS
 · Checking openshift_deployment_type..........................  PASS
 · Checking openshift_release..................................  PASS
 · Checking openshift_image_tag................................  PASS
 · Checking os_firewall_use_firewalld..........................  PASS
 · Checking openshift_master_api_port..........................  PASS
 · Checking openshift_master_console_port......................  PASS
 · Checking openshift_master_default_subdomain.................  PASS
 · Checking openshift_master_identity_providers................  PASS
 · Skipping openshift_master_htpasswd_users....................  PASS
 · Checking openshift_enable_unsupported_configurations........  PASS
 · Checking openshift_hosted_registry_storage_kind.............  PASS
 · Checking openshift_hosted_registry_storage_access_mode......  PASS
 · Checking openshift_hosted_registry_storage_nfs_directo......  PASS
 · Checking openshift_hosted_registry_storage_nfs_options......  PASS
 · Checking openshift_hosted_registry_storage_volume_name......  PASS
 · Checking openshift_hosted_registry_storage_volume_size......  PASS
 · Checking openshift_hosted_etcd_storage_kind.................  PASS
 · Checking openshift_hosted_etcd_storage_nfs_options..........  PASS
 · Checking openshift_hosted_etcd_storage_nfs_directory........  PASS
 · Checking openshift_hosted_etcd_storage_volume_name..........  PASS
 · Checking openshift_hosted_etcd_storage_access_modes.........  PASS
 · Checking openshift_hosted_etcd_storage_volume_size..........  PASS
 · Checking openshift_hosted_etcd_storage_labels...............  PASS
 · Checking oreg_url...........................................  PASS
 · Checking openshift_examples_modify_imagestreams.............  PASS
 · Checking openshift_docker_additional_registries.............  PASS
 · Checking openshift_docker_blocked_registries................  PASS
 · Checking openshift_web_console_prefix.......................  PASS
 · Checking openshift_cockpit_deployer_prefix..................  PASS
 · Checking openshift_service_catalog_image_prefix.............  PASS
 · Checking template_service_broker_prefix.....................  PASS
 · Checking ansible_service_broker_image_prefix................  PASS
 · Checking ansible_service_broker_etcd_image_prefix...........  PASS

Comparing Entries in [etcd]

 · Checking master.lab.example.com.............................  PASS

Comparing Entries in [masters]

 · Checking master.lab.example.com.............................  PASS

Comparing Entries in [nfs]

 · Checking services.lab.example.com...........................  PASS

Comparing Entries in [nodes]

 · Checking master.lab.example.com.............................  PASS
 · Checking node1.lab.example.com openshift_node_labels........  PASS
 · Checking node2.lab.example.com openshift_node_labels........  PASS

Overall inventory file check: .................................  PASS

[student@workstation install-run]$ 
```

まあよさそう。


```
[student@workstation install-run]$ date ; ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml ; date
Tue Sep 24 14:09:24 JST 2019

（中略）


PLAY RECAP *****************************************************************************************************
localhost                  : ok=13   changed=0    unreachable=0    failed=0   
master.lab.example.com     : ok=600  changed=250  unreachable=0    failed=0   
node1.lab.example.com      : ok=133  changed=52   unreachable=0    failed=0   
node2.lab.example.com      : ok=133  changed=51   unreachable=0    failed=0   
services.lab.example.com   : ok=31   changed=8    unreachable=0    failed=0   
workstation.lab.example.com : ok=21   changed=0    unreachable=0    failed=0   


INSTALLER STATUS ***********************************************************************************************
Initialization             : Complete (0:00:11)
Health Check               : Complete (0:00:10)
etcd Install               : Complete (0:00:32)
NFS Install                : Complete (0:00:07)
Master Install             : Complete (0:01:14)
Master Additional Install  : Complete (0:00:34)
Node Install               : Complete (0:01:35)
Hosted Install             : Complete (0:01:21)
Web Console Install        : Complete (0:00:30)
Service Catalog Install    : Complete (0:01:16)

Tue Sep 24 14:16:58 JST 2019

```

### Executing Postinstallation Tasks
追加課題？分。

 - 各ノードがReady状態になっているか
 - 各PodsがRunning状態になっているか
 - 適当なアプリケーションをDeployしてみて試してみる。

OCコマンドは専用コマンド。オプションを無くして実行すると対話式になる。


```
[student@workstation install-run]$ oc login -u admin -p redhat https://master.lab.example.com
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

Welcome! See 'oc help' to get started.



-------------

[student@workstation install-run]$ ssh master
Last login: Tue Sep 24 14:16:57 2019 from workstation.lab.example.com
[student@master ~]$ sudo -i
[root@master ~]# oc whoami
system:admin
[root@master ~]# 

このユーザはOpenShiftのRootユーザー。証明書認証。リモートからのログインでは使えない。

---------------

この時点ではadmin/developerいずれもただの一般ユーザ。
なので、ここでadminユーザーにcluster-adminの権限を付与し、システム管理者の権限を持たせる。

[root@master ~]# oc adm policy add-cluster-role-to-user cluster-admin admin
cluster role "cluster-admin" added: "admin"

----------------

権限を付与した後のログイン後。いろいろな操作ができるガイドが応答されることがわかる。

[student@workstation install-run]$ oc login -u admin -p redhat https://master.lab.example.comLogin successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation install-run]$


--------------------------------
これが各ノードの状態

[student@workstation install-run]$ oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    14m       v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   14m       v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   14m       v1.9.1+a0ce1bc657
[student@workstation install-run]$ 
---------------------------------
これが各Podの状態
registory-consoleは、内部レジストリ。内部レジストリをブラウザで参照できたりする。

[student@workstation install-run]$ oc get pods
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-68nzl    1/1       Running   0          13m
docker-registry-1-746wt    1/1       Running   0          13m
registry-console-1-b4mdc   1/1       Running   0          12m
router-1-hgrb5             1/1       Running   0          13m
router-1-vgx5r             1/1       Running   0          13m
[student@workstation install-run]$ 

---------------------------------

[student@workstation install-run]$ oc new-project smoke-test
Now using project "smoke-test" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation install-run]$ 

-------------------
[student@workstation install-run]$ oc get routes
No resources found.
--------------------

[student@workstation install-run]$ oc new-app python:3.4~http://gitserver.example.com/my-python-app
error: the image match "python:3.4" for source repository "http://gitserver.example.com/my-python-app" does not appear to be a source-to-image builder.

- to attempt to use this image as a source builder, pass "--strategy=source"
- to use it as a base image for a Docker build, pass "--strategy=docker"

--------------

[student@workstation install-run]$ oc new-app python:3.4~http://services.lab.example.com/my-python-app
error: the image match "python:3.4" for source repository "http://services.lab.example.com/my-python-app" does not appear to be a source-to-image builder.

- to attempt to use this image as a source builder, pass "--strategy=source"
- to use it as a base image for a Docker build, pass "--strategy=docker"

-------------------
```
ここは、教材と演習環境が違うところかな？
後で講師の人に確認をしてみよう。URLとかアプリケーションが違いそう。

->この内容は、あくまでガイドっぽいな。実際の研修の内容は、次の項目で具体的に指示があった。



アプリケーションDeploy後のアクセス確認は、Workstationのなかのブラウザからアクセスすること。
DNSによる名前解決がうまくいかなくなる。(master/servicesだけはいけるけど。)

------------------



### Guided Execuse: Completing Postinstallation Tasks
```
[student@workstation ~]$ oc login -u developer
Authentication required for https://master.lab.example.com:443 (openshift)
Username: developer
Password: 
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc status
You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc new-project smoke-test
Error from server (AlreadyExists): project.project.openshift.io "smoke-test" already exists
[student@workstation ~]$ oc new-project smoke-test
Now using project "smoke-test" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


-----------------------

[student@workstation ~]$ oc new-app php:5.6~http://services.lab.example.com/php-helloworld --name hello
--> Found image 520f0e9 (17 months old) in image stream "openshift/php" under tag "5.6" for "php:5.6"

    Apache 2.4 with PHP 5.6 
    ----------------------- 
    PHP 5.6 available as container is a base platform for building and running various PHP 5.6 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php56, rh-php56

    * A source build using source code from http://services.lab.example.com/php-helloworld will be created
      * The resulting image will be pushed to image stream "hello:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    buildconfig "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Build scheduled, use 'oc logs -f bc/hello' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.

-------------------------

[student@workstation ~]$ oc logs -f bc/hello
Cloning "http://services.lab.example.com/php-helloworld" ...
	Commit:	6d61e75647124d02aa761f994532ef29eae46f8e (Establish remote repository)
	Author:	root <root@services.lab.example.com>
	Date:	Thu Aug 9 11:33:29 2018 -0700
---> Installing application source...
=> sourcing 20-copy-config.sh ...
---> 05:59:33     Processing additional arbitrary httpd configuration provided by s2i ...
=> sourcing 00-documentroot.conf ...
=> sourcing 50-mpm-tuning.conf ...
=> sourcing 40-ssl-certs.sh ...

Pushing image docker-registry.default.svc:5000/smoke-test/hello:latest ...
Pushed 0/6 layers, 1% complete
Pushed 1/6 layers, 21% complete
Pushed 2/6 layers, 37% complete
Pushed 3/6 layers, 53% complete
Pushed 4/6 layers, 84% complete
Pushed 5/6 layers, 99% complete
Pushed 6/6 layers, 100% complete
Push successful
[student@workstation ~]$ 

--------------------

oc expose svcコマンドで、外部のクライアントに対してrouteを定義する。
デフォルトでは、<appname>-<projname>-hostnameとかになる。
強制的にFQDNを宣言することも可能らしい。

[student@workstation ~]$ oc expose svc hello
route "hello" exposed
[student@workstation ~]$ oc get routes
NAME      HOST/PORT                               PATH      SERVICES   PORT       TERMINATION   WILDCARD
hello     hello-smoke-test.apps.lab.example.com             hello      8080-tcp                 None
[student@workstation ~]$ curl hello-smoke-test.apps.lab.example.com
Hello, World! php version is 5.6.25

--------------------

[student@workstation ~]$ oc status -v
In project smoke-test on server https://master.lab.example.com:443

http://hello-smoke-test.apps.lab.example.com to pod port 8080-tcp (svc/hello)
  dc/hello deploys istag/hello:latest <-
    bc/hello source builds http://services.lab.example.com/php-helloworld on openshift/php:5.6 
    deployment #1 deployed about a minute ago - 1 pod

Info:
  * dc/hello has no readiness probe to verify pods are ready to accept traffic or ensure deployment is successful.
    try: oc set probe dc/hello --readiness ...
  * dc/hello has no liveness probe to verify pods are still running.
    try: oc set probe dc/hello --liveness ...

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.


```
特に問題なく確認ができ、curlコマンドの結果も問題なく応答があった。


```
[student@workstation ~]$ oc get pods -n default
Error from server (Forbidden): pods is forbidden: User "developer" cannot list pods in the namespace "default": User "developer" cannot list pods in project "default"

developerだとdefaultのものに権限はないので表示されない。

[student@workstation ~]$ oc login -u admin
Logged into "https://master.lab.example.com:443" as "admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * smoke-test

Using project "smoke-test".
[student@workstation ~]$ oc get pods -n default
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-68nzl    1/1       Running   1          52m
docker-registry-1-746wt    1/1       Running   2          52m
registry-console-1-b4mdc   1/1       Running   3          52m
router-1-hgrb5             1/1       Running   2          53m
router-1-vgx5r             1/1       Running   1          53m

adminだといける。


ビルド中は、buildというPodsが見えるはずだが、logs -f とかで見れるコマンドは完了後なのでそのログは消えてしまう。
逆に、失敗した場合はbuild関係ログはのこる。

logs -f は、tail -f と同じような感じ。

GUIでも、Followボタンを押すことでアプリケーション側ログを観ることも可能。
Applications -> Pods -> " hello" -> Logs 

GUIでやる場合、Project name -> Catalog Select -> Apllication Name & Git URL -> 

Gitに認証がかかっている場合はこのへんはどうなる？
OCPのアカウントにGITアカウント情報などが登録可能なのか？？



```

------------

## Summary

- Prepare the enviroment for OpenShift Container Platform installation using Ansible Playbooks.
- Configure an OpenShift Advanced Installation inventory file with appropriate host groups,group variables,and host variables.
- Configure master and node servers using the OpenShift Advanced Installation Ansible Playbooks.
- Validate a running OpenShift cluster by creating an application from source code and deploying it to OpenShift.


---------------------
(15:35)
# Chapter 3 Describing and exploring openshift networking concepts


Pluginがいくつかある。デフォルトはovs-subnet
注意点、デフォルトはすべてのPod間で通信が可能。
-> ovs-multitenantを使うことでプロジェクト間の分離が可能。

ovs-networkpolicyで個別定義も可能？

セットアップがないので補足。
NetworkPluginNameの宣言。
マルチテナントに変えた場合、Project間で通信させたい場合には、コマンドで個別で定義する必要がある。
どのファイルだったか捕捉できず。cd /etc/origin/master/ これの配下か？
master-config.yaml これっぽい。

networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14
  clusterNetworks:
  - cidr: 10.128.0.0/14
    hostSubnetLength: 9
  externalIPNetworkCIDRs:
  - 0.0.0.0/0
  hostSubnetLength: 9
  networkPluginName: redhat/openshift-ovs-subnet

あった！！

DeployされるコンテナのIPは毎回変わる。
だから、Pod間でアクセスさせる場合には、svc経由でアクセスさせる。
内部で解決できるローカルの名前なんかも自動で作成してくれる。


いらないプロジェクトはこまめに消しておくこと。
マシンノードあたりのリソースもあるし、IPのバッティングなどで演習がうまくいかなったりするので。

SVCで定義されたIPに対してOCP内部でアクセスすると、起動しているどこかのPODにアクセスされる。
HOSTNAMEとかも勝手にフラレるので、それを使うことでPOD指定でアクセスしたりすることも可能。

アプリの人は、環境変数から名前を引くようにアプリを作らないといけない。


## OpenShift Network Topoligy

etcdにYAMLでファイルを書いてあげれば、svcは定義が可能。K8S触っている人はお馴染み。

サービスのロードバランすは、ラベルで紐づけてPODとくっつけている。

POD->Edit YAML 
etcdのなかのYAMLのファイルがそのまま見える。
labelsのところを見ると、app : demo
 deployment: demo-1
とかついている。

Serviceの方をのぞいてみる。
ここにも、selector:
 deploymentconfig: demo とかがついている。
このあたりで紐付をやっている。
他にも、ランタイム情報（実行時間？）の情報が直接YAMLみると掲載されていたりする。

oc export svc <svc名> > <filename>
vi <filename> -> ファイルの編集(svc名かえるとか)
oc create -f <filename > で読み込ませることでsvcを個別で作成することが出来る。


## Getting Traffic into and out of the Cluster

- HostPort/HostNetwork:
NIC->POD
PODを直接NICにBindするやりかた。講習では対象外。
PODが複製されるとポートがバッティングする可能性があるので、同じマシンノードでやろうとするとエラーになる。
セキュリティも弱めなくてはいけないが、可能は可能。普通には使わない内容。コンテナの旨みがなくなる。

- NodePort:
NIC->SVC->POD
SVCを経由してBindするやりかた。サービスを通じてロードバランスするので複数PODを立てることも可能。
HTTP以外のプロトコルを使う場合。
routerを経由するのはHTTPを使う場合のみ。これ以外はHAProxyでは実現出来無い。(FTPとか)
このようなアプリケーションの場合、NodePortを使う可能性がある。

- routes:
NIC->SVC->router->SVC->PODS
これがデフォルト。中身はHAProxy。Webプロトコル形。URL経由してコンテナへ接続してくれる。

絵にあるのはNodePortの場合。
NodePortはGUI／CLIではできないので、YAMLを直接編集が必要になる。

type: NodePortなどにする。
nodeportは、30000番台以上のポートを使う。30000-32767の範囲で使う。
書かなければ30000以上のランダムポートになる。
演習するときには、2ヶ所編集する。nodeportは忘れてもいいが、typeは忘れるとだめ。

--------------------------

## Guide Exercise: Exploring Software-Defined-Networking


```
[student@workstation ~]$ lab openshift-network setup

Checking prerequisites for GE: Exploring Software-Defined Networking

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Overall setup status...........................................  SUCCESS


-----------------

[student@workstation ~]$ oc login -u developer -p redhat https://master.lab.example.com
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc whoami
developer
[student@workstation ~]$ oc new-project network-test
Now using project "network-test" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation ~]$ 

-------------------

[student@workstation ~]$ oc new-app --name=hello -i php:7.0 http://registory.lab.example.com/scaling
error: unable to load template file "http://registory.lab.example.com/scaling": Get http://registory.lab.example.com/scaling: dial tcp: lookup registory.lab.example.com on 172.25.250.254:53: no such host
error: git ls-remote failed with: fatal: unable to access 'http://registory.lab.example.com/scaling/': Could not resolve host: registory.lab.example.com; Unknown error;  local file access failed with: stat http://registory.lab.example.com/scaling: no such file or directory
error: unable to locate any images in image streams, templates loaded in accessible projects, template files, local docker images with name "http://registory.lab.example.com/scaling"

Argument 'http://registory.lab.example.com/scaling' was classified as an image, image~source, or loaded template reference.

The 'oc new-app' command will match arguments to the following types:

  1. Images tagged into image streams in the current project or the 'openshift' project
     - if you don't specify a tag, we'll add ':latest'
  2. Images in the Docker Hub, on remote registries, or on the local Docker engine
  3. Templates in the current project or the 'openshift' project
  4. Git repository URLs or local paths that point to Git repositories

--allow-missing-images can be used to point to an image that does not exist yet.

See 'oc new-app -h' for examples.


Typo してた！！！

oc new-app --name=hello -i php:7.0 http://registry.lab.example.com/scaling

[student@workstation ~]$ oc new-app --name=hello -i php:7.0 http://registry.lab.example.com/scaling
--> Found image c101534 (2 years old) in image stream "openshift/php" under tag "7.0" for "php:7.0"

    Apache 2.4 with PHP 7.0 
    ----------------------- 
    PHP 7.0 available as docker container is a base platform for building and running various PHP 7.0 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php70, rh-php70

    * The source repository appears to match: php
    * A source build using source code from http://registry.lab.example.com/scaling will be created
      * The resulting image will be pushed to image stream "hello:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "hello"
    * Port 8080/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    buildconfig "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Build scheduled, use 'oc logs -f bc/hello' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation ~]$ 


[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-build   1/1       Running   0          14s
[student@workstation ~]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-build   1/1       Running   0          27s
[student@workstation ~]$ oc get pods
NAME            READY     STATUS      RESTARTS   AGE
hello-1-build   0/1       Completed   0          38s
hello-1-gjmbt   1/1       Running     0          3s


[student@workstation ~]$ oc scale --replicas=2 dc hello

deploymentconfig "hello" scaled
[student@workstation ~]$ 
[student@workstation ~]$ oc get pods
NAME            READY     STATUS              RESTARTS   AGE
hello-1-build   0/1       Completed           0          1m
hello-1-gjmbt   1/1       Running             0          37s
hello-1-hsfd8   0/1       ContainerCreating   0          4s


スケールされた！！！

[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS      RESTARTS   AGE       IP            NODE
hello-1-build   0/1       Completed   0          1m        10.129.0.30   node2.lab.example.com
hello-1-gjmbt   1/1       Running     0          1m        10.129.0.31   node2.lab.example.com
hello-1-hsfd8   1/1       Running     0          32s       10.128.0.16   node1.lab.example.com

ほうほう

[student@workstation ~]$ curl http://10.129.0.31:8080
^C
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ curl http://10.129.0.16:8080
^C

そのままcurlしてもアクセスはできない。

[student@workstation ~]$ ssh node1
Last login: Tue Sep 24 14:51:58 2019 from workstation.lab.example.com
[student@node1 ~]$ sudo -i
[root@node1 ~]# curl http://10.129.0.31:8080
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.129.0.31 
 </body>
</html>
[root@node1 ~]# 
[root@node1 ~]# curl http://10.128.0.16:8080
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.128.0.16 
 </body>
</html>
[root@node1 ~]# 

-> node1からは直接アクセスが可能だった。

[student@workstation ~]$ ssh node2
Last login: Tue Sep 24 14:51:58 2019 from workstation.lab.example.com
[student@node2 ~]$ sudo -i
[root@node2 ~]# curl http://10.129.0.31:8080
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.129.0.31 
 </body>
</html>
[root@node2 ~]# curl http://10.129.0.16:8080
^C
[root@node2 ~]# curl http://10.128.0.16:8080
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.128.0.16 
 </body>
</html>
[root@node2 ~]# 

-> node2からは直接アクセスが可能だった。

[student@workstation ~]$ oc whoami
developer
[student@workstation ~]$ oc get svc hello
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
hello     ClusterIP   172.30.213.25   <none>        8080/TCP   7m

[student@workstation ~]$ curl http://172.30.213.25:8080
^C
[st

やっぱりだめ。

[student@workstation ~]$ ssh root@node1 curl http://172.30.213.25:8080
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.129.0.31 
 </body>
</html>
100   106  100   106    0     0  22973      0 --:--:-- --:--:-- --:--:-- 26500

Node1からならsvcのアドレスへも行ける。

-------------------


[student@workstation ~]$ oc describe svc hello
Name:              hello
Namespace:         network-test
Labels:            app=hello
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=hello,deploymentconfig=hello
Type:              ClusterIP
IP:                172.30.213.25
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.128.0.16:8080,10.129.0.31:8080
Session Affinity:  None
Events:            <none>

------------

各コンテナの状態を確認してみる。

[student@workstation ~]$ oc describe pod hello-1-gjmbt
Name:           hello-1-gjmbt
Namespace:      network-test
Node:           node2.lab.example.com/172.25.250.12
Start Time:     Tue, 24 Sep 2019 16:07:43 +0900
Labels:         app=hello
                deployment=hello-1
                deploymentconfig=hello
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=hello
                openshift.io/deployment.name=hello-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Running
IP:             10.129.0.31
Controlled By:  ReplicationController/hello-1
Containers:
  hello:
    Container ID:   docker://e3caa8fa9ff4b975eca08ee7d1e23e0463c5898089466b03091e3c5a2b620471
    Image:          docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae
    Image ID:       docker-pullable://docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae
    Port:           8080/TCP
    State:          Running
      Started:      Tue, 24 Sep 2019 16:07:45 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7cxg8 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-7cxg8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-7cxg8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              9m    default-scheduler               Successfully assigned hello-1-gjmbt to node2.lab.example.com
  Normal  SuccessfulMountVolume  9m    kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-7cxg8"
  Normal  Pulling                9m    kubelet, node2.lab.example.com  pulling image "docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae"
  Normal  Pulled                 9m    kubelet, node2.lab.example.com  Successfully pulled image "docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae"
  Normal  Created                9m    kubelet, node2.lab.example.com  Created container
  Normal  Started                9m    kubelet, node2.lab.example.com  Started container


------------------------------


[student@workstation ~]$ oc describe pod hello-1-hsfd8
Name:           hello-1-hsfd8
Namespace:      network-test
Node:           node1.lab.example.com/172.25.250.11
Start Time:     Tue, 24 Sep 2019 16:08:16 +0900
Labels:         app=hello
                deployment=hello-1
                deploymentconfig=hello
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=hello
                openshift.io/deployment.name=hello-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Running
IP:             10.128.0.16
Controlled By:  ReplicationController/hello-1
Containers:
  hello:
    Container ID:   docker://95ef148525b13055ae474c4bfdbbf4eec5efccd3b3671c610de77dd3ae9c584e
    Image:          docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae
    Image ID:       docker-pullable://docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae
    Port:           8080/TCP
    State:          Running
      Started:      Tue, 24 Sep 2019 16:08:30 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7cxg8 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-7cxg8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-7cxg8
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              9m    default-scheduler               Successfully assigned hello-1-hsfd8 to node1.lab.example.com
  Normal  SuccessfulMountVolume  9m    kubelet, node1.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-7cxg8"
  Normal  Pulling                9m    kubelet, node1.lab.example.com  pulling image "docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae"
  Normal  Pulled                 9m    kubelet, node1.lab.example.com  Successfully pulled image "docker-registry.default.svc:5000/network-test/hello@sha256:17cee5e1f46c4fe5421c41c8b7f08c76f352860c8ab406acf331407e7d70acae"
  Normal  Created                9m    kubelet, node1.lab.example.com  Created container
  Normal  Started                9m    kubelet, node1.lab.example.com  Started container

---------------------------

ほーん


-----------------

oc edit svc hello

初期状態

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-24T07:07:07Z
  labels:
    app: hello
  name: hello
  namespace: network-test
  resourceVersion: "17343"
  selfLink: /api/v1/namespaces/network-test/services/hello
  uid: ebd86791-de99-11e9-98ef-52540000fa0a
spec:
  clusterIP: 172.30.213.25
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
    deploymentconfig: hello
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}


-------------------

かえてみた。

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-24T07:07:07Z
  labels:
    app: hello
  name: hello
  namespace: network-test
  resourceVersion: "19212"
  selfLink: /api/v1/namespaces/network-test/services/hello
  uid: ebd86791-de99-11e9-98ef-52540000fa0a
spec:
  clusterIP: 172.30.213.25
  externalTrafficPolicy: Cluster
  ports:
  - name: 8080-tcp
    nodePort: 30800
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
    deploymentconfig: hello
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}


-------------------------
[student@workstation ~]$ oc describe svc hello
Name:                     hello
Namespace:                network-test
Labels:                   app=hello
Annotations:              openshift.io/generated-by=OpenShiftNewApp
Selector:                 app=hello,deploymentconfig=hello
Type:                     NodePort
IP:                       172.30.213.25
Port:                     8080-tcp  8080/TCP
TargetPort:               8080/TCP
NodePort:                 8080-tcp  30800/TCP
Endpoints:                10.128.0.16:8080,10.129.0.31:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

-> TypeがNodePortに変わった！！

[student@workstation ~]$ curl http://node1.lab.example.com:30800
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.128.0.16 
 </body>
</html>
[student@workstation ~]$ curl http://node2.lab.example.com:30800
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.128.0.16 
 </body>
</html>

いけた！！

[student@workstation ~]$ curl http://node2.lab.example.com:30800
<html>
 <head>
  <title>PHP Test</title>
 </head>
 <body>
 <br/> Server IP: 10.129.0.31 
 </body>
</html>
[student@

31がかえってくることもある。



----------------

[student@workstation ~]$ oc rsh hello-1-hsfd8
sh-4.2$ curl http://services.lab.example.com
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<!-- git web interface version 1.8.3.1, (C) 2005-2006, Kay Sievers <kay.sievers@vrfy.org>, Christian Gierke -->
<!-- git core binaries version 1.8.3.1 -->
<head>


--

お掃除。


[student@workstation ~]$ oc delete project network-test
project "network-test" deleted
[student@workstation ~]$ whoami
student
[student@workstation ~]$ oc status
In project network-test on server https://master.lab.example.com:443

svc/hello (all nodes):30800 -> 8080
  dc/hello deploys istag/hello:latest <-
    bc/hello source builds http://registry.lab.example.com/scaling on openshift/php:7.0 
    deployment #1 deployed 16 minutes ago - 2 pods


2 infos identified, use 'oc status -v' to see details.
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc status
Error from server (Forbidden): projects.project.openshift.io "network-test" is forbidden: User "developer" cannot get projects.project.openshift.io in the namespace "network-test": User "developer" cannot get project "network-test"

この動作は、最後のプロジェクト情報などをConfigとして保存しているための模様。
ファイルとしては、.kube/config の配下。
で、その状態でstatusコマンドを叩いたので、configの中の情報から読み込んで、
そのプロジェクトが既に存在しないため、Forbiddenとしてエラーになっている。



[student@workstation ~]$ cat .kube/config 
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://master.lab.example.com:443
  name: master-lab-example-com:443
contexts:
- context:
    cluster: master-lab-example-com:443
    user: admin/master-lab-example-com:443
  name: /master-lab-example-com:443/admin
- context:
    cluster: master-lab-example-com:443
    user: developer/master-lab-example-com:443
  name: /master-lab-example-com:443/developer
- context:
    cluster: master-lab-example-com:443
    namespace: default
    user: admin/master-lab-example-com:443
  name: default/master-lab-example-com:443/admin
- context:
    cluster: master-lab-example-com:443
    namespace: network-test
    user: developer/master-lab-example-com:443
  name: network-test/master-lab-example-com:443/developer
- context:
    cluster: master-lab-example-com:443
    namespace: smoke-test
    user: admin/master-lab-example-com:443
  name: smoke-test/master-lab-example-com:443/admin
- context:
    cluster: master-lab-example-com:443
    namespace: smoke-test
    user: developer/master-lab-example-com:443
  name: smoke-test/master-lab-example-com:443/developer
current-context: network-test/master-lab-example-com:443/developer
kind: Config
preferences: {}
users:
- name: admin/master-lab-example-com:443
  user:
    token: MGgMsWwJSkOMvDCf54fLGU1iVNdBE19tTOMpN_li0Ec
- name: developer/master-lab-example-com:443
  user:
    token: lTBR2vqwulyIKQf0oOgfLHOl-wbh5TA-hyAHDPhFwa4




```



----------------------

