---
title: DO280ｰDAY2
date: 2019-09-26T05:24:00.000Z
id: "26006613440408083"
draft: true
---
# 昨日の訂正

SDNのrouteの構成は、
物理NIC->svc->router->svc->Pod
ではなく
物理NIC->router->svc->Pod
となる。routerの手前のsvcも存在する。

このことから、routerは特定のNODEで動かす必要がある。（冗長構成も可能ではある）

------------------------------

## Creating Route (P80)

外からリクエスト受けると、routerはETCDに対してサービスエンドポイントのクエリをしにいく。
その後、routerPODからロードバランスし、サービスのPODへ繋がっている。
svc経由で行っているわけではなく、あくまでエンドポイントを探しているだけ。
接続はrouter->POD。なるほど。

routeのYAMLファイルの説明(P81)
routeリソースはOCP専用リソース。K8Sには存在しない。(ingressというあとから追加された機能はある)
4.台からは両方使える。routeリソースの方が歴史が古いこともあり、より高機能。
OCPではrouteリソースを使うことを推奨。

routeは、YAML書かなくてもいい。
oc exposeというコマンドを使うことで設定が可能。

serviceをexposeするとrouteができる。
podをexposeするとserviceができる。

--hostname をつけると個別のFQDN指定が可能。

oc new-appコマンドではrouteは造られないので、exposeする必要がある。
GUIでは自動で作成してくれる。

routerPODがhostnameの名前解決を出来ない場合、404がでる。アクセスできないとかではない。

master-config.yaml内の中のroutingConfigを変えることであとからsubdomainを変更することは可能。
もちろん、古い名前で作ったものはそのまま。

対応するプロトコル
- HTTP
- HTTPS With SNI
- WebSockets
- TLS With SNI

サーバ証明書をどうするか？3つのパターンがある。
- Edge Termination
	Routerに入れる。
- Pass-through
	コンテナまでスルー
- Re-Encryption Termination
	routerで終端しつつ、再暗号化


サーバ証明書の指定方法
```
oc create route edge --service=<servicename> --hostname=<hostname> --key=<keypath> --cert=<certpath>
```

ワイルドカード証明書の設定も可能
1つのサービスに複数のFQDNを付ける場合、DNS側で頑張ってもいいし、router側で対応することも可能。

- まずdcのレプリカ数を0にして、routerをシャットダウンする。
- 次に、環境変数としてROUTER_ALLOW_WILDCARD_ROUTERS=true を入れる。
- その後、dcのレプリカ数を1にすることで、環境変数に先程の設定が埋め込まれた状態でrouterPODが起動する。(OS環境変数を拾って起動するので、これでワイルドカードが有効になる)

oc expose svc test --wildcard-poloicy=Subdomain --hostname='www.lab.example.com'
先頭の部分はなんでもいい。

HAPROXYの統計ページの紹介。
```
# oc env pod <router-pod> --list | tail -n 6

GUIでも見れる。
```

ユーザ名、パスワード、ポート番号などがENVでかいてある。デフォルトのポート番号は1936。
デフォルトではfirewalldで閉じられているので、firewalldで開放してやる必要がある。

------------------------------

## Guided Execise: Creating a Route(P86)

試験はGuideではないLABから出題されるよ！！

- pre

```
[student@workstation ~]$ lab secure-route setup

Checking prerequisites for GE: Create a Route

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for GE: Create a Route

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.

Overall setup status...........................................  SUCCESS

これを実行する前、masterノードで別ユーザに切り替えてしまうと失敗することがある。
なので、system:adminにしておくこと。

```

1.1.
```
[student@workstation ~]$ oc login -u developer -p redhat https://master.lab.example.com
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc whoami
developer


```
1.2.
```
[student@workstation ~]$ oc new-project secure-route
Now using project "secure-route" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

```
2.1

```
[student@workstation ~]$ oc new-app --docker-image=registry.lab.example.com/openshift/hello-openshift --name=hello
--> Found Docker image 7af3297 (17 months old) from registry.lab.example.com for "registry.lab.example.com/openshift/hello-openshift"

    * An image stream will be created as "hello:latest" that will track this image
    * This image will be deployed in deployment config "hello"
    * Ports 8080/tcp, 8888/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.

```

2.2.

```
[student@workstation ~]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP            NODE
hello-1-6chxq   1/1       Running   0          20s       10.128.0.17   node1.lab.example.com

```

3.1.

```
[student@workstation ~]$ cat /home/student/DO280/labs/secure-route/create-cert.sh
#!/bin/bash

echo "Generating a private key..."
openssl genrsa -out hello.apps.lab.example.com.key 2048
echo

echo "Generating a CSR..."
openssl req -new -key hello.apps.lab.example.com.key -out hello.apps.lab.example.com.csr -subj "/C=US/ST=NC/L=Raleigh/O=RedHat/OU=RHT/CN=hello.apps.lab.example.com"
echo

echo "Generating a certificate..."
openssl x509 -req -days 366 -in hello.apps.lab.example.com.csr -signkey hello.apps.lab.example.com.key -out hello.apps.lab.example.com.crt
echo
echo  "DONE."
echo
[student@workstation ~]$ 
```

3.2.

```
[student@workstation ~]$ cd /home/student/DO280/labs/secure-route
[student@workstation secure-route]$ ls
commands.txt  create-cert.sh

[student@workstation secure-route]$ ./create-cert.sh 
Generating a private key...
Generating RSA private key, 2048 bit long modulus
.......................................................................................+++
....................................................+++
e is 65537 (0x10001)

Generating a CSR...

Generating a certificate...
Signature ok
subject=/C=US/ST=NC/L=Raleigh/O=RedHat/OU=RHT/CN=hello.apps.lab.example.com
Getting Private key

DONE.

[student@workstation secure-route]$ ls -la
total 20
drwxr-xr-x. 2 student student  162 Sep 25 09:59 .
drwxr-xr-x. 5 student student   68 Sep 25 09:54 ..
-rw-r--r--. 1 student student  550 Aug  7  2018 commands.txt
-rwxr-xr-x. 1 student student  506 Jul 19  2018 create-cert.sh
-rw-rw-r--. 1 student student 1224 Sep 25 09:59 hello.apps.lab.example.com.crt
-rw-rw-r--. 1 student student 1017 Sep 25 09:59 hello.apps.lab.example.com.csr
-rw-rw-r--. 1 student student 1675 Sep 25 09:59 hello.apps.lab.example.com.key


```

4.1
```
[student@workstation secure-route]$ oc create route edge --service=hello --hostname=hello.apps.lab.example.com --key=hello.apps.lab.example.com.key --cert=hello.apps.lab.example.com.crt 
route "hello" created
[student@workstation s
```

4.2.
WILDCARDの記述はあるが、NONE
```
[student@workstation secure-route]$ oc get route
NAME      HOST/PORT                    PATH      SERVICES   PORT       TERMINATION   WILDCARD
hello     hello.apps.lab.example.com             hello      8080-tcp   edge          None

```

4.3.

```
[student@workstation secure-route]$ oc get route/hello -o yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  creationTimestamp: 2019-09-25T01:01:03Z
  labels:
    app: hello
  name: hello
  namespace: secure-route
  resourceVersion: "151247"
  selfLink: /apis/route.openshift.io/v1/namespaces/secure-route/routes/hello
  uid: f266e542-df2f-11e9-98ef-52540000fa0a
spec:
  host: hello.apps.lab.example.com
  port:
    targetPort: 8080-tcp
  tls:
    certificate: |
      -----BEGIN CERTIFICATE-----
      MIIDXDCCAkQCCQCoZ/xg3iOOfzANBgkqhkiG9w0BAQsFADBwMQswCQYDVQQGEwJV
      UzELMAkGA1UECAwCTkMxEDAOBgNVBAcMB1JhbGVpZ2gxDzANBgNVBAoMBlJlZEhh
      dDEMMAoGA1UECwwDUkhUMSMwIQYDVQQDDBpoZWxsby5hcHBzLmxhYi5leGFtcGxl
      LmNvbTAeFw0xOTA5MjUwMDU5NDdaFw0yMDA5MjUwMDU5NDdaMHAxCzAJBgNVBAYT
      AlVTMQswCQYDVQQIDAJOQzEQMA4GA1UEBwwHUmFsZWlnaDEPMA0GA1UECgwGUmVk
      SGF0MQwwCgYDVQQLDANSSFQxIzAhBgNVBAMMGmhlbGxvLmFwcHMubGFiLmV4YW1w
      bGUuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmjW8gSZxWDDY
      3DBA+MJItCLoFvbDlhFDZYbDw+gOU0VQSBLUAdf1w5lv4de+JQel2MjkaXRzMRsv
      DPLbL0OsY/cmeHQaKmLJsRdgD5IWqiP2QVtCE7FV2Mb13wc6sRzWHZyDU7+UnHQ9
      TOmhpPjthAVsPPoQlbque5UlODOhPsls8UjNTNCc5fzFEglBnrAD6+GsY4KPkCeu
      2Dhciudlk5ZGzVtFlI64iTfjZKGOozUCdvqOSslvd2d5hbwtGc2aJE8KXBVqWLFx
      9id/+3P1y7+ZCxGr9vcoRGxMlOTkIDS8WOwqUevWUIZt6GmOas8PBzm7Kxg6cDHz
      g0v4elPaVQIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQAtxgMdfsZCoq0t55C2O0SF
      Ou8zDnZSOgw+wxzMR9utBKUsY/SyqWS73gIDsqC8vPTMhs1PFTi0JYuhKdWOSgKI
      59i7sJGnD8TSWcDh067qeyISfFZfgFFo4CnJ4WSoP6RttNNjpy8wIe9T/goCFx9f
      O4mM+iVdqCc3ugdpA1UiA0Z7RhtG/ts/t7+R4IWtUsjXtYBc0EZE41cY01I0Ui7Q
      wO7Fcx9ZUEw8wvYs2Q3X5llINiuKgruwaKYSpKLgRNC70fMNk050EDeY0VpMWzrr
      3fNYfSmFj+wYACsEx4ahrIbmug6c8/4GAuiVO69ov12WabB87YZRS49ykoPd7NBb
      -----END CERTIFICATE-----
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      MIIEowIBAAKCAQEAmjW8gSZxWDDY3DBA+MJItCLoFvbDlhFDZYbDw+gOU0VQSBLU
      Adf1w5lv4de+JQel2MjkaXRzMRsvDPLbL0OsY/cmeHQaKmLJsRdgD5IWqiP2QVtC
      E7FV2Mb13wc6sRzWHZyDU7+UnHQ9TOmhpPjthAVsPPoQlbque5UlODOhPsls8UjN
      TNCc5fzFEglBnrAD6+GsY4KPkCeu2Dhciudlk5ZGzVtFlI64iTfjZKGOozUCdvqO
      Sslvd2d5hbwtGc2aJE8KXBVqWLFx9id/+3P1y7+ZCxGr9vcoRGxMlOTkIDS8WOwq
      UevWUIZt6GmOas8PBzm7Kxg6cDHzg0v4elPaVQIDAQABAoIBAGph5cAG4Cxhzkjg
      NQInLXavmgK1iyoMkL0KYiTVAoTpjT+bVYxMCmw8fpWDetYa14uc0w342rqmsSey
      pgiOG+/+gRiZp69T5SX5JTi4pFANbQWbShNBxCI+50FLwG6MoR1eaq6svH3OJrfX
      eL7RRzFLeGb8b9KLLXFt63ZBD9y6j/to5QRa8yoRWew1jG8Sy1dsFh4hjOmp+zLv
      TIGHlvBAW17BG8/6O0gifTVDSJTeQl9/Fc5Y/zT7QStGbRzO1FeG5kXjaJ9NUme6
      zvdgRWjMHk6B/ZhIFegzwQJgOE02cp3W+QWRpbBDDaTy5lnpFcPqUGoqXbNVRz8N
      PbnJ4iECgYEAx+/RDv9cYSoGryORhqazppNXXf+/0iE2fxNkKujlNu5hChP14G9C
      YmbNRGm6KTvdJrXGTLs5/rkYRznRXNvEle1/YmjSGqYyEWXQPfJM2JMgVKSeU+PZ
      Kq76Cfx21ddszyPLS9lBlUx3MW4IGaAsZhQbeHVlD9t6GIJ4kkp9QEkCgYEAxXN6
      yYvqXR/7yxm8kzihpV0p+ba+gRmVPghjm2rp1lBSeVg17KxcmOQu2mQIo/0Yka/h
      ZEegiChEHr3SuUHv2LVVsjHSf2X/62vLfwSiBad1nUuddEtSxhjNnXg1u/23fWLT
      Z0JR3nr4WAxHcoiJVBgH0aFREp6fKhZVkPc2Ia0CgYBxzIgblzFHhJolWYNdNskO
      SNLzh7vwqELSdYEQA1tjtq65A79xLSKFy6mOBPeWvKIVhGUIC5Do8QVmlPwbdGMi
      Svm0U6Ey1eGtkVvDxoQCyUsfoksTyJ408z8SLAaflGw0QlVWKKRxjAJTuiMWItri
      hxNTrs4FBpEqHijua0N7MQKBgDjfe2WkTHxIYyFsJ1oMfNsBy75muda7xLYaen5g
      vDwD5M2Y7+dc+kr7ptc26xo5I6OLEpPs4bt2RbDJInJEIM695JIwMZ6Khb2Mzg+n
      CPgx9Pm6tv6xV4VBza/m2Stt+DVAiv36J1S6/N93ofjGhS/YwB/MpGqFj/jyd/EK
      k+f9AoGBAIKu6PgxQgwgdkDBxPzbEOhWWz7EtkXA3LeFJogHVDhX6vR0AqjLTuO6
      YP39cYb24glgJRHo6uvC/7P6e06ocBoNdbQN8/4O6GSLznHjbPkH889zuL5RkcS/
      LO9jPApseTApE727eQuJCatWCHIqUetr2E+Uz5O6BYQvrdxWH9gO
      -----END RSA PRIVATE KEY-----
    termination: edge
  to:
    kind: Service
    name: hello
    weight: 100
  wildcardPolicy: None
status:
  ingress:
  - conditions:
    - lastTransitionTime: 2019-09-25T01:01:03Z
      status: "True"
      type: Admitted
    host: hello.apps.lab.example.com
    routerName: router
    wildcardPolicy: None
[student@workstation secure-route]$ 


```

5.1

```
[student@workstation secure-route]$ curl http://hello.apps.lab.example.com
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <style type="text/css">
  /*!
   * Bootstrap v3.3.5 (http://getbootstrap.com)
   * Copyright 2011-2015 Twitter, Inc.
   * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
   */
  /*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
  html {
    font-family: sans-serif;
    -ms-text-size-adjust: 100%;
    -webkit-text-size-adjust: 100%;
  }
  body {
    margin: 0;
  }
  h1 {
    font-size: 1.7em;
    font-weight: 400;
    line-height: 1.3;
    margin: 0.68em 0;
  }
  * {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    box-sizing: border-box;
  }
  *:before,
  *:after {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    box-sizing: border-box;
  }
  html {
    -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
  }
  body {
    font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    line-height: 1.66666667;
    font-size: 13px;
    color: #333333;
    background-color: #ffffff;
    margin: 2em 1em;
  }
  p {
    margin: 0 0 10px;
    font-size: 13px;
  }
  .alert.alert-info {
    padding: 15px;
    margin-bottom: 20px;
    border: 1px solid transparent;
    background-color: #f5f5f5;
    border-color: #8b8d8f;
    color: #363636;
    margin-top: 30px;
  }
  .alert p {
    padding-left: 35px;
  }
  a {
    color: #0088ce;
  }

  ul {
    position: relative;
    padding-left: 51px;
  }
  p.info {
    position: relative;
    font-size: 15px;
    margin-bottom: 10px;
  }
  p.info:before, p.info:after {
    content: "";
    position: absolute;
    top: 9%;
    left: 0;
  }
  p.info:before {
    content: "i";
    left: 3px;
    width: 20px;
    height: 20px;
    font-family: serif;
    font-size: 15px;
    font-weight: bold;
    line-height: 21px;
    text-align: center;
    color: #fff;
    background: #4d5258;
    border-radius: 16px;
  }

  @media (min-width: 768px) {
    body {
      margin: 4em 3em;
    }
    h1 {
      font-size: 2.15em;}
  }

  </style>
  </head>
  <body>
    <div>
      <h1>Application is not available</h1>
      <p>The application is currently not serving requests at this endpoint. It may not have been started or is still starting.</p>

      <div class="alert alert-info">
        <p class="info">
          Possible reasons you are seeing this page:
        </p>
        <ul>
          <li>
            <strong>The host doesn't exist.</strong>
            Make sure the hostname was typed correctly and that a route matching this hostname exists.
          </li>
          <li>
            <strong>The host exists, but doesn't have a matching path.</strong>
            Check if the URL path was typed correctly and that the route was created using the desired path.
          </li>
          <li>
            <strong>Route and path matches, but all pods are down.</strong>
            Make sure that the resources exposed by this route (pods, services, deployment configs, etc) have at least one pod running.
          </li>
        </ul>
      </div>
    </div>
  </body>
</html>
[student@workstation secure-route]$ 

```

5.2
```
[student@workstation secure-route]$ curl -k -vvv https://hello.apps.lab.example.com
* About to connect() to hello.apps.lab.example.com port 443 (#0)
*   Trying 172.25.250.11...
* Connected to hello.apps.lab.example.com (172.25.250.11) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
* 	subject: CN=hello.apps.lab.example.com,OU=RHT,O=RedHat,L=Raleigh,ST=NC,C=US
* 	start date: Sep 25 00:59:47 2019 GMT
* 	expire date: Sep 25 00:59:47 2020 GMT
* 	common name: hello.apps.lab.example.com
* 	issuer: CN=hello.apps.lab.example.com,OU=RHT,O=RedHat,L=Raleigh,ST=NC,C=US
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: hello.apps.lab.example.com
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Wed, 25 Sep 2019 01:03:48 GMT
< Content-Length: 17
< Content-Type: text/plain; charset=utf-8
< Set-Cookie: 0dca6369ebce37a9206a19316b32350e=aabefa7f3465ffcd907245e8dac9df2f; path=/; HttpOnly; Secure
< Cache-control: private
< 
Hello OpenShift!
* Connection #0 to host hello.apps.lab.example.com left intact

```

5.3. Node1での確認。

```
[student@workstation secure-route]$ ssh node1 curl -vvv http://10.128.0.17:8080
* About to connect() to 10.128.0.17 port 8080 (#0)
*   Trying 10.128.0.17...
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 10.128.0.17 (10.128.0.17) port 8080 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 10.128.0.17:8080
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Wed, 25 Sep 2019 01:06:34 GMT
< Content-Length: 17
< Content-Type: text/plain; charset=utf-8
< 
{ [data not shown]
100    17  100    17    0     0  19675      0 --:--:-- --:--:-- --:--:-- 17000
* Connection #0 to host 10.128.0.17 left intact
Hello OpenShift!
[student@workstation secure-route]$ 

```

cleanup

```
[student@workstation secure-route]$ oc delete project secure-route
project "secure-route" deleted
[student@workstation secure-route]$ 


```

-------------------------------------------------------

## Lab: Exploring OpenShift Networking Concepts

- pre

```
[student@workstation secure-route]$ lab network-review setup

Checking prerequisites for Lab: Exploring OpenShift Networking

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS
 Setting up for the lab: 
 . Logging in as the developer user............................  SUCCESS
 . Creating the network-review project.........................  SUCCESS
 . Creating resources for the network-review project...........  SUCCESS

 Back to OpenShift as system:admin.............................  SUCCESS

Overall setup status...........................................  SUCCESS


```

1.labコマンドで以降の課題に必要なプロジェクト作成やアプリケーションの作成をやってくれるよ。

2.各PODなどの状態を確認せよ
2.1 PODのIPアドレスを確認せよ
```
[student@workstation secure-route]$ oc get pods network-review
Error from server (Forbidden): pods "network-review" is forbidden: User "developer" cannot get pods in the namespace "secure-route": User "developer" cannot get pods in project "secure-route"
[student@workstation secure-route]$ oc whoami
developer

-> 前の情報が残ってるのか。切り替えはどうやるのか。。。

[student@workstation secure-route]$ oc project network-review
Now using project "network-review" on server "https://master.lab.example.com:443".
[student@workstation secure-route]$ oc get pods
NAME                      READY     STATUS    RESTARTS   AGE
hello-openshift-1-wrxsb   1/1       Running   0          7m


きたーーーーーーこれだ！

[student@workstation secure-route]$ oc get pods -o wide
NAME                      READY     STATUS    RESTARTS   AGE       IP            NODE
hello-openshift-1-wrxsb   1/1       Running   0          8m        10.129.0.33   node2.lab.example.com
[student@workstation secure-route]$ 

IPは10.129.0.33

```

2.2 ClusterのIPアドレスを確認せよ

```
[student@workstation secure-route]$ oc get service
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
hello-openshift   ClusterIP   172.30.214.57   <none>        8080/TCP,8888/TCP   10m

```

2.3 routesが作成されたことを確認せよ

```
[student@workstation secure-route]$ oc get routes
NAME              HOST/PORT                    PATH      SERVICES         PORT       TERMINATION   WILDCARD
hello-openshift   hello.apps.lab.example.com             hello-opensift   8080-tcp                 None
```

memo
- Container IP: 10.129.0.33
- Cluster IP: 172.30.214.57
- routes URL: hello.apps.lab.example.com

3. curlコマンドでroute URLにアクセスせよ。応答はあるが、サービスはできない状態であるはず。
```
[student@workstation secure-route]$ curl hello.apps.lab.example.com -v
* About to connect() to hello.apps.lab.example.com port 80 (#0)
*   Trying 172.25.250.11...
* Connected to hello.apps.lab.example.com (172.25.250.11) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: hello.apps.lab.example.com
> Accept: */*
> 
* HTTP 1.0, assume close after body
< HTTP/1.0 503 Service Unavailable
< Pragma: no-cache
< Cache-Control: private, max-age=0, no-cache, no-store
< Connection: close
< Content-Type: text/html
< 
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">

-> 503エラーでたので想定どおりか？

```
4. routeが失敗している原因のトラブルシュートをせよ

4.1 curlコマンドでPODにダイレクトアクセスして状態を確認せよ

```
[student@workstation secure-route]$ curl -v 10.129.0.33
* About to connect() to 10.129.0.33 port 80 (#0)
*   Trying 10.129.0.33...
^C
[student@workstation secure-route]$ curl -v http://10.129.0.33:8080
* About to connect() to 10.129.0.33 port 8080 (#0)
*   Trying 10.129.0.33...
^C
[student@workstation secure-route]$ curl -v http://10.129.0.33:8888
* About to connect() to 10.129.0.33 port 8888 (#0)
*   Trying 10.129.0.33...
^C

ー＞アクセスはできない、が正解か？

[student@workstation secure-route]$ oc describe pod hello-openshift-1-wrxsb
Name:         hello-openshift-1-wrxsb
Namespace:    network-review
Node:         node2.lab.example.com/172.25.250.12
Start Time:   Wed, 25 Sep 2019 10:08:52 +0900
Labels:       app=hello-openshift
              deployment=hello-openshift-1
              deploymentconfig=hello-openshift
Annotations:  kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"network-review","name":"hello-openshift-1","uid":"5e91df96-57e1-11e7-9...
              openshift.io/deployment-config.latest-version=1
              openshift.io/deployment-config.name=hello-openshift
              openshift.io/deployment.name=hello-openshift-1
              openshift.io/generated-by=OpenShiftNewApp
              openshift.io/scc=restricted
Status:       Running
IP:           10.129.0.33
Containers:
  hello-openshift:
    Container ID:   docker://5717fc947f267e7ed3d78b9c0f442359c4b26819ac0e08617ec856bcfcc14e7f
    Image:          registry.lab.example.com/openshift/hello-openshift:latest
    Image ID:       docker-pullable://registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    Ports:          8888/TCP, 8080/TCP
    State:          Running
      Started:      Wed, 25 Sep 2019 10:08:55 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-69hwt (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-69hwt:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-69hwt
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              21m   default-scheduler               Successfully assigned hello-openshift-1-wrxsb to node2.lab.example.com
  Normal  SuccessfulMountVolume  21m   kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-69hwt"
  Normal  Pulling                21m   kubelet, node2.lab.example.com  pulling image "registry.lab.example.com/openshift/hello-openshift:latest"
  Normal  Pulled                 21m   kubelet, node2.lab.example.com  Successfully pulled image "registry.lab.example.com/openshift/hello-openshift:latest"
  Normal  Created                21m   kubelet, node2.lab.example.com  Created container
  Normal  Started                21m   kubelet, node2.lab.example.com  Started container

[student@workstation secure-route]$ ssh node1 curl http://10.129.0.33:8888
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    17  Hello OpenShift!   0      0      0 --:--:-- --:--:-- --:--:--     0
100    17    0     0  16314      0 --:--:-- --:--:-- --:--:-- 17000
[student@workstation secure-route]$ ssh node1 curl http://10.129.0.33:8080
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Hello OpenShift!
100    17  100    17    0     0  17634      0 --:--:-- --:--:-- --:--:-- 17000
[student@workstation secure-route]$ 


```

4.2 curlコマンドでcluster ip addressにアクセスし正しい応答を返さない事を確認せよ。なので、svcで何か問題があるはずだ。

ってことは、前のはだめなんだな。ちょっとやり直そう。
わかった。Nodeからいかないとだめなんだった。



```
[student@workstation secure-route]$ curl -v http://172.30.214.57:8888/
* About to connect() to 172.30.214.57 port 8888 (#0)
*   Trying 172.30.214.57...
^C
[student@workstation secure-route]$ curl -v http://172.30.214.57/
* About to connect() to 172.30.214.57 port 80 (#0)
*   Trying 172.30.214.57...
^C
[student@workstation secure-route]$ curl -v http://172.30.214.57:8080/
* About to connect() to 172.30.214.57 port 8080 (#0)
*   Trying 172.30.214.57...
^C

うん。だめだね。

```

4.3 Workstation から、oc describe svc コマンドを使ってサービスの状態を確認せよ。

```
[student@workstation secure-route]$ oc describe svc hello-openshift
Name:              hello-openshift
Namespace:         network-review
Labels:            app=hello-openshift
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=hello_openshift,deploymentconfig=hello-openshift
Type:              ClusterIP
IP:                172.30.214.57
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         <none>
Port:              8888-tcp  8888/TCP
TargetPort:        8888/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>

Type ClusterIPなのが原因か？endPoingがNONEなのもあやしいな。
```
4.4 oc describe pod コマンドを使ってラベル情報を確認せよ

```
[student@workstation secure-route]$ oc describe pod
Name:         hello-openshift-1-wrxsb
Namespace:    network-review
Node:         node2.lab.example.com/172.25.250.12
Start Time:   Wed, 25 Sep 2019 10:08:52 +0900
Labels:       app=hello-openshift
              deployment=hello-openshift-1
              deploymentconfig=hello-openshift
Annotations:  kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"network-review","name":"hello-openshift-1","uid":"5e91df96-57e1-11e7-9...
              openshift.io/deployment-config.latest-version=1
              openshift.io/deployment-config.name=hello-openshift
              openshift.io/deployment.name=hello-openshift-1
              openshift.io/generated-by=OpenShiftNewApp
              openshift.io/scc=restricted
Status:       Running
IP:           10.129.0.33
Containers:
  hello-openshift:
    Container ID:   docker://5717fc947f267e7ed3d78b9c0f442359c4b26819ac0e08617ec856bcfcc14e7f
    Image:          registry.lab.example.com/openshift/hello-openshift:latest
    Image ID:       docker-pullable://registry.lab.example.com/openshift/hello-openshift@sha256:aaea76ff622d2f8bcb32e538e7b3cd0ef6d291953f3e7c9f556c1ba5baf47e2e
    Ports:          8888/TCP, 8080/TCP
    State:          Running
      Started:      Wed, 25 Sep 2019 10:08:55 +0900
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-69hwt (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  default-token-69hwt:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-69hwt
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              29m   default-scheduler               Successfully assigned hello-openshift-1-wrxsb to node2.lab.example.com
  Normal  SuccessfulMountVolume  29m   kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-69hwt"
  Normal  Pulling                29m   kubelet, node2.lab.example.com  pulling image "registry.lab.example.com/openshift/hello-openshift:latest"
  Normal  Pulled                 29m   kubelet, node2.lab.example.com  Successfully pulled image "registry.lab.example.com/openshift/hello-openshift:latest"
  Normal  Created                29m   kubelet, node2.lab.example.com  Created container
  Normal  Started                29m   kubelet, node2.lab.example.com  Started container


Labels:       app=hello-openshift
ここか？

```

4.5 サービスのConfigを修正せよ。
hello-openshift svcのapp attribute をhello-openshiftに変更せよ。

```
[student@workstation secure-route]$ oc edit svc hello-openshift
service "hello-openshift" edited


  selector:
    app: hello_openshift
    deploymentconfig: hello-openshift
  sessionAffinity: None
  type: ClusterIP

ここか


[student@workstation secure-route]$ oc describe svc hello-openshift
Name:              hello-openshift
Namespace:         network-review
Labels:            app=hello-openshift
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=hello-openshift,deploymentconfig=hello-openshift
Type:              ClusterIP
IP:                172.30.214.57
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.129.0.33:8080
Port:              8888-tcp  8888/TCP
TargetPort:        8888/TCP
Endpoints:         10.129.0.33:8888
Session Affinity:  None
Events:            <none>

かわった。

```

4.6 Masterから確認せよ

```
[root@master ~]# curl -v http://172.30.214.57:8080/
* About to connect() to 172.30.214.57 port 8080 (#0)
*   Trying 172.30.214.57...
* Connected to 172.30.214.57 (172.30.214.57) port 8080 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 172.30.214.57:8080
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Wed, 25 Sep 2019 01:47:03 GMT
< Content-Length: 17
< Content-Type: text/plain; charset=utf-8
< 
Hello OpenShift!
* Connection #0 to host 172.30.214.57 left intact
```

4.7 Workstationからはまだ失敗することを確認せよ

```
[student@workstation secure-route]$ curl -v http://172.30.214.57:8080/
* About to connect() to 172.30.214.57 port 8080 (#0)
*   Trying 172.30.214.57...
^C
[student@workstation secure-route]$ 
[student@workstation secure-route]$ curl -v http://hello.apps.lab.example.com:8080/
* About to connect() to hello.apps.lab.example.com port 8080 (#0)
*   Trying 172.25.250.11...
* No route to host
* Failed connect to hello.apps.lab.example.com:8080; No route to host
* Closing connection 0
curl: (7) Failed connect to hello.apps.lab.example.com:8080; No route to host

ふむ
```

4.8 oc describe routeコマンドで状態を確認せよ

```
[student@workstation secure-route]$ oc describe route
Name:			hello-openshift
Namespace:		network-review
Created:		40 minutes ago
Labels:			app=hello-openshift
Annotations:		<none>
Requested Host:		hello.apps.lab.example.com
			  exposed on router router 40 minutes ago
Path:			<none>
TLS Termination:	<none>
Insecure Policy:	<none>
Endpoint Port:		8080-tcp

Service:	hello-opensift
Weight:		100 (100%)
Endpoints:	<error: endpoints "hello-opensift" not found>

Endpointsがエラーになってる。これもTypoか

```

4.9 修正してoc describe route hello-openshift で状態確認せよ。

```
[student@workstation secure-route]$ oc edit route
route "hello-openshift" edited


spec:
  host: hello.apps.lab.example.com
  port:
    targetPort: 8080-tcp
  to:
    kind: Service
    name: hello-opensift
    weight: 100
  wildcardPolicy: None


[student@workstation secure-route]$ oc describe route
Name:			hello-openshift
Namespace:		network-review
Created:		42 minutes ago
Labels:			app=hello-openshift
Annotations:		<none>
Requested Host:		hello.apps.lab.example.com
			  exposed on router router 42 minutes ago
Path:			<none>
TLS Termination:	<none>
Insecure Policy:	<none>
Endpoint Port:		8080-tcp

Service:	hello-openshift
Weight:		100 (100%)
Endpoints:	10.129.0.33:8888, 10.129.0.33:8080

かわった！

```
4.10 URLにアクセスせよ
```

[student@workstation secure-route]$ curl -v http://hello.apps.lab.example.com:8080/
* About to connect() to hello.apps.lab.example.com port 8080 (#0)
*   Trying 172.25.250.11...
* No route to host
* Failed connect to hello.apps.lab.example.com:8080; No route to host
* Closing connection 0
curl: (7) Failed connect to hello.apps.lab.example.com:8080; No route to host


ぐぬぬ。。。

[student@workstation secure-route]$ curl http://hello.apps.lab.example.com
Hello OpenShift!

表向きは80なのか！！コレどうやって判定するんだ？あー、route使う時点で標準か

```

5. lab grade コマンドを使用し、結果を判定せよ。

```
[student@workstation secure-route]$ lab network-review grade

Grading the student's work for Lab: Exploring OpenShift Networking

 · Check if the hello-openshift pod is in Running state........  PASS
 . Checking if service configuration was fixed correctly.......  PASS
 . Checking if route configuration was fixed correctly.........  PASS
 . Checking if route can be invoked successfully...............  PASS

Overall exercise grade.........................................  PASS


ひゃっほう。
```
おそうじ

```
student@workstation secure-route]$ oc get project
NAME             DISPLAY NAME   STATUS
network-review                  Active
[student@workstation secure-route]$ oc delete project network-review
project "network-review" deleted
[student@workstation secure-route]$ oc status
In project network-review on server https://master.lab.example.com:443

http://hello.apps.lab.example.com to pod port 8080-tcp (svc/hello-openshift)
  dc/hello-openshift deploys istag/hello-openshift:latest 
    deployment #1 waiting on image or update
  pod/hello-openshift-1-wrxsb runs registry.lab.example.com/openshift/hello-openshift:latest

Errors:
  * The image trigger for dc/hello-openshift will have no effect because is/hello-openshift does not exist.

1 error, 3 infos identified, use 'oc status -v' to see details.
[student@workstation secure-route]$ oc status
Error from server (Forbidden): projects.project.openshift.io "network-review" is forbidden: User "developer" cannot get projects.project.openshift.io in the namespace "network-review": User "developer" cannot get project "network-review"
[student@workstation secure-route]$ 

```


------------------------------

# Chapter4 EXECUTING COMMANDS

windows版もある。
oc help ってやるとサブコマンドが紹介される。

oc new-app --help とかも詳細がでる。サンプルも出る。
認定試験のときにも出るので、コマンドの使い方を調べたい時に便利。

コマンドを使って出来ること
- ログイン
```
oc login <url> -u <username> -p <password>
```
入力しない場合は対話式になる。

トークン情報などの残っている場所もある。（昨日確認したところ）

- 現在のログイン情報調査
```
oc whoami
```

- 現在のプロジェクト
```
oc status
```
new-projectを作ると、そのプロジェクトがcurrentになる。
adminユーザで作業しないように注意。
消すときは、ちゃんとラベル張ってるとoc delete all -l <labelname> で全部消せるんだけどそうでない場合個別で全部今朝なきゃいけない。

- projectの削除
```
oc delete <project-name>

全消しするにはprojectを消すのが一番楽チン。
```

- ログアウト
```
oc logout
```
基本的に作業が終わったらログアウトしましょう。
masterノードでは特別にログイン状態になってます。system:adminで常時ログインされている。

- リソース情報の取得
```
oc get <resource-name>
```
省略形が許されている。
oc get allとかもある。主要なリソースの一覧が出てくる。
このへんは、スペースと／はどちらでもいい。
oc describe pods/hogehoge とかもできる。

```
[student@workstation secure-route]$ oc get all
NAME                                REVISION   DESIRED   CURRENT   TRIGGERED BY
deploymentconfigs/hello-openshift   0          1         0         config,image(hello-openshift:latest)

NAME                     HOST/PORT                    PATH      SERVICES          PORT       TERMINATION   WILDCARD
routes/hello-openshift   hello.apps.lab.example.com             hello-openshift   8080-tcp                 None

NAME                         READY     STATUS    RESTARTS   AGE
po/hello-openshift-1-wrxsb   1/1       Running   0          53m

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
svc/hello-openshift   ClusterIP   172.30.214.57   <none>        8080/TCP,8888/TCP   53m
```

-wをつけると継続監視になるが、watchコマンドでやったほうがいいかも。古い情報がでちゃうので。

```
[student@workstation secure-route]$ oc get svc -o wide
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE       SELECTOR
hello-openshift   ClusterIP   172.30.214.57   <none>        8080/TCP,8888/TCP   54m       app=hello-openshift,deploymentconfig=hello-openshift
[student@workstation secure-route]$ oc get svc -w
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
hello-openshift   ClusterIP   172.30.214.57   <none>        8080/TCP,8888/TCP   54m
```

```
[student@workstation secure-route]$ oc describe routes/hello-openshift
Name:			hello-openshift
Namespace:		network-review
Created:		About an hour ago
Labels:			app=hello-openshift
Annotations:		<none>
Requested Host:		hello.apps.lab.example.com
			  exposed on router router about an hour ago
Path:			<none>
TLS Termination:	<none>
Insecure Policy:	<none>
Endpoint Port:		8080-tcp

Service:	hello-openshift
Weight:		100 (100%)
Endpoints:	10.129.0.33:8888, 10.129.0.33:8080


```

- 設定データの吐き出し
```
oc export <resource> <name>

普通は画面に表示される。
ファイルに吐き出す場合にはりダイレクトする事。

oc export svc demo > hoge.yml

編集。その後持ち込むと

oc create -f <export-file-name>

複数吐き出しもできる。

oc export svc,pod とか。

--as-templateとかでテンプレート化もできる。あとから出てくる。
これを登録すると、GUIから必要パラメータだけいれればデプロいされたりするが、
リソースについては吐き出す順番があるので注意。
このあたりの詳細は、DO288の開発講座で取り扱っている。テンプレート作ったりが入ってる。

```
- 登録された物を消す
```
oc delete <resource-type> <resource-name>

oc new-appで出てきたものを控えておくと、あとから消すのに楽。試験のときとかにも癖つけたほうがいいかも。
```

- 起動中のコンテナに新しいプロセスをたてるコマンド

```
oc exec <pod-name>
oc rsh <pod-name>

[student@workstation secure-route]$ oc exec hello-openshift-1-wrxsb -it /bin/bash
rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "exec: \"/bin/bash\": stat /bin/bash: no such file or directory"

command terminated with exit code 126
[student@workstation secure-route]$ oc exec hello-openshift-1-wrxsb -it /bin/bash

dockerコマンドと同じ。
ログインシェルがないかも。

```

認定試験では、CLIでもGUIでもどちらでもいい。

- リソースタイプの確認
```
oc types

[student@workstation secure-route]$ oc types
Concepts and Types 

Kubernetes and OpenShift help developers and operators build, test, and deploy applications in a
containerized cloud environment. Applications may be composed of all of the components below,
although most developers will be concerned with Services, Deployments, and Builds for delivering
changes. 

Concepts: 

* Containers:
    A definition of how to run one or more processes inside of a portable Linux
    environment. Containers are started from an Image and are usually isolated
    from other containers on the same machine.
    
* Image:
    A layered Linux filesystem that contains application code, dependencies,
    and any supporting operating system libraries. An image is identified by
    a name that can be local to the current cluster or point to a remote Docker
    registry (a storage server for images).
    
* Pods [pod]:
    A set of one or more containers that are deployed onto a Node together and
    share a unique IP and Volumes (persistent storage). Pods also define the
    security and runtime policy for each container.
    
* Labels:
    Labels are key value pairs that can be assigned to any resource in the
    system for grouping and selection. Many resources use labels to identify
    sets of other resources.
    
* Volumes:
    Containers are not persistent by default - on restart their contents are
    cleared. Volumes are mounted filesystems available to Pods and their
    containers which may be backed by a number of host-local or network
    attached storage endpoints. The simplest volume type is EmptyDir, which
    is a temporary directory on a single machine. Administrators may also
    allow you to request a Persistent Volume that is automatically attached
    to your pods.
    
* Nodes [node]:
    Machines set up in the cluster to run containers. Usually managed
    by administrators and not by end users.
    
* Services [svc]:
    A name representing a set of pods (or external servers) that are
    accessed by other pods. The service gets an IP and a DNS name, and can be
    exposed externally to the cluster via a port or a Route. It's also easy
    to consume services from pods because an environment variable with the
    name <SERVICE>_HOST is automatically injected into other pods.
    
* Routes [route]:
    A route is an external DNS entry (either a top level domain or a
    dynamically allocated name) that is created to point to a service so that
    it can be accessed outside the cluster. The administrator may configure
    one or more Routers to handle those routes, typically through an Apache
    or HAProxy load balancer / proxy.
    
* Replication Controllers [rc]:
    A replication controller maintains a specific number of pods based on a
    template that match a set of labels. If pods are deleted (because the
    node they run on is taken out of service) the controller creates a new
    copy of that pod. A replication controller is most commonly used to
    represent a single deployment of part of an application based on a
    built image.
    
* Deployment Configuration [dc]:
    Defines the template for a pod and manages deploying new images or
    configuration changes whenever those change. A single deployment
    configuration is usually analogous to a single micro-service. Can support
    many different deployment patterns, including full restart, customizable
    rolling updates, and fully custom behaviors, as well as pre- and post-
    hooks. Each deployment is represented as a replication controller.
    
* Build Configuration [bc]:
    Contains a description of how to build source code and a base image into a
    new image - the primary method for delivering changes to your application.
    Builds can be source based and use builder images for common languages like
    Java, PHP, Ruby, or Python, or be Docker based and create builds from a
    Dockerfile. Each build configuration has web-hooks and can be triggered
    automatically by changes to their base images.
    
* Builds [build]:
    Builds create a new image from source code, other images, Dockerfiles, or
    binary input. A build is run inside of a container and has the same
    restrictions normal pods have. A build usually results in an image pushed
    to a Docker registry, but you can also choose to run a post-build test that
    does not push an image.
    
* Image Streams and Image Stream Tags [is,istag]:
    An image stream groups sets of related images under tags - analogous to a
    branch in a source code repository. Each image stream may have one or
    more tags (the default tag is called "latest") and those tags may point
    at external Docker registries, at other tags in the same stream, or be
    controlled to directly point at known images. In addition, images can be
    pushed to an image stream tag directly via the integrated Docker
    registry.
    
* Secrets [secret]:
    The secret resource can hold text or binary secrets for delivery into
    your pods. By default, every container is given a single secret which
    contains a token for accessing the API (with limited privileges) at
    /var/run/secrets/kubernetes.io/serviceaccount. You can create new
    secrets and mount them in your own pods, as well as reference secrets
    from builds (for connecting to remote servers) or use them to import
    remote images into an image stream.
    
* Projects [project]:
    All of the above resources (except Nodes) exist inside of a project.
    Projects have a list of members and their roles, like viewer, editor,
    or admin, as well as a set of security controls on the running pods, and
    limits on how many resources the project can use. The names of each
    resource are unique within a project. Developers may request projects
    be created, but administrators control the resources allocated to
    projects.
    
For more, see https://docs.openshift.com

Usage:
  oc types [options]

Examples:
  # View all projects you have access to
  oc get projects
  
  # See a list of all services in the current project
  oc get svc
  
  # Describe a deployment configuration in detail
  oc describe dc mydeploymentconfig
  
  # Show the images tagged into an image stream
  oc describe is ruby-centos7

Use "oc options" for a list of global command-line options (applies to all commands).
[student@workstation secure-route]$ 

```

### oc new-appについてもう少し詳しく。
OCPの専用コマンド。
YAMLとか書かずに色々準備してくれる。板書した説明のものが必ず造られるわけではなく、コマンドセット次第で変る。

oc new-app -o json / -o yaml とかやると、構築するときに作成されるリソースが指定された形式で出力される。

実行しないので、ソース判定などができなくなる場合もある。
--storategy=source とかのオプションをつけると強制的にS2Iやれ！という意味になる。

oc newapp <name> では、nameでmysqlとか指定すると、勝手に探してくる。ビルドはしない？
どこのレジストリから探してくるかはdockerの設定ファイルで確認できる。
master上の/etc/sysconfig/docker にADD_REGISTORYとかで入っている。

oc new-app <name> <env>
oc new-app --docker-image=<path> --name=<name> 
 latestとかつけないとダメだったりするので注意。:4.xとかの指定をしたり。
oc new-app <git-url> --name=<name>
 イメージの自動判定が実施される。
 イメージも最新版が使われる。その場合には -i <image-version> を指定する。
ex) 
oc new-app <git-url> -i php:7.0 --name=<name>
oc new-app        -l db=mysql  とかでやると、db=mysql というラベルが全てのリソースについてくれる。
oc new-app  --storategy=<storategy>
  S2Iかdockerのどちらか。--storategy=docker とやると、docker --storategy=source とやるとS2Iを強制。
細かいオプションはヘルプを参照。
oc new-app --help

認定試験でもオプションをうまく使わないとダメなケースがある。
よくみておくこと。テキストに書いてあるオプションくらいはしっかり把握しておくこと。
--docker-images とか。

```
[root@master ~]# cat /etc/sysconfig/docker | tail 

# docker-latest daemon can be used by starting the docker-latest unitfile.
# To use docker-latest client, uncomment below lines
#DOCKERBINARY=/usr/bin/docker-latest
#DOCKERDBINARY=/usr/bin/dockerd-latest
#DOCKER_CONTAINERD_BINARY=/usr/bin/docker-containerd-latest
#DOCKER_CONTAINERD_SHIM_BINARY=/usr/bin/docker-containerd-shim-latest
INSECURE_REGISTRY='--insecure-registry registry.lab.example.com'
ADD_REGISTRY='--add-registry registry.lab.example.com --add-registry registry.access.redhat.com'
BLOCK_REGISTRY='--block-registry registry.access.redhat.com --block-registry docker.io'



--insecure-registory は証明書エラーを無視する奴。あんまり入れないほうがいい。
```


## Guide Exercise: Managing an OpenShift Instance Using oc(P112)

- pre

```
[student@workstation secure-route]$ lab manage-oc setup

Checking prerequisites for GE: Managing an OpenShift Instance Using oc

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Overall setup status...........................................  SUCCESS

```

1.1 adminでログインせよ

```
[student@workstation secure-route]$ oc whoami
developer
[student@workstation secure-route]$ oc login -u admin -p redhat https://master.lab.example.com
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation secure-route]$ oc whoami
admin

```

1.2 default project に移動せよ

```
[student@workstation secure-route]$ oc project default
Already on project "default" on server "https://master.lab.example.com:443".


```

1.3 Clusterの状態を確認せよ

```
[student@workstation secure-route]$ oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    21h       v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   21h       v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   21h       v1.9.1+a0ce1bc657
```

1.4 node の詳細情報をdescribeオプションを付けて表示せよ
Events にはライフサイクル上重要な情報が出る。


```
[student@workstation secure-route]$ oc describe nodes master.lab.example.com
Name:               master.lab.example.com
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=master.lab.example.com
                    node-role.kubernetes.io/master=true
                    openshift-infra=apiserver
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Tue, 24 Sep 2019 14:13:08 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Wed, 25 Sep 2019 11:42:39 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 25 Sep 2019 11:42:39 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 25 Sep 2019 11:42:39 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Wed, 25 Sep 2019 11:42:39 +0900   Tue, 24 Sep 2019 14:13:50 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.250.10
  Hostname:    master.lab.example.com
Capacity:
 cpu:     2
 memory:  1882608Ki
 pods:    20
Allocatable:
 cpu:     2
 memory:  1780208Ki
 pods:    20
System Info:
 Machine ID:                 a1fb71323fcb41eda3f879506ed98dba
 System UUID:                A4A7FBC4-2706-41B1-B33F-656D9E2DB6F5
 Boot ID:                    a4d39d2b-05dd-40ed-aaa8-ad221ac27d52
 Kernel Version:             3.10.0-862.el7.x86_64
 OS Image:                   Red Hat Enterprise Linux Server 7.5 (Maipo)
 Operating System:           linux
 Architecture:               amd64
 Container Runtime Version:  docker://1.13.1
 Kubelet Version:            v1.9.1+a0ce1bc657
 Kube-Proxy Version:         v1.9.1+a0ce1bc657
ExternalID:                  master.lab.example.com
Non-terminated Pods:         (3 in total)
  Namespace                  Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                  ----                           ------------  ----------  ---------------  -------------
  kube-service-catalog       apiserver-5gn6t                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  kube-service-catalog       controller-manager-b4zff       0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-web-console      webconsole-579cc76bb6-584f7    100m (5%)     0 (0%)      100Mi (5%)       0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  100m (5%)     0 (0%)      100Mi (5%)       0 (0%)
Events:         <none>

```

1.5 node1も確認せよ

```
[student@workstation secure-route]$ oc describe nodes node1.lab.example.com
Name:               node1.lab.example.com
Roles:              compute
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=node1.lab.example.com
                    node-role.kubernetes.io/compute=true
                    region=infra
Annotations:        volumes.kubernetes.io/controller-managed-attach-detach=true
Taints:             <none>
CreationTimestamp:  Tue, 24 Sep 2019 14:13:08 +0900
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  OutOfDisk        False   Wed, 25 Sep 2019 11:44:33 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientDisk     kubelet has sufficient disk space available
  MemoryPressure   False   Wed, 25 Sep 2019 11:44:33 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 25 Sep 2019 11:44:33 +0900   Tue, 24 Sep 2019 14:13:08 +0900   KubeletHasNoDiskPressure     kubelet has no disk pressure
  Ready            True    Wed, 25 Sep 2019 11:44:33 +0900   Tue, 24 Sep 2019 14:13:50 +0900   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.250.11
  Hostname:    node1.lab.example.com
Capacity:
 cpu:     2
 memory:  8009700Ki
 pods:    20
Allocatable:
 cpu:     2
 memory:  7907300Ki
 pods:    20
System Info:
 Machine ID:                         a1fb71323fcb41eda3f879506ed98dba
 System UUID:                        49513816-4254-449C-9991-7CFC78540365
 Boot ID:                            2eb4056a-5214-4661-a7d9-f5f2077b1061
 Kernel Version:                     3.10.0-862.el7.x86_64
 OS Image:                           Red Hat Enterprise Linux Server 7.5 (Maipo)
 Operating System:                   linux
 Architecture:                       amd64
 Container Runtime Version:          docker://1.13.1
 Kubelet Version:                    v1.9.1+a0ce1bc657
 Kube-Proxy Version:                 v1.9.1+a0ce1bc657
ExternalID:                          node1.lab.example.com
Non-terminated Pods:                 (4 in total)
  Namespace                          Name                       CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------                          ----                       ------------  ----------  ---------------  -------------
  default                            docker-registry-1-68nzl    100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  default                            router-1-vgx5r             100m (5%)     0 (0%)      256Mi (3%)       0 (0%)
  openshift-ansible-service-broker   asb-1-vwnfm                0 (0%)        0 (0%)      0 (0%)           0 (0%)
  openshift-template-service-broker  apiserver-t74f2            0 (0%)        0 (0%)      0 (0%)           0 (0%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ------------  ----------  ---------------  -------------
  200m (10%)    0 (0%)      512Mi (6%)       0 (0%)
Events:         <none>

```


1.6 oc get pods コマンドで存在するpodの情報を確認せよ

```
[student@workstation secure-route]$ oc get pods
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-1-68nzl    1/1       Running   1          21h
docker-registry-1-746wt    1/1       Running   2          21h
registry-console-1-b4mdc   1/1       Running   3          21h
router-1-hgrb5             1/1       Running   2          21h
router-1-vgx5r             1/1       Running   1          21h

```

1.7 oc describe コマンドでPODの詳細を確認せよ

```
[student@workstation secure-route]$ oc describe pod docker-registry-1-68nzl
Name:           docker-registry-1-68nzl
Namespace:      default
Node:           node1.lab.example.com/172.25.250.11
Start Time:     Tue, 24 Sep 2019 14:14:41 +0900
Labels:         deployment=docker-registry-1
                deploymentconfig=docker-registry
                docker-registry=default
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=docker-registry
                openshift.io/deployment.name=docker-registry-1
                openshift.io/scc=restricted
Status:         Running
IP:             10.128.0.8
Controlled By:  ReplicationController/docker-registry-1
Containers:
  registry:
    Container ID:   docker://be6bdd465db0b6650343ac5035e17f030feeb9db646734102b989dd0d334dd2f
    Image:          registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14
    Image ID:       docker-pullable://registry.lab.example.com/openshift3/ose-docker-registry@sha256:d761309923f7ecddc1eabedbae0cc9eafe00b125d191851e6c8c50389857f7ce
    Port:           5000/TCP
    State:          Running
      Started:      Tue, 24 Sep 2019 14:52:03 +0900
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Tue, 24 Sep 2019 14:14:47 +0900
      Finished:     Tue, 24 Sep 2019 14:51:58 +0900
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      100m
      memory:   256Mi
    Liveness:   http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Environment:
      REGISTRY_HTTP_ADDR:                                     :5000
      REGISTRY_HTTP_NET:                                      tcp
      REGISTRY_HTTP_SECRET:                                   p/KCsDgz3tFOeH9UZB8Ojtp+OsYMbY76OgEmT/SKIyw=
      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:  false
      REGISTRY_OPENSHIFT_SERVER_ADDR:                         docker-registry.default.svc:5000
      REGISTRY_HTTP_TLS_KEY:                                  /etc/secrets/registry.key
      REGISTRY_HTTP_TLS_CERTIFICATE:                          /etc/secrets/registry.crt
    Mounts:
      /etc/secrets from registry-certificates (rw)
      /registry from registry-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from registry-token-l75ml (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  registry-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  registry-claim
    ReadOnly:   false
  registry-certificates:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  registry-certificates
    Optional:    false
  registry-token-l75ml:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  registry-token-l75ml
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  region=infra
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>

```

2.1 oc exec コマンドでPODのhostname/lsを出力せよ

```
[student@workstation secure-route]$ oc exec docker-registry-1-68nzl hostname
docker-registry-1-68nzl
[student@workstation secure-route]$ 

[student@workstation secure-route]$ oc exec docker-registry-1-68nzl ls /
bin
boot
config.yml
dev
etc
home
lib
lib64
lost+found
media
mnt
opt
proc
registry
root
run
sbin
srv
sys
tmp
usr
var


```

2.2 podsの中のresolv.confを確認せよ


```
[student@workstation secure-route]$ oc exec docker-registry-1-68nzl cat /etc/resolv.conf
nameserver 172.25.250.11
search default.svc.cluster.local svc.cluster.local cluster.local lab.example.com example.com
options ndots:5
[student@workst
```

2.3 oc rsh コマンドを利用し、リモート接続せよ。

router pod の細かいトラブルシュートはMaster ノードからじゃないと入れない。
やってみよ。

```
[student@workstation secure-route]$ oc rsh docker-registry-1-68nzl
sh-4.2$ ls
bin   config.yml  etc	lib    lost+found  mnt	proc	  root	sbin  sys  usr
boot  dev	  home	lib64  media	   opt	registry  run	srv   tmp  var
sh-4.2$ exit
exit

```
普通のworkstationからrouterへ

```
[student@workstation secure-route]$ oc rsh router-1-hgrb5
sh-4.2$ ls
cert_config.map		 haproxy.config       os_route_http_expose.map	  os_wildcard_domain.map
default_pub_keys.pem	 os_edge_http_be.map  os_route_http_redirect.map
error-page-503.http	 os_http_be.map       os_sni_passthrough.map
haproxy-config.template  os_reencrypt.map     os_tcp_be.map

```
あれ？入れた。英語読み直すか。。。



3.1 oc status -v コマンドをつかってみよう

```
[student@workstation secure-route]$ oc status -v
In project default on server https://master.lab.example.com:443

https://docker-registry-default.apps.lab.example.com (passthrough) (svc/docker-registry)
  dc/docker-registry deploys registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14 
    deployment #1 deployed 22 hours ago - 2 pods

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.apps.lab.example.com (passthrough) (svc/registry-console)
  dc/registry-console deploys registry.lab.example.com/openshift3/registry-console:v3.9 
    deployment #1 deployed 22 hours ago - 1 pod

svc/router - 172.30.34.216 ports 80, 443, 1936
  dc/router deploys registry.lab.example.com/openshift3/ose-haproxy-router:v3.9.14 
    deployment #1 deployed 22 hours ago - 2 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.

```

サンプルでvなしのもの。ちがいはどこだ。。？ないきがするぞ。。？

```
[student@workstation secure-route]$ oc status
In project default on server https://master.lab.example.com:443

https://docker-registry-default.apps.lab.example.com (passthrough) (svc/docker-registry)
  dc/docker-registry deploys registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14 
    deployment #1 deployed 22 hours ago - 2 pods

svc/kubernetes - 172.30.0.1 ports 443, 53->8053, 53->8053

https://registry-console-default.apps.lab.example.com (passthrough) (svc/registry-console)
  dc/registry-console deploys registry.lab.example.com/openshift3/registry-console:v3.9 
    deployment #1 deployed 22 hours ago - 1 pod

svc/router - 172.30.34.216 ports 80, 443, 1936
  dc/router deploys registry.lab.example.com/openshift3/ose-haproxy-router:v3.9.14 
    deployment #1 deployed 22 hours ago - 2 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.

```

3.2 oc get events コマンドを使ってみよう

```
[student@workstation secure-route]$ oc get events
LAST SEEN   FIRST SEEN   COUNT     NAME                                       KIND                   SUBOBJECT   TYPE      REASON                SOURCE                               MESSAGE
10m         21h          75        ansible-service-broker.15c74a0a0cc79b83    ClusterServiceBroker               Normal    FetchedCatalog        service-catalog-controller-manager   Successfully fetched catalog entries from broker.
1h          2h           29        ansible-service-broker.15c78a408818e40c    ClusterServiceBroker               Warning   ErrorSyncingCatalog   service-catalog-controller-manager   Error getting catalog payload for broker "ansible-service-broker"; received zero services; at least one service is required
9m          21h          71        template-service-broker.15c74a092a563f4c   ClusterServiceBroker               Normal    FetchedCatalog        service-catalog-controller-manager   Successfully fetched catalog entries from broker.

```

4. import/export

4.1 oc get all をつかって、全てのリソースを取得してみよう。

```
[student@workstation secure-route]$ oc get all
NAME                                 REVISION   DESIRED   CURRENT   TRIGGERED BY
deploymentconfigs/docker-registry    1          2         2         config
deploymentconfigs/registry-console   1          1         1         config
deploymentconfigs/router             1          2         2         config

NAME                            DOCKER REPO                                                 TAGS      UPDATED
imagestreams/registry-console   docker-registry.default.svc:5000/default/registry-console   v3.9      22 hours ago

NAME                      HOST/PORT                                       PATH      SERVICES           PORT      TERMINATION   WILDCARD
routes/docker-registry    docker-registry-default.apps.lab.example.com              docker-registry    <all>     passthrough   None
routes/registry-console   registry-console-default.apps.lab.example.com             registry-console   <all>     passthrough   None

NAME                          READY     STATUS    RESTARTS   AGE
po/docker-registry-1-68nzl    1/1       Running   1          22h
po/docker-registry-1-746wt    1/1       Running   2          22h
po/registry-console-1-b4mdc   1/1       Running   3          22h
po/router-1-hgrb5             1/1       Running   2          22h
po/router-1-vgx5r             1/1       Running   1          22h

NAME                    DESIRED   CURRENT   READY     AGE
rc/docker-registry-1    2         2         2         22h
rc/registry-console-1   1         1         1         22h
rc/router-1             2         2         2         22h

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
svc/docker-registry    ClusterIP   172.30.144.99    <none>        5000/TCP                  22h
svc/kubernetes         ClusterIP   172.30.0.1       <none>        443/TCP,53/UDP,53/TCP     22h
svc/registry-console   ClusterIP   172.30.217.201   <none>        9000/TCP                  22h
svc/router             ClusterIP   172.30.34.216    <none>        80/TCP,443/TCP,1936/TCP   22h

```

4.2 oc export コマンドを使って、PODの情報をYAML／JSONフォーマットで出力してみよう。（デフォルトはYAML）

```
[student@workstation secure-route]$ oc export pods docker-registry-1-68nzl
apiVersion: v1
kind: Pod
metadata:
  annotations:
    openshift.io/deployment-config.latest-version: "1"
    openshift.io/deployment-config.name: docker-registry
    openshift.io/deployment.name: docker-registry-1
    openshift.io/scc: restricted
  creationTimestamp: null
  generateName: docker-registry-1-
  labels:
    deployment: docker-registry-1
    deploymentconfig: docker-registry
    docker-registry: default
  ownerReferences:
  - apiVersion: v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicationController
    name: docker-registry-1
    uid: 2a115022-de8a-11e9-98ef-52540000fa0a
spec:
  containers:
  - env:
    - name: REGISTRY_HTTP_ADDR
      value: :5000
    - name: REGISTRY_HTTP_NET
      value: tcp
    - name: REGISTRY_HTTP_SECRET
      value: p/KCsDgz3tFOeH9UZB8Ojtp+OsYMbY76OgEmT/SKIyw=
    - name: REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA
      value: "false"
    - name: REGISTRY_OPENSHIFT_SERVER_ADDR
      value: docker-registry.default.svc:5000
    - name: REGISTRY_HTTP_TLS_KEY
      value: /etc/secrets/registry.key
    - name: REGISTRY_HTTP_TLS_CERTIFICATE
      value: /etc/secrets/registry.crt
    image: registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 5000
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    name: registry
    ports:
    - containerPort: 5000
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 5000
        scheme: HTTPS
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
    securityContext:
      capabilities:
        drop:
        - KILL
        - MKNOD
        - SETGID
        - SETUID
      privileged: false
      runAsUser: 1000000000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /registry
      name: registry-storage
    - mountPath: /etc/secrets
      name: registry-certificates
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: registry-token-l75ml
      readOnly: true
  dnsPolicy: ClusterFirst
  imagePullSecrets:
  - name: registry-dockercfg-vll9r
  nodeName: node1.lab.example.com
  nodeSelector:
    region: infra
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1000000000
    seLinuxOptions:
      level: s0:c1,c0
  serviceAccount: registry
  serviceAccountName: registry
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  volumes:
  - name: registry-storage
    persistentVolumeClaim:
      claimName: registry-claim
  - name: registry-certificates
    secret:
      defaultMode: 420
      secretName: registry-certificates
  - name: registry-token-l75ml
    secret:
      defaultMode: 420
      secretName: registry-token-l75ml
status:
  phase: Pending
  qosClass: Burstable
[student@workstation secure-route]$ 

```

JSON版は？
-o json とかってやるらしい。
```
[student@workstation secure-route]$ oc export --help
Export resources so they can be used elsewhere 

The export command makes it easy to take existing objects and convert them to configuration files
for backups or for creating elsewhere in the cluster. Fields that cannot be specified on create will
be set to empty, and any field which is assigned on creation (like a service's clusterIP, or a
deployment config's latestVersion). The status part of objects is also cleared. 

Some fields like clusterIP may be useful when exporting an application from one cluster to apply to
another - assuming another service on the destination cluster does not already use that IP. The
--exact flag will instruct export to not clear fields that might be useful. You may also use --raw
to get the exact values for an object - useful for converting a file on disk between API versions. 

Another use case for export is to create reusable templates for applications. Pass --as-template to
generate the API structure for a template to which you can add parameters and object labels.

Usage:
  oc export RESOURCE/NAME ... [options]

Examples:
  # export the services and deployment configurations labeled name=test
  oc export svc,dc -l name=test
  
  # export all services to a template
  oc export service --as-template=test
  
  # export to JSON
  oc export service -o json


```
やってみる。

```
[student@workstation secure-route]$ oc export pods docker-registry-1-68nzl -o json
{
    "kind": "Pod",
    "apiVersion": "v1",
    "metadata": {
        "generateName": "docker-registry-1-",
        "creationTimestamp": null,
        "labels": {
            "deployment": "docker-registry-1",
            "deploymentconfig": "docker-registry",
            "docker-registry": "default"
        },
        "annotations": {
            "openshift.io/deployment-config.latest-version": "1",
            "openshift.io/deployment-config.name": "docker-registry",
            "openshift.io/deployment.name": "docker-registry-1",
            "openshift.io/scc": "restricted"
        },
        "ownerReferences": [
            {
                "apiVersion": "v1",
                "kind": "ReplicationController",
                "name": "docker-registry-1",
                "uid": "2a115022-de8a-11e9-98ef-52540000fa0a",
                "controller": true,
                "blockOwnerDeletion": true
            }
        ]
    },
    "spec": {
        "volumes": [
            {
                "name": "registry-storage",
                "persistentVolumeClaim": {
                    "claimName": "registry-claim"
                }
            },
            {
                "name": "registry-certificates",
                "secret": {
                    "secretName": "registry-certificates",
                    "defaultMode": 420
                }
            },
            {
                "name": "registry-token-l75ml",
                "secret": {
                    "secretName": "registry-token-l75ml",
                    "defaultMode": 420
                }
            }
        ],
        "containers": [
            {
                "name": "registry",
                "image": "registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14",
                "ports": [
                    {
                        "containerPort": 5000,
                        "protocol": "TCP"
                    }
                ],
                "env": [
                    {
                        "name": "REGISTRY_HTTP_ADDR",
                        "value": ":5000"
                    },
                    {
                        "name": "REGISTRY_HTTP_NET",
                        "value": "tcp"
                    },
                    {
                        "name": "REGISTRY_HTTP_SECRET",
                        "value": "p/KCsDgz3tFOeH9UZB8Ojtp+OsYMbY76OgEmT/SKIyw="
                    },
                    {
                        "name": "REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA",
                        "value": "false"
                    },
                    {
                        "name": "REGISTRY_OPENSHIFT_SERVER_ADDR",
                        "value": "docker-registry.default.svc:5000"
                    },
                    {
                        "name": "REGISTRY_HTTP_TLS_KEY",
                        "value": "/etc/secrets/registry.key"
                    },
                    {
                        "name": "REGISTRY_HTTP_TLS_CERTIFICATE",
                        "value": "/etc/secrets/registry.crt"
                    }
                ],
                "resources": {
                    "requests": {
                        "cpu": "100m",
                        "memory": "256Mi"
                    }
                },
                "volumeMounts": [
                    {
                        "name": "registry-storage",
                        "mountPath": "/registry"
                    },
                    {
                        "name": "registry-certificates",
                        "mountPath": "/etc/secrets"
                    },
                    {
                        "name": "registry-token-l75ml",
                        "readOnly": true,
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                    }
                ],
                "livenessProbe": {
                    "httpGet": {
                        "path": "/healthz",
                        "port": 5000,
                        "scheme": "HTTPS"
                    },
                    "initialDelaySeconds": 10,
                    "timeoutSeconds": 5,
                    "periodSeconds": 10,
                    "successThreshold": 1,
                    "failureThreshold": 3
                },
                "readinessProbe": {
                    "httpGet": {
                        "path": "/healthz",
                        "port": 5000,
                        "scheme": "HTTPS"
                    },
                    "timeoutSeconds": 5,
                    "periodSeconds": 10,
                    "successThreshold": 1,
                    "failureThreshold": 3
                },
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "imagePullPolicy": "IfNotPresent",
                "securityContext": {
                    "capabilities": {
                        "drop": [
                            "KILL",
                            "MKNOD",
                            "SETGID",
                            "SETUID"
                        ]
                    },
                    "privileged": false,
                    "runAsUser": 1000000000
                }
            }
        ],
        "restartPolicy": "Always",
        "terminationGracePeriodSeconds": 30,
        "dnsPolicy": "ClusterFirst",
        "nodeSelector": {
            "region": "infra"
        },
        "serviceAccountName": "registry",
        "serviceAccount": "registry",
        "nodeName": "node1.lab.example.com",
        "securityContext": {
            "seLinuxOptions": {
                "level": "s0:c1,c0"
            },
            "fsGroup": 1000000000
        },
        "imagePullSecrets": [
            {
                "name": "registry-dockercfg-vll9r"
            }
        ],
        "schedulerName": "default-scheduler",
        "tolerations": [
            {
                "key": "node.kubernetes.io/memory-pressure",
                "operator": "Exists",
                "effect": "NoSchedule"
            }
        ]
    },
    "status": {
        "phase": "Pending",
        "qosClass": "Burstable"
    }
}


```

いけた。


4.3 oc export のオプションである--as-template を使ってみよう

```
[student@workstation secure-route]$ oc export svc,dc docker-registry --as-template=docker-registry
apiVersion: v1
kind: Template
metadata:
  creationTimestamp: null
  name: docker-registry
objects:
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: null
    labels:
      docker-registry: default
    name: docker-registry
  spec:
    ports:
    - name: 5000-tcp
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      docker-registry: default
    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    creationTimestamp: null
    generation: 1
    labels:
      docker-registry: default
    name: docker-registry
  spec:
    replicas: 2
    selector:
      docker-registry: default
    strategy:
      activeDeadlineSeconds: 21600
      resources: {}
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        creationTimestamp: null
        labels:
          docker-registry: default
      spec:
        containers:
        - env:
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_HTTP_NET
            value: tcp
          - name: REGISTRY_HTTP_SECRET
            value: p/KCsDgz3tFOeH9UZB8Ojtp+OsYMbY76OgEmT/SKIyw=
          - name: REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA
            value: "false"
          - name: REGISTRY_OPENSHIFT_SERVER_ADDR
            value: docker-registry.default.svc:5000
          - name: REGISTRY_HTTP_TLS_KEY
            value: /etc/secrets/registry.key
          - name: REGISTRY_HTTP_TLS_CERTIFICATE
            value: /etc/secrets/registry.crt
          image: registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 5000
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: registry
          ports:
          - containerPort: 5000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 5000
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /registry
            name: registry-storage
          - mountPath: /etc/secrets
            name: registry-certificates
        dnsPolicy: ClusterFirst
        nodeSelector:
          region: infra
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: registry
        serviceAccountName: registry
        terminationGracePeriodSeconds: 30
        volumes:
        - name: registry-storage
          persistentVolumeClaim:
            claimName: registry-claim
        - name: registry-certificates
          secret:
            defaultMode: 420
            secretName: registry-certificates
    test: false
    triggers:
    - type: ConfigChange
  status:
    availableReplicas: 0
    latestVersion: 0
    observedGeneration: 0
    replicas: 0
    unavailableReplicas: 0
    updatedReplicas: 0

```


as-template つけない場合は？

```
[student@workstation secure-route]$ oc export svc,dc docker-registry 
apiVersion: v1
items:
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: null
    labels:
      docker-registry: default
    name: docker-registry
  spec:
    ports:
    - name: 5000-tcp
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      docker-registry: default
    sessionAffinity: ClientIP
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    creationTimestamp: null
    generation: 1
    labels:
      docker-registry: default
    name: docker-registry
  spec:
    replicas: 2
    selector:
      docker-registry: default
    strategy:
      activeDeadlineSeconds: 21600
      resources: {}
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        creationTimestamp: null
        labels:
          docker-registry: default
      spec:
        containers:
        - env:
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_HTTP_NET
            value: tcp
          - name: REGISTRY_HTTP_SECRET
            value: p/KCsDgz3tFOeH9UZB8Ojtp+OsYMbY76OgEmT/SKIyw=
          - name: REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA
            value: "false"
          - name: REGISTRY_OPENSHIFT_SERVER_ADDR
            value: docker-registry.default.svc:5000
          - name: REGISTRY_HTTP_TLS_KEY
            value: /etc/secrets/registry.key
          - name: REGISTRY_HTTP_TLS_CERTIFICATE
            value: /etc/secrets/registry.crt
          image: registry.lab.example.com/openshift3/ose-docker-registry:v3.9.14
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 5000
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: registry
          ports:
          - containerPort: 5000
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 5000
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /registry
            name: registry-storage
          - mountPath: /etc/secrets
            name: registry-certificates
        dnsPolicy: ClusterFirst
        nodeSelector:
          region: infra
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: registry
        serviceAccountName: registry
        terminationGracePeriodSeconds: 30
        volumes:
        - name: registry-storage
          persistentVolumeClaim:
            claimName: registry-claim
        - name: registry-certificates
          secret:
            defaultMode: 420
            secretName: registry-certificates
    test: false
    triggers:
    - type: ConfigChange
  status:
    availableReplicas: 0
    latestVersion: 0
    observedGeneration: 0
    replicas: 0
    unavailableReplicas: 0
    updatedReplicas: 0
kind: List
metadata: {}

```

冒頭にtemplateの宣言があったり、itemsがobjectsに代わってたり、結構な違いがあるな。
テンプレートフォーマットとして利用できるようにフォーマットを変換してるのかな。

----------------------------------------------------------
13：00
----------------------------------------------------------

## Executing Troubleshooting Commands

sosreport のおはなし。
オプションを付けることでdocker関連のログも出力される。
```
$ sosreport -k docker.all=on -k docker.logs=on
```
OpenShiftでは、別のコマンドもある。
```
oc adm diagnostics
```
これをつかって、何か問題がないかとかを確認する。

OpenShift上のイベントログの確認はoc get events
これは、現在ログインしている自分のプロジェクトのイベント情報を取得する。
権限があれば、プロジェクト名指定すれば別のプロジェクトの情報も得られる。

```
oc get events -n <project-name>
```

GUIでもみれるよ。
Monitoring -> Events

POD自身からのログはoc logs <pod-name>
oc logs bc/build-name とかをやるとビルド情報のログも出る。
標準ログだけがでるので、アプリケーション固有のログの場合には別の方法で確認する必要がある。

```
oc logs <pod-name>
```

コンテナのファイルをローカルシステムに持ってくる、逆も可能なコマンドがoc rsync

```
oc rsync <pod>:<pod_dir> <local_dir> -c <container>
```
逆方向も可能だが、コンテナの中のデータは永続化されていないと
再起動すると消えるので、いい手ではない。

コンテナ内のログとかのバックアップにも使えるよ！！

稼働しているコンテナのポートをローカルシステムにつなぐコマンドがoc port-forward
あたかも、コンテナが自分のマシンに動いているようにつなぐことができる。
DBに問題があった時にport-forwardで繋いでmysqlコマンドつかったりdump/restoreもしたり。
Javaのリモートデバッグ用のポートなどをつないで、Eclipseなどでつなぐことも。
```
oc port-forward <pod> [<local_port>:]<remote_port>

```

### Troubleshooting Common Issues

- リソース不足になるとPODが起動しなかったりする。
プロジェクトが無限にリソースを使わないように、リソース制限をかけることも可能。
この場合にログがでたりする。
コンテナはCreateはされるが、起動はできない。PENDINGという状態になる。

ログをみるのには、oc get events などを使う。

- S2I Image Build時のログ

oc logs bc/hello

BUILD_LOGLEVELというEnvを入れることで、ログレベルを変えることが可能。1-5。最大が5。


- ErrImagePull and ImgPullBackOff Errors

外部レジストリや内部レジストリからイメージが取り出せない場合。 
設定を修正するときには、oc edit dc/<deploymentconfig> コマンドで実施する。
サードパーティ製のレジストリを追加していたりする場合には、スペルミスなどがないか注意する。

- Incorrect Docker Configuration
```
--log-level=debug 
```
などでDocker関連の詳細ログも出力可能。

- Master and Node Service Failures

systemctl status コマンドとかで確認する。
  - atomic-openshift-master
  - atomic-openshift-node
  - etcd
  - docker 
などが起動している必要がある。

journalctl -u <unit-name> などもつかえる。


修正対象は/etc/sysconfig/atomic-openshift-master-controllerで弄る。
ただし、サービス再起動が必要。

- Failures in Scheduling Pods

スケジュールされたPODにかんするタスクが動けなかった場合。
ノードがいなかったり。

oc get nodes とか oc get pods -o wide などで切り分けする。


------------------------------------------------

## Guided Exercise: Troubleshooting Common Problems(P125)

- pre

```
[student@workstation secure-route]$ lab common-troubleshoot setup

Checking prerequisites for GE: Troubleshooting Common Problems

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Please wait for setup script to complete...


Overall setup status...........................................  SUCCESS


```

1.1 developerでログインしよう。

```
[student@workstation secure-route]$ oc login -u developer -p redhat https://master.lab.example.com
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation secure-route]$ oc whoami
developer
```

1.2 common-troubleshootプロジェクトを作ろう

```
[student@workstation secure-route]$ oc new-project common-troubleshoot
Now using project "common-troubleshoot" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
```


2. S2I Application をDeployする。

2.1 php-helloworldのアプリケーションをソースコードから作ろう。
エラーが出るはず。

```
[student@workstation secure-route]$ oc new-app --name=hello -i php:5.4 http://services.lab.example.com/php-helloworld
error: multiple images or templates matched "php:5.4": 2

The argument "php:5.4" could apply to the following Docker images, OpenShift image streams, or templates:

* Image stream "php" (tag "5.6") in project "openshift"
  Use --image-stream="openshift/php:5.6" to specify this image or template

* Image stream "php" (tag "7.0") in project "openshift"
  Use --image-stream="openshift/php:7.0" to specify this image or template


```
STREAMTAG回りで問題がありそうだよね。（5.4なんてないってことかな）

2.2 php のイメージストリームの状態を確認してみよう。

-n はnamespaceかな？必須なのかな？
```
[student@workstation secure-route]$ oc describe is php -n openshift
Name:			php
Namespace:		openshift
Created:		23 hours ago
Labels:			<none>
Annotations:		openshift.io/display-name=PHP
			openshift.io/image.dockerRepositoryCheck=2019-09-24T05:11:45Z
Docker Pull Spec:	docker-registry.default.svc:5000/openshift/php
Image Lookup:		local=false
Unique Images:		2
Tags:			5

7.1 (latest)
  tagged from registry.lab.example.com/rhscl/php-71-rhel7:latest

  Build and run PHP 7.1 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.1/README.md.
  Tags: builder, php
  Supports: php:7.1, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/rhscl/php-71-rhel7:latest" not found
      23 hours ago

7.0
  tagged from registry.lab.example.com/rhscl/php-70-rhel7:latest

  Build and run PHP 7.0 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/7.0/README.md.
  Tags: builder, php
  Supports: php:7.0, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd
      23 hours ago

5.6
  tagged from registry.lab.example.com/rhscl/php-56-rhel7:latest

  Build and run PHP 5.6 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.6/README.md.
  Tags: builder, php
  Supports: php:5.6, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  * registry.lab.example.com/rhscl/php-56-rhel7@sha256:920c2cf85b5da5d0701898f0ec9ee567473fa4b9af6f3ac5b2b3f863796bbd68
      23 hours ago

5.5
  tagged from registry.lab.example.com/openshift3/php-55-rhel7:latest

  Build and run PHP 5.5 applications on RHEL 7. For more information about using this builder image, including OpenShift considerations, see https://github.com/sclorg/s2i-php-container/blob/master/5.5/README.md.
  Tags: hidden, builder, php
  Supports: php:5.5, php
  Example Repo: https://github.com/openshift/cakephp-ex.git

  ! error: Import failed (NotFound): dockerimage.image.openshift.io "registry.lab.example.com/openshift3/php-55-rhel7:latest" not found
      23 hours ago

```
-> 7.1 / 5.5 で、それぞれImport failedでdocker.imageがないっていってる。
ここでわかるのは、7.0/5.6が有効なタグであるという点。


2.3 7.0でDeployしなおしてみよう。

```
[student@workstation secure-route]$ oc new-app --name=hello -i php:7.0 http://services.lab.example.com/php-helloworld
--> Found image c101534 (2 years old) in image stream "openshift/php" under tag "7.0" for "php:7.0"

    Apache 2.4 with PHP 7.0 
    ----------------------- 
    PHP 7.0 available as docker container is a base platform for building and running various PHP 7.0 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php70, rh-php70

    * The source repository appears to match: php
    * A source build using source code from http://services.lab.example.com/php-helloworld will be created
      * The resulting image will be pushed to image stream "hello:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "hello"
    * Port 8080/tcp will be load balanced by service "hello"
      * Other containers can access this service through the hostname "hello"

--> Creating resources ...
    imagestream "hello" created
    buildconfig "hello" created
    deploymentconfig "hello" created
    service "hello" created
--> Success
    Build scheduled, use 'oc logs -f bc/hello' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.

```
2.4 oc get pods -o wideコマンドで情報をとってみよう。
いろいろ情報をとってみた。

```
[student@workstation secure-route]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
hello-1-build   0/1       Pending   0          26s
[student@workstation secure-route]$ oc get svc
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
hello     ClusterIP   172.30.255.26   <none>        8080/TCP   30s
[student@workstation secure-route]$ oc get pods -o wide
NAME            READY     STATUS    RESTARTS   AGE       IP        NODE
hello-1-build   0/1       Pending   0          34s       <none>    <none>
[student@workstation secure-route]$ oc get svc -o wide
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
hello     ClusterIP   172.30.255.26   <none>        8080/TCP   38s       app=hello,deploymentconfig=hello

```
-> hello-1-build のSTATUS がPendingになっていることがわかる。

3. 対象のPODのlogを取得してみよう。

```
[student@workstation secure-route]$ oc logs hello-1-build
[student@workstation secure-route]$ 

```
なんもでない。これで正常みたい。ここからさらにもぐる。

4. Projectのログをおいかけてみよう。

```
[student@workstation secure-route]$ oc get events
LAST SEEN   FIRST SEEN   COUNT     NAME                             KIND      SUBOBJECT   TYPE      REASON             SOURCE              MESSAGE
30s         3m           15        hello-1-build.15c793fbff6acf95   Pod                   Warning   FailedScheduling   default-scheduler   0/3 nodes are available: 1 MatchNodeSelector, 2 NodeNotReady.

```
利用可能なNodeがない？
oc describe pod <pod-name> で詳細をみてみよう。

```
[student@workstation secure-route]$ oc describe pod hello-1-build
Name:           hello-1-build
Namespace:      common-troubleshoot
Node:           <none>
Labels:         openshift.io/build.name=hello-1
Annotations:    openshift.io/build.name=hello-1
                openshift.io/scc=privileged
Status:         Pending
IP:             
Controlled By:  Build/hello-1
Init Containers:
  git-clone:
    Image:  registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14
    Port:   <none>
    Command:
      openshift-git-clone
    Args:
      --loglevel=0
    Environment:
      BUILD:  {"kind":"Build","apiVersion":"v1","metadata":{"name":"hello-1","namespace":"common-troubleshoot","selfLink":"/apis/build.openshift.io/v1/namespaces/common-troubleshoot/builds/hello-1","uid":"ea7e076a-df4c-11e9-98ef-52540000fa0a","resourceVersion":"177496","creationTimestamp":"2019-09-25T04:28:25Z","labels":{"app":"hello","buildconfig":"hello","openshift.io/build-config.name":"hello","openshift.io/build.start-policy":"Serial"},"annotations":{"openshift.io/build-config.name":"hello","openshift.io/build.number":"1"},"ownerReferences":[{"apiVersion":"build.openshift.io/v1","kind":"BuildConfig","name":"hello","uid":"ea756cc1-df4c-11e9-98ef-52540000fa0a","controller":true}]},"spec":{"serviceAccount":"builder","source":{"type":"Git","git":{"uri":"http://services.lab.example.com/php-helloworld"}},"strategy":{"type":"Source","sourceStrategy":{"from":{"kind":"DockerImage","name":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd"}}},"output":{"to":{"kind":"DockerImage","name":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest"},"pushSecret":{"name":"builder-dockercfg-6st72"}},"resources":{},"postCommit":{},"nodeSelector":null,"triggeredBy":[{"message":"Image change","imageChangeBuild":{"imageID":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd","fromRef":{"kind":"ImageStreamTag","namespace":"openshift","name":"php:7.0"}}}]},"status":{"phase":"New","outputDockerImageReference":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest","config":{"kind":"BuildConfig","namespace":"common-troubleshoot","name":"hello"},"output":{}}}

      SOURCE_REPOSITORY:  http://services.lab.example.com/php-helloworld
      SOURCE_URI:         http://services.lab.example.com/php-helloworld
      ORIGIN_VERSION:     v3.9.14
      ALLOWED_UIDS:       1-
      DROP_CAPS:          KILL,MKNOD,SETGID,SETUID
    Mounts:
      /tmp/build from buildworkdir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from builder-token-fqfxc (ro)
  manage-dockerfile:
    Image:  registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14
    Port:   <none>
    Command:
      openshift-manage-dockerfile
    Args:
      --loglevel=0
    Environment:
      BUILD:  {"kind":"Build","apiVersion":"v1","metadata":{"name":"hello-1","namespace":"common-troubleshoot","selfLink":"/apis/build.openshift.io/v1/namespaces/common-troubleshoot/builds/hello-1","uid":"ea7e076a-df4c-11e9-98ef-52540000fa0a","resourceVersion":"177496","creationTimestamp":"2019-09-25T04:28:25Z","labels":{"app":"hello","buildconfig":"hello","openshift.io/build-config.name":"hello","openshift.io/build.start-policy":"Serial"},"annotations":{"openshift.io/build-config.name":"hello","openshift.io/build.number":"1"},"ownerReferences":[{"apiVersion":"build.openshift.io/v1","kind":"BuildConfig","name":"hello","uid":"ea756cc1-df4c-11e9-98ef-52540000fa0a","controller":true}]},"spec":{"serviceAccount":"builder","source":{"type":"Git","git":{"uri":"http://services.lab.example.com/php-helloworld"}},"strategy":{"type":"Source","sourceStrategy":{"from":{"kind":"DockerImage","name":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd"}}},"output":{"to":{"kind":"DockerImage","name":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest"},"pushSecret":{"name":"builder-dockercfg-6st72"}},"resources":{},"postCommit":{},"nodeSelector":null,"triggeredBy":[{"message":"Image change","imageChangeBuild":{"imageID":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd","fromRef":{"kind":"ImageStreamTag","namespace":"openshift","name":"php:7.0"}}}]},"status":{"phase":"New","outputDockerImageReference":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest","config":{"kind":"BuildConfig","namespace":"common-troubleshoot","name":"hello"},"output":{}}}

      SOURCE_REPOSITORY:  http://services.lab.example.com/php-helloworld
      SOURCE_URI:         http://services.lab.example.com/php-helloworld
      ORIGIN_VERSION:     v3.9.14
      ALLOWED_UIDS:       1-
      DROP_CAPS:          KILL,MKNOD,SETGID,SETUID
    Mounts:
      /tmp/build from buildworkdir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from builder-token-fqfxc (ro)
Containers:
  sti-build:
    Image:  registry.lab.example.com/openshift3/ose-sti-builder:v3.9.14
    Port:   <none>
    Command:
      openshift-sti-build
    Args:
      --loglevel=0
    Environment:
      BUILD:  {"kind":"Build","apiVersion":"v1","metadata":{"name":"hello-1","namespace":"common-troubleshoot","selfLink":"/apis/build.openshift.io/v1/namespaces/common-troubleshoot/builds/hello-1","uid":"ea7e076a-df4c-11e9-98ef-52540000fa0a","resourceVersion":"177496","creationTimestamp":"2019-09-25T04:28:25Z","labels":{"app":"hello","buildconfig":"hello","openshift.io/build-config.name":"hello","openshift.io/build.start-policy":"Serial"},"annotations":{"openshift.io/build-config.name":"hello","openshift.io/build.number":"1"},"ownerReferences":[{"apiVersion":"build.openshift.io/v1","kind":"BuildConfig","name":"hello","uid":"ea756cc1-df4c-11e9-98ef-52540000fa0a","controller":true}]},"spec":{"serviceAccount":"builder","source":{"type":"Git","git":{"uri":"http://services.lab.example.com/php-helloworld"}},"strategy":{"type":"Source","sourceStrategy":{"from":{"kind":"DockerImage","name":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd"}}},"output":{"to":{"kind":"DockerImage","name":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest"},"pushSecret":{"name":"builder-dockercfg-6st72"}},"resources":{},"postCommit":{},"nodeSelector":null,"triggeredBy":[{"message":"Image change","imageChangeBuild":{"imageID":"registry.lab.example.com/rhscl/php-70-rhel7@sha256:23765e00df8d0a934ce4f2e22802bc0211a6d450bfbb69144b18cb0b51008cdd","fromRef":{"kind":"ImageStreamTag","namespace":"openshift","name":"php:7.0"}}}]},"status":{"phase":"New","outputDockerImageReference":"docker-registry.default.svc:5000/common-troubleshoot/hello:latest","config":{"kind":"BuildConfig","namespace":"common-troubleshoot","name":"hello"},"output":{}}}

      SOURCE_REPOSITORY:    http://services.lab.example.com/php-helloworld
      SOURCE_URI:           http://services.lab.example.com/php-helloworld
      ORIGIN_VERSION:       v3.9.14
      ALLOWED_UIDS:         1-
      DROP_CAPS:            KILL,MKNOD,SETGID,SETUID
      PUSH_DOCKERCFG_PATH:  /var/run/secrets/openshift.io/push
    Mounts:
      /tmp/build from buildworkdir (rw)
      /var/run/crio/crio.sock from crio-socket (rw)
      /var/run/docker.sock from docker-socket (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from builder-token-fqfxc (ro)
      /var/run/secrets/openshift.io/push from builder-dockercfg-6st72-push (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  buildworkdir:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:  
  docker-socket:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/docker.sock
    HostPathType:  
  crio-socket:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/crio/crio.sock
    HostPathType:  
  builder-dockercfg-6st72-push:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  builder-dockercfg-6st72
    Optional:    false
  builder-token-fqfxc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  builder-token-fqfxc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  0s (x22 over 5m)  default-scheduler  0/3 nodes are available: 1 MatchNodeSelector, 2 NodeNotReady.

```
-> Eventsのセクション内にScheduleでFailしているログが確認できる。

5. このトラブルシュートをしてみよう。なお、本作業はmasterノードのrootユーザで実行する必要がある。

```
[root@master ~]# hostname
master.lab.example.com
[root@master ~]# oc whoami
system:admin

```
あ、SSHコマンドでいいのか。。。

```
[student@workstation secure-route]$ ssh root@master oc get nodes
NAME                     STATUS     ROLES     AGE       VERSION
master.lab.example.com   Ready      master    23h       v1.9.1+a0ce1bc657
node1.lab.example.com    NotReady   compute   23h       v1.9.1+a0ce1bc657
node2.lab.example.com    NotReady   compute   23h       v1.9.1+a0ce1bc657
```
-> Node1/2ともにNotReadyやんけ！！

6. これを解消せよ。
新たなターミナルを2つ起動して、各NodeにrootでSSHログインする。

各ホスト上でatomic-openshift-node サービスの起動状態を確認する。

node1
```
[root@node1 ~]# systemctl status atomic-openshift-node.service -l
● atomic-openshift-node.service - OpenShift Node
   Loaded: loaded (/etc/systemd/system/atomic-openshift-node.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/atomic-openshift-node.service.d
           └─openshift-sdn-ovs.conf
   Active: active (running) since Tue 2019-09-24 14:13:40 JST; 23h ago
     Docs: https://github.com/openshift/origin
 Main PID: 26461 (openshift)
   CGroup: /system.slice/atomic-openshift-node.service
           └─26461 /usr/bin/openshift start node --config=/etc/origin/node/node-config.yaml --loglevel=2

Sep 25 13:39:24 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:24.487839   26461 generic.go:197] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:25 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:25.488467   26461 remote_runtime.go:169] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:25 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:25.488626   26461 kuberuntime_sandbox.go:192] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:25 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:25.488638   26461 generic.go:197] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:26.022364   26461 remote_image.go:67] ListImages with filter nil from image service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:26.022777   26461 kuberuntime_image.go:101] ListImages failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: W0925 13:39:26.023090   26461 image_gc_manager.go:188] [imageGCManager] Failed to update image list: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:26.489226   26461 remote_runtime.go:169] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:26.489253   26461 kuberuntime_sandbox.go:192] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:26 node1.lab.example.com atomic-openshift-node[26461]: E0925 13:39:26.489263   26461 generic.go:197] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?

```
node2
```
[root@node2 ~]# systemctl status atomic-openshift-node.service -l
● atomic-openshift-node.service - OpenShift Node
   Loaded: loaded (/etc/systemd/system/atomic-openshift-node.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/atomic-openshift-node.service.d
           └─openshift-sdn-ovs.conf
   Active: active (running) since Tue 2019-09-24 14:13:40 JST; 23h ago
     Docs: https://github.com/openshift/origin
 Main PID: 26496 (openshift)
   CGroup: /system.slice/atomic-openshift-node.service
           └─26496 /usr/bin/openshift start node --config=/etc/origin/node/node-config.yaml --loglevel=2

Sep 25 13:39:46 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:46.713007   26496 kuberuntime_sandbox.go:192] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:46 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:46.713016   26496 generic.go:197] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:46 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:46.762531   26496 kubelet.go:2118] Container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:46 node2.lab.example.com atomic-openshift-node[26496]: I0925 13:39:46.947211   26496 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 19m8.435249888s ago; threshold is 3m0s]
Sep 25 13:39:47 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:47.713668   26496 remote_runtime.go:169] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:47 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:47.714149   26496 kuberuntime_sandbox.go:192] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:47 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:47.714344   26496 generic.go:197] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:48 node2.lab.example.com atomic-openshift-node[26496]: I0925 13:39:48.236472   26496 kubelet_node_status.go:466] Using node IP: "172.25.250.12"
Sep 25 13:39:48 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:48.236947   26496 remote_runtime.go:69] Version from runtime service failed: rpc error: code = Unknown desc = failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Sep 25 13:39:48 node2.lab.example.com atomic-openshift-node[26496]: E0925 13:39:48.236963   26496 kuberuntime_manager.go:245] Get remote runtime version failed: rpc error: code = Unknown desc = failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?

```

-> node1はdockerデーモンがしんでそう。node2もおなじかな。

7. 各ノードのdockerデーモンのステータスを確認する。

node1
```
[root@node1 ~]# systemctl status docker.service -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: inactive (dead) since Wed 2019-09-25 13:20:25 JST; 21min ago
     Docs: http://docs.docker.com
 Main PID: 34806 (code=exited, status=0/SUCCESS)

Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.287518785+09:00" level=error msg="containerd: deleting container" error="exit status 1: \"container 51dfd2471f63d80f0f3dae766fdaef1dd218ef25d72ecc2ff06f9d5100c00825 does not exist\\none or more of the container deletions failed\\n\""
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.291863758+09:00" level=warning msg="92dc5dfc74b6d82ac1a9d2190fa7f172a60c1f77c085213c8fe54ce5d7f17148 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.295078701+09:00" level=error msg="containerd: deleting container" error="exit status 1: \"container 0596e57ceaf37e3a3f82e76e2341257e25e9f41c631c77055a80a68401884a11 does not exist\\none or more of the container deletions failed\\n\""
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.299686676+09:00" level=error msg="containerd: deleting container" error="exit status 1: \"container be6bdd465db0b6650343ac5035e17f030feeb9db646734102b989dd0d334dd2f does not exist\\none or more of the container deletions failed\\n\""
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.303068649+09:00" level=warning msg="0596e57ceaf37e3a3f82e76e2341257e25e9f41c631c77055a80a68401884a11 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.312349057+09:00" level=warning msg="51dfd2471f63d80f0f3dae766fdaef1dd218ef25d72ecc2ff06f9d5100c00825 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.317076954+09:00" level=warning msg="883f9b15ca8961546af614650a81cd327e76312b72727f029d01edb7928a371d cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.318722059+09:00" level=warning msg="be6bdd465db0b6650343ac5035e17f030feeb9db646734102b989dd0d334dd2f cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:24 node1.lab.example.com dockerd-current[34806]: time="2019-09-25T13:20:24.332224538+09:00" level=info msg="stopping containerd after receiving terminated"
Sep 25 13:20:25 node1.lab.example.com systemd[1]: Stopped Docker Application Container Engine.
[root@node1 ~]# 

```

node2
```
[root@node2 ~]# systemctl status docker.service -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: inactive (dead) since Wed 2019-09-25 13:20:38 JST; 20min ago
     Docs: http://docs.docker.com
 Main PID: 36226 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/docker.service
           ├─36051 /usr/bin/docker-containerd-shim-current 3a9464548996604df72e0a40d0f4826658a26e37ddead329e8ce621ade43d37f /var/run/docker/libcontainerd/3a9464548996604df72e0a40d0f4826658a26e37ddead329e8ce621ade43d37f /usr/libexec/docker/docker-runc-current
           ├─87536 /usr/bin/docker-containerd-shim-current b22c190ca85ba405dc0e95803f580a2763cc817e3e5bda4de658085736392e7e /var/run/docker/libcontainerd/b22c190ca85ba405dc0e95803f580a2763cc817e3e5bda4de658085736392e7e /usr/libexec/docker/docker-runc-current
           ├─87636 /usr/bin/docker-containerd-shim-current d5eac3f23b849017d0e950afdd43c53cec47c1b557561154b8960e31096b5631 /var/run/docker/libcontainerd/d5eac3f23b849017d0e950afdd43c53cec47c1b557561154b8960e31096b5631 /usr/libexec/docker/docker-runc-current
           ├─87680 /usr/bin/docker-containerd-shim-current a54aa087232b2866b4986f550f34f079ba849ed6e55ac354faa177160324e950 /var/run/docker/libcontainerd/a54aa087232b2866b4986f550f34f079ba849ed6e55ac354faa177160324e950 /usr/libexec/docker/docker-runc-current
           ├─87890 /usr/bin/docker-containerd-shim-current 8236f18cd468686a148fe39c631a051ee0ea204079f2503d9a53ccdbbd1cf534 /var/run/docker/libcontainerd/8236f18cd468686a148fe39c631a051ee0ea204079f2503d9a53ccdbbd1cf534 /usr/libexec/docker/docker-runc-current
           ├─87957 /usr/bin/docker-containerd-shim-current 2659c7ac30f5bb044fe79e86f1bba94618f1b3aa9b4ae73fcccc657df22b6d3c /var/run/docker/libcontainerd/2659c7ac30f5bb044fe79e86f1bba94618f1b3aa9b4ae73fcccc657df22b6d3c /usr/libexec/docker/docker-runc-current
           ├─88057 /usr/bin/docker-containerd-shim-current e09ef7312fba503d86ebdf8db94f9e27f473677c94e8703d04fa07cfd65fba02 /var/run/docker/libcontainerd/e09ef7312fba503d86ebdf8db94f9e27f473677c94e8703d04fa07cfd65fba02 /usr/libexec/docker/docker-runc-current
           ├─88138 /usr/bin/docker-containerd-shim-current 270370d01b2b5b4423db88d44439215b77f6eff4655d687f2a92f32cce5e8651 /var/run/docker/libcontainerd/270370d01b2b5b4423db88d44439215b77f6eff4655d687f2a92f32cce5e8651 /usr/libexec/docker/docker-runc-current
           ├─88187 /usr/bin/docker-containerd-shim-current 960864431ff89538729cac29a03872723f19afabd49fc758866aa6cde50bb513 /var/run/docker/libcontainerd/960864431ff89538729cac29a03872723f19afabd49fc758866aa6cde50bb513 /usr/libexec/docker/docker-runc-current
           └─88330 /usr/bin/docker-containerd-shim-current c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365 /var/run/docker/libcontainerd/c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365 /usr/libexec/docker/docker-runc-current

Sep 25 13:20:36 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:36.851344758+09:00" level=error msg="Error during layer Store.Cleanup(): device or resource busy"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.853698682+09:00" level=error msg="Error umounting daemon repository: %!v(MISSING)"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.875046451+09:00" level=info msg="stopping containerd after receiving terminated"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.941984452+09:00" level=error msg="Create container failed with error: grpc: the client connection is closing"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.942066073+09:00" level=error msg="stream copy error: reading from a closed fifo\ngithub.com/docker/docker/vendor/github.com/tonistiigi/fifo.(*fifo).Read\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/vendor/github.com/tonistiigi/fifo/fifo.go:142\nbufio.(*Reader).fill\n\t/usr/lib/golang/src/bufio/bufio.go:97\nbufio.(*Reader).WriteTo\n\t/usr/lib/golang/src/bufio/bufio.go:481\nio.copyBuffer\n\t/usr/lib/golang/src/io/io.go:382\nio.Copy\n\t/usr/lib/golang/src/io/io.go:362\ngithub.com/docker/docker/pkg/pools.Copy\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/pkg/pools/pools.go:60\ngithub.com/docker/docker/container/stream.(*Config).CopyToPipe.func1.1\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/container/stream/streams.go:121\nruntime.goexit\n\t/usr/lib/golang/src/runtime/asm_amd64.s:2337"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.941970951+09:00" level=error msg="stream copy error: reading from a closed fifo\ngithub.com/docker/docker/vendor/github.com/tonistiigi/fifo.(*fifo).Read\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/vendor/github.com/tonistiigi/fifo/fifo.go:142\nbufio.(*Reader).fill\n\t/usr/lib/golang/src/bufio/bufio.go:97\nbufio.(*Reader).WriteTo\n\t/usr/lib/golang/src/bufio/bufio.go:481\nio.copyBuffer\n\t/usr/lib/golang/src/io/io.go:382\nio.Copy\n\t/usr/lib/golang/src/io/io.go:362\ngithub.com/docker/docker/pkg/pools.Copy\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/pkg/pools/pools.go:60\ngithub.com/docker/docker/container/stream.(*Config).CopyToPipe.func1.1\n\t/builddir/build/BUILD/docker-87f2fab3d32f145760b94b87b93daa83e6841ee7/_build/src/github.com/docker/docker/container/stream/streams.go:121\nruntime.goexit\n\t/usr/lib/golang/src/runtime/asm_amd64.s:2337"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.954524584+09:00" level=warning msg="69ae7dbfa6bb594dd87af3906091220868f9236cbea8430f9095667f193e6a60 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.954551045+09:00" level=error msg="Handler for POST /v1.26/containers/69ae7dbfa6bb594dd87af3906091220868f9236cbea8430f9095667f193e6a60/start returned error: grpc: the client connection is closing"
Sep 25 13:20:37 node2.lab.example.com dockerd-current[36226]: time="2019-09-25T13:20:37.955239993+09:00" level=error msg="Handler for POST /v1.26/containers/69ae7dbfa6bb594dd87af3906091220868f9236cbea8430f9095667f193e6a60/start returned error: grpc: the client connection is closing"
Sep 25 13:20:38 node2.lab.example.com systemd[1]: Stopped Docker Application Container Engine.

```

-> どちらもDocker.Serviceが止まっていることが分かる。
```
systemd[1]: Stopped Docker Application Container Engine.
```

8. dockerデーモンを起動する。

node1
```
[root@node1 ~]# systemctl start docker.service
[root@node1 ~]# 
[root@node1 ~]# systemctl status docker.service -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: active (running) since Wed 2019-09-25 13:45:16 JST; 5s ago
     Docs: http://docs.docker.com
 Main PID: 79381 (dockerd-current)
   Memory: 33.5M
   CGroup: /system.slice/docker.service
           ├─79381 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --authorization-plugin=rhel-push-plugin --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --signature-verification=False --storage-driver overlay2 --mtu=1450 --add-registry registry.lab.example.com --add-registry registry.access.redhat.com --block-registry registry.access.redhat.com --block-registry docker.io --insecure-registry registry.lab.example.com --add-registry registry.access.redhat.com
           ├─79387 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true
           ├─79680 /usr/bin/docker-containerd-shim-current 13664e61d0fb4555498d3fcef13cb95317571aa90a3b3c39e4b514fcda1d9410 /var/run/docker/libcontainerd/13664e61d0fb4555498d3fcef13cb95317571aa90a3b3c39e4b514fcda1d9410 /usr/libexec/docker/docker-runc-current
           └─79922 /usr/bin/docker-containerd-shim-current d3632ec8d0798a16c1833f2955519f57036b5a3f63ac3208449d5f5ee6b1c94a /var/run/docker/libcontainerd/d3632ec8d0798a16c1833f2955519f57036b5a3f63ac3208449d5f5ee6b1c94a /usr/libexec/docker/docker-runc-current

Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.77006722+09:00" level=error msg="containerd: deleting container" error="exit status 1: \"container 422d3cdaa07b6c7ee8d355067a4d8f4caeb3bd8837a90e460513bedc2a428f12 does not exist\\none or more of the container deletions failed\\n\""
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.781465800+09:00" level=warning msg="422d3cdaa07b6c7ee8d355067a4d8f4caeb3bd8837a90e460513bedc2a428f12 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:45:20 node1.lab.example.com oci-systemd-hook[79999]: systemdhook <debug>: 94530ad403a7: Skipping as container command is /usr/bin/pod, not init or systemd
Sep 25 13:45:20 node1.lab.example.com oci-umount[80000]: umounthook <debug>: 94530ad403a7: only runs in prestart stage, ignoring
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.827970232+09:00" level=error msg="containerd: deleting container" error="exit status 1: \"container 94530ad403a74e1c8a469a4ac16268561effa4258cd8dc26f772ec73710bad55 does not exist\\none or more of the container deletions failed\\n\""
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.859532451+09:00" level=warning msg="94530ad403a74e1c8a469a4ac16268561effa4258cd8dc26f772ec73710bad55 cleanup: failed to unmount secrets: invalid argument"
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.875141026+09:00" level=error msg="Handler for POST /v1.26/containers/92dc5dfc74b6d82ac1a9d2190fa7f172a60c1f77c085213c8fe54ce5d7f17148/stop?t=10 returned error: Container 92dc5dfc74b6d82ac1a9d2190fa7f172a60c1f77c085213c8fe54ce5d7f17148 is already stopped"
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.875740224+09:00" level=error msg="Handler for POST /v1.26/containers/92dc5dfc74b6d82ac1a9d2190fa7f172a60c1f77c085213c8fe54ce5d7f17148/stop returned error: Container 92dc5dfc74b6d82ac1a9d2190fa7f172a60c1f77c085213c8fe54ce5d7f17148 is already stopped"
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.884911367+09:00" level=error msg="Handler for POST /v1.26/containers/46bbf071acf5ec51f89e31b09a94d370675d3a15117a845e195d7be929f5aa18/stop?t=10 returned error: Container 46bbf071acf5ec51f89e31b09a94d370675d3a15117a845e195d7be929f5aa18 is already stopped"
Sep 25 13:45:20 node1.lab.example.com dockerd-current[79381]: time="2019-09-25T13:45:20.885720962+09:00" level=error msg="Handler for POST /v1.26/containers/46bbf071acf5ec51f89e31b09a94d370675d3a15117a845e195d7be929f5aa18/stop returned error: Container 46bbf071acf5ec51f89e31b09a94d370675d3a15117a845e195d7be929f5aa18 is already stopped"
[root@node1 ~]# 

```

node2
```
[root@node2 ~]# systemctl start docker.service
[root@node2 ~]# 
[root@node2 ~]# systemctl status docker.service -l
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
   Active: active (running) since Wed 2019-09-25 13:45:43 JST; 2s ago
     Docs: http://docs.docker.com
 Main PID: 91944 (dockerd-current)
   Memory: 2.9G
   CGroup: /system.slice/docker.service
           ├─36051 /usr/bin/docker-containerd-shim-current 3a9464548996604df72e0a40d0f4826658a26e37ddead329e8ce621ade43d37f /var/run/docker/libcontainerd/3a9464548996604df72e0a40d0f4826658a26e37ddead329e8ce621ade43d37f /usr/libexec/docker/docker-runc-current
           ├─91944 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --authorization-plugin=rhel-push-plugin --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --signature-verification=False --storage-driver overlay2 --mtu=1450 --add-registry registry.lab.example.com --add-registry registry.access.redhat.com --block-registry registry.access.redhat.com --block-registry docker.io --insecure-registry registry.lab.example.com --add-registry registry.access.redhat.com
           ├─91950 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc --runtime-args --systemd-cgroup=true
           ├─92453 /usr/bin/docker-containerd-shim-current 33e978d30fa607179aed3ffd4d3121446708e70459eaac5872bde813451dea03 /var/run/docker/libcontainerd/33e978d30fa607179aed3ffd4d3121446708e70459eaac5872bde813451dea03 /usr/libexec/docker/docker-runc-current
           └─92563 /usr/bin/docker-containerd-shim-current 88e0a0d340c3f66871782ef96deee6177f06c6e62813e17bae8ca8e2a726cd33 /var/run/docker/libcontainerd/88e0a0d340c3f66871782ef96deee6177f06c6e62813e17bae8ca8e2a726cd33 /usr/libexec/docker/docker-runc-current

Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.091200200+09:00" level=info msg="API listen on /var/run/docker.sock"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.518452175+09:00" level=error msg="Handler for POST /v1.26/containers/c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365/stop?t=10 returned error: Container c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365 is already stopped"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.519063569+09:00" level=error msg="Handler for POST /v1.26/containers/c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365/stop returned error: Container c59c140ba253e51c6453349ee9177c27b37f20287fed09092a24b7451aba1365 is already stopped"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.520628389+09:00" level=error msg="Handler for POST /v1.26/containers/00923cebedfb0f535bcd7677a8375a57412fcf688b9a24256a68083e6830cd85/stop?t=10 returned error: Container 00923cebedfb0f535bcd7677a8375a57412fcf688b9a24256a68083e6830cd85 is already stopped"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.520889755+09:00" level=error msg="Handler for POST /v1.26/containers/00923cebedfb0f535bcd7677a8375a57412fcf688b9a24256a68083e6830cd85/stop returned error: Container 00923cebedfb0f535bcd7677a8375a57412fcf688b9a24256a68083e6830cd85 is already stopped"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.522277829+09:00" level=error msg="Handler for POST /v1.26/containers/7d6f2fd3f1db06715ef2d3d36f47fb1f1ee64ff15779ea67bcd9d9747905e645/stop?t=10 returned error: Container 7d6f2fd3f1db06715ef2d3d36f47fb1f1ee64ff15779ea67bcd9d9747905e645 is already stopped"
Sep 25 13:45:43 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:43.522695454+09:00" level=error msg="Handler for POST /v1.26/containers/7d6f2fd3f1db06715ef2d3d36f47fb1f1ee64ff15779ea67bcd9d9747905e645/stop returned error: Container 7d6f2fd3f1db06715ef2d3d36f47fb1f1ee64ff15779ea67bcd9d9747905e645 is already stopped"
Sep 25 13:45:43 node2.lab.example.com oci-umount[92486]: umounthook <debug>: prestart container_id:33e978d30fa6 rootfs:/var/lib/docker/overlay2/aa8f07b21f5108665952699f9608526228675b90e400fbb3dc3284ecbd3ea205/merged
Sep 25 13:45:45 node2.lab.example.com oci-umount[92589]: umounthook <debug>: prestart container_id:88e0a0d340c3 rootfs:/var/lib/docker/overlay2/f7d7f6f9b9f6db8c3b66d9bb16f7975355d2e650f1e59bfba701da650a2fc6b5/merged
Sep 25 13:45:45 node2.lab.example.com dockerd-current[91944]: time="2019-09-25T13:45:45.203202579+09:00" level=warning msg="Unknown healthcheck type 'NONE' (expected 'CMD') in container 88e0a0d340c3f66871782ef96deee6177f06c6e62813e17bae8ca8e2a726cd33"
[root@node2 ~]#
```

両方きどうしてきた。

9. 再度get nodesコマンドでノードの状態を確認する。

```
[student@workstation secure-route]$ ssh root@master oc get nodes
NAME                     STATUS    ROLES     AGE       VERSION
master.lab.example.com   Ready     master    23h       v1.9.1+a0ce1bc657
node1.lab.example.com    Ready     compute   23h       v1.9.1+a0ce1bc657
node2.lab.example.com    Ready     compute   23h       v1.9.1+a0ce1bc657

```

Completeになった！！


10. podsの状態を確認する。

```
[student@workstation secure-route]$ oc get pods
NAME            READY     STATUS      RESTARTS   AGE
hello-1-build   0/1       Completed   0          18m
hello-1-mcvbq   1/1       Running     0          51s
[student@workstation secure-route]$ 

```

起動してきた。

おまけ

```
[student@workstation secure-route]$ oc get pods -o wide
NAME            READY     STATUS      RESTARTS   AGE       IP            NODE
hello-1-build   0/1       Completed   0          18m       10.128.0.21   node1.lab.example.com
hello-1-mcvbq   1/1       Running     0          1m        10.129.0.42   node2.lab.example.com

[student@workstation secure-route]$ oc get svc -o wide
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR
hello     ClusterIP   172.30.255.26   <none>        8080/TCP   19m       app=hello,deploymentconfig=hello

[student@workstation secure-route]$ oc get route
No resources found.
[student@workstation secure-route]$ 


```

oc describe is コマンドで状態を見てみる。

```
[student@workstation secure-route]$ oc describe is
Name:			hello
Namespace:		common-troubleshoot
Created:		20 minutes ago
Labels:			app=hello
Annotations:		openshift.io/generated-by=OpenShiftNewApp
Docker Pull Spec:	docker-registry.default.svc:5000/common-troubleshoot/hello
Image Lookup:		local=false
Unique Images:		1
Tags:			1

latest
  no spec tag

  * docker-registry.default.svc:5000/common-troubleshoot/hello@sha256:0ea2957bb73f4ab266fda0a704937ed06a42d4e42b48e25004f46bb468fa2ab4
      2 minutes ago

```
-> 2minutes ago にis からPushされたことが確認できる。

11. Cleanupせよ

```
[student@workstation secure-route]$ oc delete project common-troubleshoot
project "common-troubleshoot" deleted

```

## Lab Executing Commands(P133) [13:52]

- pre

```
[student@workstation ~]$ lab execute-review setup

Checking prerequisites for Lab: Executing Commands

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS
 Setting up for the lab: 
 . Logging in as the developer user............................  SUCCESS

Downloading files for Lab: Executing Commands

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.

Please wait. Do not press any keys or interrupt the script...

 . Creating the execute-review project.........................  SUCCESS


Overall setup status...........................................  SUCCESS


```

1. 設定概要
セットアップスクリプトは execute-reviewというプロジェクトをdeveloperユーザで作る。
Workstation上で/home/student/DO280/labs/execute-review ディレクトリに移動し、
git clone コマンドでカスタムのnode-hello を作成する。
git clone http://services.lab.example.com/node-hello

名前をnode-hello、タグにlatestを付けてDockerを使ってビルドする。
workstation上のimageを確認し、所定のレジストリから？？
その後、プライベートレジストリにpushせよ。

セットアップスクリプトを流したら、ローカルにDockerイメージができるのかな？
何もしてない状態では何もなかった。

```
[student@workstation secure-route]$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
```
その名前をかえて、かつタグを付けてPushしろ、ってことかしらね。

```
[student@workstation ~]$ cd /home/student/DO280/labs/execute-review/
[student@workstation execute-review]$ git clone http://services.lab.example.com/node-hello
Cloning into 'node-hello'...
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 0), reused 0 (delta 0)
Unpacking objects: 100% (5/5), done.
[student@workstation execute-review]$ cd node-hello


[student@workstation node-hello]$ docker build -t node-hello:latest .
Sending build context to Docker daemon 54.27 kB
Step 1/6 : FROM registry.lab.example.com/rhscl/nodejs-6-rhel7
 ---> fba56b5381b7
Step 2/6 : MAINTAINER username "username@example.com"
 ---> Using cache
 ---> e0e9f79e6ae8
Step 3/6 : EXPOSE 3000
 ---> Using cache
 ---> 3a511db75b18
Step 4/6 : COPY . /opt/app-root/src
 ---> aac66bbfbbc3
Removing intermediate container 08503ccd3223
Step 5/6 : RUN source scl_source enable rh-nodejs6 &&     npm install --registry=http://services.lab.example.com:8081/nexus/content/groups/nodejs/
 ---> Running in b5e66e14244d

node-hello@ /opt/app-root/src
+-- body-parser@1.18.3 
| +-- bytes@3.0.0 
| +-- content-type@1.0.4 
| +-- debug@2.6.9 
| | `-- ms@2.0.0 
| +-- depd@1.1.2 
| +-- http-errors@1.6.3 
| | +-- inherits@2.0.3 
| | `-- statuses@1.5.0 
| +-- iconv-lite@0.4.23 
| | `-- safer-buffer@2.1.2 
| +-- on-finished@2.3.0 
| | `-- ee-first@1.1.1 
| +-- qs@6.5.2 
| +-- raw-body@2.3.3 
| | `-- unpipe@1.0.0 
| `-- type-is@1.6.16 
|   +-- media-typer@0.3.0 
|   `-- mime-types@2.1.19 
|     `-- mime-db@1.35.0 
`-- express@4.16.3 
  +-- accepts@1.3.5 
  | `-- negotiator@0.6.1 
  +-- array-flatten@1.1.1 
  +-- body-parser@1.18.2 
  | +-- iconv-lite@0.4.19 
  | `-- raw-body@2.3.2 
  |   `-- http-errors@1.6.2 
  |     +-- depd@1.1.1 
  |     `-- setprototypeof@1.0.3 
  +-- content-disposition@0.5.2 
  +-- cookie@0.3.1 
  +-- cookie-signature@1.0.6 
  +-- encodeurl@1.0.2 
  +-- escape-html@1.0.3 
  +-- etag@1.8.1 
  +-- finalhandler@1.1.1 
  | `-- statuses@1.4.0 
  +-- fresh@0.5.2 
  +-- merge-descriptors@1.0.1 
  +-- methods@1.1.2 
  +-- parseurl@1.3.2 
  +-- path-to-regexp@0.1.7 
  +-- proxy-addr@2.0.4 
  | +-- forwarded@0.1.2 
  | `-- ipaddr.js@1.8.0 
  +-- qs@6.5.1 
  +-- range-parser@1.2.0 
  +-- safe-buffer@5.1.1 
  +-- send@0.16.2 
  | +-- destroy@1.0.4 
  | +-- mime@1.4.1 
  | `-- statuses@1.4.0 
  +-- serve-static@1.13.2 
  +-- setprototypeof@1.1.0 
  +-- statuses@1.4.0 
  +-- utils-merge@1.0.1 
  `-- vary@1.1.2 

 ---> a4c26e513e2f
Removing intermediate container b5e66e14244d
Step 6/6 : CMD /bin/bash -c 'node app.js'
 ---> Running in 5aa83588de46
 ---> 194638dc4731
Removing intermediate container 5aa83588de46
Successfully built 194638dc4731

[student@workstation node-hello]$ docker images
REPOSITORY                                      TAG                 IMAGE ID            CREATED              SIZE
node-hello                                      latest              194638dc4731        23 seconds ago       495 MB
node-hello                                      katest              1813a728f00e        About a minute ago   495 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7   latest              fba56b5381b7        2 years ago          489 MB

[student@workstation node-hello]$ docker tag 194638dc4731 registry.lab.example.com/node-hello:latest
[student@workstation node-hello]$ 
[student@workstation node-hello]$ docker images
REPOSITORY                                      TAG                 IMAGE ID            CREATED              SIZE
node-hello                                      latest              194638dc4731        About a minute ago   495 MB
registry.lab.example.com/node-hello             latest              194638dc4731        About a minute ago   495 MB
node-hello                                      katest              1813a728f00e        2 minutes ago        495 MB
registry.lab.example.com/rhscl/nodejs-6-rhel7   latest              fba56b5381b7        2 years ago          489 MB


[student@workstation node-hello]$ docker push registry.lab.example.com/node-hello
The push refers to a repository [registry.lab.example.com/node-hello]
91883585c387: Pushed 
005f054a70f2: Pushed 
82dfac496b77: Mounted from rhscl/nodejs-6-rhel7 
aa29c7023a3c: Mounted from rhscl/nodejs-6-rhel7 
45f0d85c3257: Mounted from rhscl/nodejs-6-rhel7 
5444fe2e6b50: Mounted from rhscl/nodejs-6-rhel7 
d4d408077555: Mounted from rhscl/nodejs-6-rhel7 
latest: digest: sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f size: 1790
[student@workstation node-hello]$ cd

```


その後、studentディレクトリに移動せよ。

2. OpenShiftにdeveloperでログインし、


```
[student@workstation ~]$ oc new-app --name=node-hello registry.lab.example.com/node-hello
--> Found Docker image 194638d (3 minutes old) from registry.lab.example.com for "registry.lab.example.com/node-hello"

    Node.js 6 
    --------- 
    Node.js 6 available as docker container is a base platform for building and running various Node.js 6 applications and frameworks. Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices.

    Tags: builder, nodejs, nodejs6

    * An image stream will be created as "node-hello:latest" that will track this image
    * This image will be deployed in deployment config "node-hello"
    * Ports 3000/tcp, 8080/tcp will be load balanced by service "node-hello"
      * Other containers can access this service through the hostname "node-hello"

--> Creating resources ...
    imagestream "node-hello" created
    deploymentconfig "node-hello" created
    service "node-hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/node-hello' 
    Run 'oc status' to view your app.

[student@workstation ~]$ oc status
In project execute-review on server https://master.lab.example.com:443

svc/node-hello - 172.30.90.197 ports 3000, 8080
  dc/node-hello deploys istag/node-hello:latest 
    deployment #1 running for about a minute - 0/1 pods


3 infos identified, use 'oc status -v' to see details.


[student@workstation ~]$ oc get all
NAME                           REVISION   DESIRED   CURRENT   TRIGGERED BY
deploymentconfigs/node-hello   1          1         1         config,image(node-hello:latest)

NAME                      DOCKER REPO                                                  TAGS      UPDATED
imagestreams/node-hello   docker-registry.default.svc:5000/execute-review/node-hello   latest    2 minutes ago

NAME                     READY     STATUS             RESTARTS   AGE
po/node-hello-1-deploy   1/1       Running            0          2m
po/node-hello-1-q6r8z    0/1       ImagePullBackOff   0          1m

NAME              DESIRED   CURRENT   READY     AGE
rc/node-hello-1   1         1         0         2m

NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
svc/node-hello   ClusterIP   172.30.90.197   <none>        3000/TCP,8080/TCP   2m




```


3.

```
student@workstation ~]$ oc logs node-hello-1-q6r8z
Error from server (BadRequest): container "node-hello" in pod "node-hello-1-q6r8z" is waiting to start: trying and failing to pull image


[student@workstation ~]$ oc describe pod node-hello-1-q6r8z
Name:           node-hello-1-q6r8z
Namespace:      execute-review
Node:           node2.lab.example.com/172.25.250.12
Start Time:     Wed, 25 Sep 2019 17:48:13 +0900
Labels:         app=node-hello
                deployment=node-hello-1
                deploymentconfig=node-hello
Annotations:    openshift.io/deployment-config.latest-version=1
                openshift.io/deployment-config.name=node-hello
                openshift.io/deployment.name=node-hello-1
                openshift.io/generated-by=OpenShiftNewApp
                openshift.io/scc=restricted
Status:         Pending
IP:             10.129.0.102
Controlled By:  ReplicationController/node-hello-1
Containers:
  node-hello:
    Container ID:   
    Image:          registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f
    Image ID:       
    Ports:          3000/TCP, 8080/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hbslq (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          False 
  PodScheduled   True 
Volumes:
  default-token-hbslq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-hbslq
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type     Reason                 Age              From                            Message
  ----     ------                 ----             ----                            -------
  Normal   Scheduled              5m               default-scheduler               Successfully assigned node-hello-1-q6r8z to node2.lab.example.com
  Normal   SuccessfulMountVolume  5m               kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "default-token-hbslq"
  Normal   Pulling                4m (x2 over 5m)  kubelet, node2.lab.example.com  pulling image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f"
  Warning  Failed                 4m (x2 over 5m)  kubelet, node2.lab.example.com  Failed to pull image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f": rpc error: code = Unknown desc = All endpoints blocked.
  Warning  Failed                 4m (x2 over 5m)  kubelet, node2.lab.example.com  Error: ErrImagePull
  Normal   BackOff                4m (x5 over 5m)  kubelet, node2.lab.example.com  Back-off pulling image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f"
  Normal   SandboxChanged         4m (x7 over 5m)  kubelet, node2.lab.example.com  Pod sandbox changed, it will be killed and re-created.
  Warning  Failed                 4m (x6 over 5m)  kubelet, node2.lab.example.com  Error: ImagePullBackOff


[student@workstation ~]$ oc describe pods node-hello-1-deploy
Name:         node-hello-1-deploy
Namespace:    execute-review
Node:         node2.lab.example.com/172.25.250.12
Start Time:   Wed, 25 Sep 2019 17:48:10 +0900
Labels:       openshift.io/deployer-pod-for.name=node-hello-1
Annotations:  openshift.io/deployment-config.name=node-hello
              openshift.io/deployment.name=node-hello-1
              openshift.io/scc=restricted
Status:       Running
IP:           10.129.0.65
Containers:
  deployment:
    Container ID:   docker://4e35d55c6a2edf0d38ae23177805a225759d84a6ff3c65229a69f5c18821e82e
    Image:          registry.lab.example.com/openshift3/ose-deployer:v3.9.14
    Image ID:       docker-pullable://registry.lab.example.com/openshift3/ose-deployer@sha256:1e8a2599bbafdb154e18b379ee689e54d19e95f7c4beed6ffc9d0b12c2a65621
    Port:           <none>
    State:          Running
      Started:      Wed, 25 Sep 2019 17:48:13 +0900
    Ready:          True
    Restart Count:  0
    Environment:
      KUBERNETES_MASTER:  https://master.lab.example.com
      OPENSHIFT_MASTER:   https://master.lab.example.com
      BEARER_TOKEN_FILE:  /var/run/secrets/kubernetes.io/serviceaccount/token
      OPENSHIFT_CA_DATA:  -----BEGIN CERTIFICATE-----
MIIC6jCCAdKgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtvcGVu
c2hpZnQtc2lnbmVyQDE1NjkzMDE4MzQwHhcNMTkwOTI0MDUxMDMzWhcNMjQwOTIy
MDUxMDM0WjAmMSQwIgYDVQQDDBtvcGVuc2hpZnQtc2lnbmVyQDE1NjkzMDE4MzQw
ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDO/oWLE5WqURg7nx63qWK/
0Z61h40PP3PcZ1cbv7EoP5y3WNRBg5/AXE45bOhUXoW2AuO6GzpPkZxtX9vyu4RB
D3FKOqIc7UVvuF+wFIvZ4nPb7UlJSTpdBuGVcwjPDkYGU//VXGyada3Hb4J+7uz3
aUAVCIr/SM7g3qggvwjS+U0gOqec/jJMfhfDHajmBFlV7DNoBZPZzTJ+jyaXuYyv
eRgienw/Ut+ePJqvdWAVMofI8cWpoPYK6lsoLKU1QTqn2m65MyNTKMhZTmQnZORE
/Ar43fSgsWX5T8jLkWoLb3XMQxOYTj6pn/e84x3vKNUB0HXeuDGcb181Gwx3I9ir
AgMBAAGjIzAhMA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MA0GCSqG
SIb3DQEBCwUAA4IBAQBlAGbbpT2kUViI5+ISHvGpPSIPRgCJcVcaaDSBnYO7yyKi
rcig5GVv6hoqXzfZkQz9UK2f2B90Tb45jK7FcOS3x7C09AUOtp1js7RrR6+1G0T7
jaM1PbtB2iCsKyzRVOXZgwv4y2kzA7deHk+mfjwi4JBv9141OKYcTZIN+WoGtd08
9H6XzMNgvpOvEidHE+hKTZ00lyo806voF0YijTjTH3zFbVJFpgXLUUuCTidNm0ud
WfAZznx/iIBTJbkQpOFMP6DnqgvLqIYIEiPvXlI2G5RptdhBMVJkagf3TaAsG86V
Amx+a5nN7Gr3p7gMswwkUcWcI5/I2n2CxmhjR4bd
-----END CERTIFICATE-----

      OPENSHIFT_DEPLOYMENT_NAME:       node-hello-1
      OPENSHIFT_DEPLOYMENT_NAMESPACE:  execute-review
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from deployer-token-95xmx (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  deployer-token-95xmx:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  deployer-token-95xmx
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/compute=true
Tolerations:     <none>
Events:
  Type    Reason                 Age   From                            Message
  ----    ------                 ----  ----                            -------
  Normal  Scheduled              6m    default-scheduler               Successfully assigned node-hello-1-deploy to node2.lab.example.com
  Normal  SuccessfulMountVolume  6m    kubelet, node2.lab.example.com  MountVolume.SetUp succeeded for volume "deployer-token-95xmx"
  Normal  Pulled                 6m    kubelet, node2.lab.example.com  Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
  Normal  Created                6m    kubelet, node2.lab.example.com  Created container
  Normal  Started                6m    kubelet, node2.lab.example.com  Started container



[student@workstation ~]$ oc get events --sort-by='.metadata.creationTimestamp'
LAST SEEN   FIRST SEEN   COUNT     NAME                                   KIND                    SUBOBJECT                     TYPE      REASON                  SOURCE                           MESSAGE
7m          7m           1         node-hello-1-deploy.15c7a228b849b361   Pod                                                   Normal    Scheduled               default-scheduler                Successfully assigned node-hello-1-deploy to node2.lab.example.com
7m          7m           1         node-hello.15c7a228b5ded3c7            DeploymentConfig                                      Normal    DeploymentCreated       deploymentconfig-controller      Created new replication controller "node-hello-1" for version 1
7m          7m           1         node-hello-1-deploy.15c7a228c9e249fd   Pod                                                   Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "deployer-token-95xmx" 
6m          6m           1         node-hello-1.15c7a229583376fd          ReplicationController                                 Normal    SuccessfulCreate        replication-controller           Created pod: node-hello-1-q6r8z
6m          6m           1         node-hello-1-deploy.15c7a2294990cb9a   Pod                     spec.containers{deployment}   Normal    Created                 kubelet, node2.lab.example.com   Created container
6m          6m           1         node-hello-1-deploy.15c7a2294fb98b01   Pod                     spec.containers{deployment}   Normal    Started                 kubelet, node2.lab.example.com   Started container
6m          6m           1         node-hello-1-q6r8z.15c7a22959012e49    Pod                                                   Normal    Scheduled               default-scheduler                Successfully assigned node-hello-1-q6r8z to node2.lab.example.com
6m          6m           1         node-hello-1-q6r8z.15c7a22964d671ef    Pod                                                   Normal    SuccessfulMountVolume   kubelet, node2.lab.example.com   MountVolume.SetUp succeeded for volume "default-token-hbslq" 
6m          6m           1         node-hello-1-deploy.15c7a22947af99d6   Pod                     spec.containers{deployment}   Normal    Pulled                  kubelet, node2.lab.example.com   Container image "registry.lab.example.com/openshift3/ose-deployer:v3.9.14" already present on machine
6m          6m           2         node-hello-1-q6r8z.15c7a229d810eeaa    Pod                     spec.containers{node-hello}   Warning   Failed                  kubelet, node2.lab.example.com   Failed to pull image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f": rpc error: code = Unknown desc = All endpoints blocked.
6m          6m           2         node-hello-1-q6r8z.15c7a229d8115061    Pod                     spec.containers{node-hello}   Warning   Failed                  kubelet, node2.lab.example.com   Error: ErrImagePull
6m          6m           2         node-hello-1-q6r8z.15c7a229d7f10a27    Pod                     spec.containers{node-hello}   Normal    Pulling                 kubelet, node2.lab.example.com   pulling image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f"
6m          6m           7         node-hello-1-q6r8z.15c7a22a1cf0210b    Pod                                                   Normal    SandboxChanged          kubelet, node2.lab.example.com   Pod sandbox changed, it will be killed and re-created.
6m          6m           5         node-hello-1-q6r8z.15c7a22a822a7198    Pod                     spec.containers{node-hello}   Normal    BackOff                 kubelet, node2.lab.example.com   Back-off pulling image "registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f"
1m          6m           48        node-hello-1-q6r8z.15c7a22a822a8e92    Pod                     spec.containers{node-hello}   Warning   Failed                  kubelet, node2.lab.example.com   Error: ImagePullBackOff


```


4.

```
[student@workstation ~]$ oc get dc node-hello -o yaml
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-09-25T08:48:10Z
  generation: 2
  labels:
    app: node-hello
  name: node-hello
  namespace: execute-review
  resourceVersion: "211882"
  selfLink: /apis/apps.openshift.io/v1/namespaces/execute-review/deploymentconfigs/node-hello
  uid: 3400c6b4-df71-11e9-98ef-52540000fa0a
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    app: node-hello
    deploymentconfig: node-hello
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: node-hello
        deploymentconfig: node-hello
    spec:
      containers:
      - image: registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f
        imagePullPolicy: Always
        name: node-hello
        ports:
        - containerPort: 3000
          protocol: TCP
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - node-hello
      from:
        kind: ImageStreamTag
        name: node-hello:latest
        namespace: execute-review
      lastTriggeredImage: registry.lab.example.com/node-hello@sha256:6ca551c6c74f626715df55ff847425e220d9c1f2c5a3907e7ad0321bde56c02f
    type: ImageChange
status:
  availableReplicas: 0
  conditions:
  - lastTransitionTime: 2019-09-25T08:48:10Z
    lastUpdateTime: 2019-09-25T08:48:10Z
    message: Deployment config does not have minimum availability.
    status: "False"
    type: Available
  - lastTransitionTime: 2019-09-25T08:48:13Z
    lastUpdateTime: 2019-09-25T08:48:13Z
    message: replication controller "node-hello-1" is progressing
    reason: ReplicationControllerUpdated
    status: "True"
    type: Progressing
  details:
    causes:
    - type: ConfigChange
    message: config change
  latestVersion: 1
  observedGeneration: 2
  replicas: 1
  unavailableReplicas: 1
  updatedReplicas: 1

[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS             RESTARTS   AGE       IP             NODE
node-hello-1-deploy   1/1       Running            0          8m        10.129.0.65    node2.lab.example.com
node-hello-1-q6r8z    0/1       ImagePullBackOff   0          8m        10.129.0.102   node2.lab.example.com


[root@node1 ~]# docker pull registry.lab.example.com/node-hello
Using default tag: latest
Trying to pull repository registry.lab.example.com/node-hello ... 
All endpoints blocked.


->アクセスができない？

# Added by the 'lab execute-review setup' script
BLOCK_REGISTRY='--block-registry registry.access.redhat.com --block-registry doc
ker.io --block-registry registry.lab.example.com'

BLOCK_REGISTORYに試験対象のレジストリが入ってる！！
削除してsystemctl restart docker
```

5.

```
[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS             RESTARTS   AGE       IP             NODE
node-hello-1-deploy   1/1       Running            0          8m        10.129.0.65    node2.lab.example.com
node-hello-1-q6r8z    0/1       ImagePullBackOff   0          8m        10.129.0.102   node2.lab.example.com
[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS    RESTARTS   AGE       IP            NODE
node-hello-1-deploy   0/1       Error     0          12m       10.129.0.65   node2.lab.example.com
[student@workstation ~]$ oc rollout latest node-hello
deploymentconfig "node-hello" rolled out
[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS              RESTARTS   AGE       IP             NODE
node-hello-1-deploy   0/1       Error               0          13m       10.129.0.65    node2.lab.example.com
node-hello-2-deploy   1/1       Running             0          3s        10.129.0.114   node2.lab.example.com
node-hello-2-qgfz7    0/1       ContainerCreating   0          1s        <none>         node2.lab.example.com
[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS    RESTARTS   AGE       IP             NODE
node-hello-1-deploy   0/1       Error     0          13m       10.129.0.65    node2.lab.example.com
node-hello-2-qgfz7    1/1       Running   0          5s        10.129.0.115   node2.lab.example.com
[student@workstation ~]$ oc get pods -o wide
NAME                  READY     STATUS    RESTARTS   AGE       IP             NODE
node-hello-1-deploy   0/1       Error     0          13m       10.129.0.65    node2.lab.example.com
node-hello-2-qgfz7    1/1       Running   0          7s        10.129.0.115   node2.lab.example.com


[student@workstation ~]$ oc logs node-hello-2-qgfz7
nodejs server running on http://0.0.0.0:3000

[student@workstation ~]$ oc get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
node-hello   ClusterIP   172.30.90.197   <none>        3000/TCP,8080/TCP   14m
[student@workstation ~]$ oc expose svc/node-hello --hostname=hello.app.lab.example.com
route "node-hello" exposed
[student@workstation ~]$ oc get route
NAME         HOST/PORT                   PATH      SERVICES     PORT       TERMINATION   WILDCARD
node-hello   hello.app.lab.example.com             node-hello   3000-tcp                 None

[student@workstation ~]$ curl http://hello.app.lab.example.com
curl: (6) Could not resolve host: hello.app.lab.example.com; Unknown error
[student@workstation ~]$ curl http://hello.app.lab.example.com
curl: (6) Could not resolve host: hello.app.lab.example.com; Unknown error

うぐぐ。。

[student@workstation ~]$ lab execute-review grade

Grading the student's work for Lab: Executing Commands

 · Check if the hello pod is in Running state..................  PASS
 · Check if Docker image is present............................  PASS
 · Check if Docker image is pushed.............................  PASS
 . Checking if docker configuration on node1 is fixed..........  PASS
 . Checking if docker configuration on node2 is fixed..........  PASS
 . Checking if route can be invoked successfully...............  FAIL

Overall exercise grade.........................................  FAIL


```



---------------------------------------------------------

# Chapter5 CONTROLLING ACCESS TO OPENSHIFT RESOURES(P145)

ProjectはK8Sでいうところのnamespace
-n オプションでnamespaceをつけている。
Projectは全ての中でユニークになる必要がある。
文字数は63文字以内。

Project に紐づけられるのは、リソース。

## Cluster Administration

デフォルトでユーザはProjectを作成することができるが、コマンドで剥奪することが可能。

```
oc adm policy removecluster-role-from-group self-provisioner system:authenticated system:authenticated:oauth
```

addも可能。

現状の各ユーザに関する権限の確認は、oc describe clusterPolicyBindingsとかでできる。
Policyは2種類。ClusterPolicy、ProjectPolicy
通常、ProjectにはPolicyは入っておらず、ClusterPolicyのみ。

```
oc describe clusterPolicyBindings :default
```

各種の操作したい内容に対してのコマンドの紹介(P148-149)

Roleには通常のrole権とcluster-role権がある。
ちがいは？

exampleProjectでdeveloperユーザは管理権(admin role)を持つ。

```
oc adm policy add-role-to-user admin developer -n example
```

GUIでは、Resources -> Membership -> Edit membership -> Roleで操作可能。
basic-userはProjectの操作はできず、中身を見るだけ。
ただ、ここでやれるのはProjectのRole権。Clusterのものは操作はできない。

### Security Context Constraints(SCCs)

```
Project                 | <-- user  <----  認証システム(file(htpasswd)/LDAP/Git etc)
                        |    (role: create/delete)
 ---Pod---------------- |
   Container---------|  |
    (random userID)  |  |
   ------------------|  |
    ^                   |
    dc  <--serviceaccount |
 ---------------^-----------
                |
              scc セキュリティコンテキスト(セキュリティ設定のリソース)

  特別な要件があった場合にだけこの仕組みを使う。

```
認証システム側にユーザを作らないと、ログインはできない。ユーザオブジェクトを作っただけ。
今回はファイルベースのhtpasswdでやってる。
OpenShiftはrootless。コンテナ内のプロセスをrootで実行しないようになっている。
通常は問題ないが、コンテナそのものにバグがあった時、root権で動作させていた場合には、
コンテナ外のNodeなどを操作できてしまう可能性もある。

また、コンテナによってはroot権でないとうまく動作しないコンテナがあったりする。
メーカーのサポートがそうしないと受けられないとか。
この場合にはセキュリティレベルを下げる必要がある。
その対応をするのがSCCs。

SCCs 設定はadmin権が必要。なので、masterノードで実施する。
```
[root@master ~]# oc get scc
NAME               PRIV      CAPS      SELINUX     RUNASUSER          FSGROUP     SUPGROUP    PRIORITY   READONLYROOTFS   VOLUMES
anyuid             false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    10         false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
hostaccess         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret]
hostmount-anyuid   false     []        MustRunAs   RunAsAny           RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret]
hostnetwork        false     []        MustRunAs   MustRunAsRange     MustRunAs   MustRunAs   <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
nonroot            false     []        MustRunAs   MustRunAsNonRoot   RunAsAny    RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]
privileged         true      [*]       RunAsAny    RunAsAny           RunAsAny    RunAsAny    <none>     false            [*]
restricted         false     []        MustRunAs   MustRunAsRange     MustRunAs   RunAsAny    <none>     false            [configMap downwardAPI emptyDir persistentVolumeClaim projected secret]

```
デフォルトなので、自分で増やすこともできる。
デフォルトの権限はrestricted。一番強い。
一番よわいのは、privileged。

```
[root@master ~]# oc describe scc
Name:						anyuid
Priority:					10
Access:						
  Users:					<none>
  Groups:					system:cluster-admins
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			MKNOD
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				false
  Allow Host Ports:				false
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: RunAsAny		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: RunAsAny			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


Name:						hostaccess
Priority:					<none>
Access:						
  Users:					<none>
  Groups:					<none>
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,hostPath,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				true
  Allow Host Ports:				true
  Allow Host PID:				true
  Allow Host IPC:				true
  Read Only Root Filesystem:			false
  Run As User Strategy: MustRunAsRange		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: MustRunAs			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


Name:						hostmount-anyuid
Priority:					<none>
Access:						
  Users:					system:serviceaccount:openshift-infra:pv-recycler-controller,system:serviceaccount:kube-service-catalog:service-catalog-apiserver
  Groups:					<none>
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			MKNOD
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,hostPath,nfs,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				false
  Allow Host Ports:				false
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: RunAsAny		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: RunAsAny			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


Name:						hostnetwork
Priority:					<none>
Access:						
  Users:					system:serviceaccount:default:router,system:serviceaccount:default:registry
  Groups:					<none>
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				true
  Allow Host Ports:				true
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: MustRunAsRange		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: MustRunAs			
    Ranges:					<none>
  Supplemental Groups Strategy: MustRunAs	
    Ranges:					<none>


Name:						nonroot
Priority:					<none>
Access:						
  Users:					<none>
  Groups:					<none>
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				false
  Allow Host Ports:				false
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: MustRunAsNonRoot	
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: RunAsAny			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


Name:						privileged
Priority:					<none>
Access:						
  Users:					system:admin,system:serviceaccount:openshift-infra:build-controller,system:serviceaccount:management-infra:management-admin,system:serviceaccount:management-infra:inspector-admin
  Groups:					system:cluster-admins,system:nodes,system:masters
Settings:					
  Allow Privileged:				true
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			<none>
  Allowed Capabilities:				*
  Allowed Seccomp Profiles:			*
  Allowed Volume Types:				*
  Allowed Flexvolumes:				<all>
  Allow Host Network:				true
  Allow Host Ports:				true
  Allow Host PID:				true
  Allow Host IPC:				true
  Read Only Root Filesystem:			false
  Run As User Strategy: RunAsAny		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: RunAsAny		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: RunAsAny			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


Name:						restricted
Priority:					<none>
Access:						
  Users:					<none>
  Groups:					system:authenticated
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				false
  Allow Host Ports:				false
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: MustRunAsRange		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: MustRunAs			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>
[root@master ~]# ^C

```
ふむ
```
[root@master ~]# oc describe scc privilege
Name:						privileged
Priority:					<none>
Access:						
  Users:					system:admin,system:serviceaccount:openshift-infra:build-controller,system:serviceaccount:management-infra:management-admin,system:serviceaccount:management-infra:inspector-admin
  Groups:					system:cluster-admins,system:nodes,system:masters
Settings:					
  Allow Privileged:				true
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			<none>
  Allowed Capabilities:				*
  Allowed Seccomp Profiles:			*
  Allowed Volume Types:				*
  Allowed Flexvolumes:				<all>
  Allow Host Network:				true
  Allow Host Ports:				true
  Allow Host PID:				true
  Allow Host IPC:				true
  Read Only Root Filesystem:			false
  Run As User Strategy: RunAsAny		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: RunAsAny		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: RunAsAny			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>
```
ふむ
```
[root@master ~]# oc describe scc restricted 
Name:						restricted
Priority:					<none>
Access:						
  Users:					<none>
  Groups:					system:authenticated
Settings:					
  Allow Privileged:				false
  Default Add Capabilities:			<none>
  Required Drop Capabilities:			KILL,MKNOD,SETUID,SETGID
  Allowed Capabilities:				<none>
  Allowed Seccomp Profiles:			<none>
  Allowed Volume Types:				configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
  Allowed Flexvolumes:				<all>
  Allow Host Network:				false
  Allow Host Ports:				false
  Allow Host PID:				false
  Allow Host IPC:				false
  Read Only Root Filesystem:			false
  Run As User Strategy: MustRunAsRange		
    UID:					<none>
    UID Range Min:				<none>
    UID Range Max:				<none>
  SELinux Context Strategy: MustRunAs		
    User:					<none>
    Role:					<none>
    Type:					<none>
    Level:					<none>
  FSGroup Strategy: MustRunAs			
    Ranges:					<none>
  Supplemental Groups Strategy: RunAsAny	
    Ranges:					<none>


```

各ユーザにscc権限を付ける場合もadmin 権限がないとだめ。
ただ、ユーザーにつけるとそのユーザにいろいろなことができてしまうため、推奨しない。
特定のコンテナに対してだけ権限を付ける、とかの場合にはservice accountをつかう。
特定のコンテナにservice accountを紐づけ、そのservice accountにsccの権限を付ける。
service account はProject内リソースなので、admin は不要。
service accountは実行用。
user accountはProject管理用。全く用途が異なる。

oc patchコマンドは、1コマンドで設定ファイルを上書きするコマンド。
oc editでviでいじってもいいが、ワンライナーで設定することも可能。自動化とかやるときにつかう。

-zオプションが注意。これがservice accountに紐づける、という宣言。
-z がないと、通常のユーザーに付与しようとして、そんなユーザいないって言われて怒られる。

GUIで操作できるのは自分のProjectの中だけ。
sccの付与についてはcluster admin権が必要。

```

```
OpenShiftでは、ユーザとグループで権限を管理。
今回はユーザしか使ってない。
ログイン成功すると、X.509のセッショントークンが発行されて、24時間の有効期限が付与される。

認証のタイプ
- HTPasswdIdentityProvider
- Basic Authentication
- Request Header Authentication
- Keystone Authentication
- LDAP Authentication
- GitHub Authentication

OCPのバージョンによって、使えるものが増減したり。
あとは、認証方法によってパラメータが違ったりする。

----------------------------------------

## Guide Exercise: Managing Projects and Accounts(154)

- pre
```
[student@workstation secure-route]$ lab secure-resources setup

Checking prerequisites for GE: Managing projects and accounts

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for GE: Managing projects and accounts

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS

```
1. user1をProject adminに、user2をproject developerにしてみよう。Passwordはredhat

1.1 MasterVMにrootでログインしよう。
```
[root@master ~]# hostname
master.lab.example.com
[root@master ~]# whoami
root
[root@master ~]# oc whoami
system:admin
[root@master ~]#
```

1.2 user1を作成しよう。(htpasswd側：認証システム側）

```
[root@master ~]# htpasswd -b /etc/origin/master/htpasswd user1 redhat
Adding password for user user1
[root@master ~]# 

```

1.3 user2を作成しよう。(htpasswd側：認証システム側）

```
[root@master ~]# htpasswd -b /etc/origin/master/htpasswd user2 redhat
Adding password for user user2


```

1.4 Master VMからログアウトしよう。

```
[root@master ~]# exit
logout
Connection to master closed.
[student@workstation ~]$ 

```


2. 通常ユーザのプロジェクト作成権限を削除しよう。

2.1 workstationからadminユーザでOpenShiftへログインしよう。

```
[student@workstation secure-route]$ oc whoami
developer
[student@workstation secure-route]$ 
[student@workstation secure-route]$ oc logout
Logged "developer" out on "https://master.lab.example.com:443"
[student@workstation secure-route]$ 
[student@workstation secure-route]$ 
[student@workstation secure-route]$ oc login -u admin -p redhat https://master.lab.example.com
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation secure-route]$ oc whoami
admin
[student@workstation secure-route]$ 


```

2.2 全てのユーザから、プロジェクト作成権限を削除しよう。

```
[student@workstation secure-route]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authentication:oauth
error: unable to find target [system:authentication:oauth]
```
スクリプト動かしてからやれって書いてあるな。。

```
[student@workstation secure-route]$ ls /home/student/DO280/labs/secure-resources/
configure-policy.sh  configure-sc.sh  restore-policy.sh
[student@workstation secure-route]$ cat /home/student/DO280/labs/secure-resources/configure-policy.sh 
#!/bin/bash

oc adm policy remove-cluster-role-from-group \
    self-provisioner system:authenticated system:authenticated:oauth


ふむ

[student@workstation secure-route]$ /home/student/DO280/labs/secure-resources/configure-policy.sh 
cluster role "self-provisioner" removed: ["system:authenticated" "system:authenticated:oauth"]

できたかな？サイド実行。

[student@workstation secure-route]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authentication:oauth
error: unable to find target [system:authentication:oauth]

あー、スクリプトで実行しろ、ってことか。何が違うねん。
。。。教材にかいてあるやつ、オプションたりんじゃん。

```

3. 通常ユーザがプロジェクト作れないことを確認しよう。

3.1 user1でログインしよう。

```
[student@workstation secure-route]$ oc login -u user1 -p redhat https://master.lab.example.com
Login successful.

You don't have any projects. Contact your system administrator to request a project.

http://にしてたら変なエラーが出たわ。
[student@workstation secure-route]$ oc whoami
Error from server (Forbidden): users.user.openshift.io "~" is forbidden: User "system:anonymous" cannot get users.user.openshift.io at the cluster scope: User "system:anonymous" cannot get users.user.openshift.io at the cluster scope


```

3.2 projectをつくてみよう。
```
[student@workstation secure-route]$ oc whoami
user1
[student@workstation secure-route]$ oc new-project test
Error from server (Forbidden): You may not request a new project via this API.

```

4. ClusterAdminで2つProjectをつくってみよう？

4.1 adminユーザでOCPへログインしよう。

```
[student@workstation secure-route]$ oc logout
Logged "user1" out on "https://master.lab.example.com:443"
[student@workstation secure-route]$ 
[student@workstation secure-route]$ oc login -u admin -p redhat https://master.lab.example.comLogin successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation secure-route]$ oc whoami
admin
[student@workstation secure-route]$ 

```

4.2 Projectをつくろう その1

```
[student@workstation secure-route]$ oc new-project project-user1
Now using project "project-user1" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.


```

4.3 Projectをつくろう その2

```
[student@workstation secure-route]$ oc new-project project-user2
Now using project "project-user2" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

```

5. user1をproject-user1に、user2を両方に参加させよう。

5.1 user1をproject-user1に参加させよう

```
[student@workstation secure-route]$ oc project project-user1
Now using project "project-user1" on server "https://master.lab.example.com:443".
[student@workstation secure-route]$ 
[student@workstation secure-route]$ oc policy add-role-to-user admin user1
role "admin" added: "user1"

```

5.2 user1をproject-user1に参加させよう。(権限がadminじゃなくeditだった）

```
[student@workstation secure-route]$ oc policy add-role-to-user edit user2
role "edit" added: "user2"


```

5.3 user2をproject-user2に参加させよう。

```
[student@workstation secure-route]$ oc project project-user2
Now using project "project-user2" on server "https://master.lab.example.com:443".
[student@workstation secure-route]$ 
[student@workstation secure-route]$ oc policy add-role-to-user edit user2
role "edit" added: "user2"
```

6. 各アカウントのTestをしよう。


6.1 user1がproject-user1にしかアクセスできないことを確認しよう。

```
[student@workstation secure-route]$ oc login -u user1 -p redhat https://master.lab.example.com
Login successful.

You have one project on this server: "project-user1"

Using project "project-user1".

```

6.2 project-user1に関するコマンドが動くことを確認しよう。

```
[student@workstation secure-route]$ oc project project-user1
Already on project "project-user1" on server "https://master.lab.example.com:443".
```

6.3 project-user2に入れるか確認してみよう。

```
[student@workstation secure-route]$ oc project project-user2
error: You are not a member of project "project-user2".
You have one project on this server: project-user1

```

6.4 user2の確認をしよう。先ずはログイン。

```
[student@workstation secure-route]$ oc login -u user2 -p redhat https://master.lab.example.com
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * project-user1
    project-user2

Using project "project-user1".

```

6.5 project-user1の確認

```
[student@workstation secure-route]$ oc project project-user1
Already on project "project-user1" on server "https://master.lab.example.com:443".

```

6.6 project-user2の確認

```
[student@workstation secure-route]$ oc project project-user2
Now using project "project-user2" on server "https://master.lab.example.com:443".
```

7. 

7.1 

```
[student@workstation secure-route]$ oc project project-user1
Now using project "project-user1" on server "https://master.lab.example.com:443".
[student@workstation secure-route]$ oc whoami
user2

```

7.2 

```
[student@workstation secure-route]$ oc new-app --name=nginx --docker-image=registry.lab.example.com/nginx:latest
--> Found Docker image c825216 (14 months old) from registry.lab.example.com for "registry.lab.example.com/nginx:latest"

    * An image stream will be created as "nginx:latest" that will track this image
    * This image will be deployed in deployment config "nginx"
    * Port 80/tcp will be load balanced by service "nginx"
      * Other containers can access this service through the hostname "nginx"
    * WARNING: Image "registry.lab.example.com/nginx:latest" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    imagestream "nginx" created
    deploymentconfig "nginx" created
    service "nginx" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/nginx' 
    Run 'oc status' to view your app.

```

7.3 Verify

```
student@workstation secure-route]$ oc get pods
NAME             READY     STATUS             RESTARTS   AGE
nginx-1-deploy   1/1       Running            0          39s
nginx-1-r7zth    0/1       CrashLoopBackOff   2          36s
```

 8.

8.1 

```
[student@workstation secure-route]$ oc login -u user1 -p redhat
Login successful.

You have one project on this server: "project-user1"

Using project "project-user1".

```

8.2
```
[student@workstation secure-route]$ oc create serviceaccount userroot
serviceaccount "userroot" created

```

8.3 

```
[student@workstation secure-route]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * project-user1
    project-user2

Using project "project-user1".

```

8.4

```
[student@workstation secure-route]$ oc adm policy add-scc-to-user anyuid -z userroot
scc "anyuid" added to: ["system:serviceaccount:project-user1:userroot"]
[student@workstation secure-route]$ 

```

8.5

```
[student@workstation secure-route]$ oc login -u user2 -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * project-user1
    project-user2

Using project "project-user1".
[student@workstation secure-route]$ 

```

8.6

```
[student@workstation secure-route]$ /home/student/DO280/labs/secure-resources/configure-sc.sh
Login successful.

You have one project on this server: "project-user1"

Using project "project-user1".
error: You are not a member of project "proj-user1".
You have one project on this server: project-user1
serviceaccount "useroot" created
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * project-user1
    project-user2

Using project "project-user1".
error: A project named "proj-user1" does not exist on "https://master.lab.example.com:443".
Your projects are:
* default
* hoge (hogehoge)
* kube-public
* kube-service-catalog
* kube-system
* logging
* management-infra
* openshift
* openshift-ansible-service-broker
* openshift-infra
* openshift-node
* openshift-template-service-broker
* openshift-web-console
* project-user1
* project-user2
scc "anyuid" added to: ["system:serviceaccount:project-user1:useroot"]
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * project-user1
    project-user2

Using project "project-user1".
error: You are not a member of project "proj-user1".
Your projects are:
* project-user1
* project-user2
deploymentconfig "nginx" patched
NAME             READY     STATUS              RESTARTS   AGE
nginx-1-r7zth    0/1       Terminating         5          3m
nginx-2-ccn5j    0/1       ContainerCreating   0          1s
nginx-2-deploy   1/1       Running             0          3s

```

8.7

```
[student@workstation secure-route]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
nginx-2-ccn5j   1/1       Running   0          16s

```

9.

9.1

```
[student@workstation secure-route]$ oc expose svc nginx
route "nginx" exposed

```


9.2
```
[student@workstation secure-route]$ curl -s http://nginx-project-user1.apps.lab.example.com
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


```


10. Cleanup


```
[student@workstation secure-route]$ /home/student/DO280/labs/secure-resources/restore-policy.sh 
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * project-user1
    project-user2

Using project "project-user1".
cluster role "self-provisioner" added: ["system:authenticated" "system:authenticated:oauth"]
[student@workstation secure-route]$ 

```


```
[student@workstation secure-route]$ oc delete project project-user1
project "project-user1" deleted
[student@workstation secure-route]$ oc delete project project-user
Error from server (NotFound): namespaces "project-user" not found
[student@workstation secure-route]$ oc delete project project-user2
project "project-user2" deleted

```

```
[root@master ~]# htpasswd -D /etc/origin/master/htpasswd user1
Deleting password for user user1
[root@master ~]# htpasswd -D /etc/origin/master/htpasswd user2
Deleting password for user user2
[root@master ~]# exit
logout

```


## Managing Sensitive Information with Secrets(P162)

### Secretリソース
中身はKey=value形式。

この内容を、コンテナに対してアタッチできる。
File or ENV

Secretはサービスアカウントやdcに紐付けることができる。

マウントする(--for=mount)際に、サービスアカウントとSecretを紐づけられる。

GUIでも作成が可能。
Resources > Secrets
Key-value形式の場合は、GUIでは作れないのでCLIで作成する必要がある。

このあと説明する、ConfigMapはKEYーVALUEでGUIで作成可能。
書いただけではダメで、DC（ApplicationのENV）に紐づける必要がある。
関連付けしてSAVEして初めて使われる。

ターミナルで入ってget (ENV)すれば、環境変数に設定されていることが確認できる。

用途
- User Password
- TLS and Key Pairs


### ConfigMap

ConfigMapはdcに紐づける。その結果、環境変数としてコンテナに反映される。

configmapは値は直接確認できる。暗号化されているわけではない。
oc get configmaps special-config -o yaml

KeyValue形式でコンテナに対して環境変数を差し込む方法がConfigMap。
環境変数に個別で定義してもいい。使い回しをどこまでやるか、という程度。
1回だけで済むなら直接ENVにかいてもいい。

--------------------------------------------------------------------

演習の前に、先にP170を紹介。

Cluster関係のロール

つうじょうのろーる 


User Typs

- Regilar users ログインユーザ
- System users  内部コンポーネントが使うユーザ(system:adminなど)
- Service accounts SCCsなどで使う、コンテナを実行するための特殊なアカウント 一部は自動で作成されている。


SCCの話。

SELINUXの話


## Guided Exercise: Oritected a Database Password(P166)

- pre

```
[student@workstation secure-route]$ lab secure-secrets setup 

Checking prerequisites for GE: Protecting a Database Password

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for GE: Protecting a Database Password

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS

```

```
[student@workstation secure-route]$ oc login -u developer -p redhat https://master.lab.example.com
Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".
```

```
[student@workstation secure-route]$ oc new-project secure-secrets
Now using project "secure-secrets" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation secure-route]$ 

```

```
[student@workstation secure-route]$ cd ~/DO280/labs/secure-secrets/
[student@workstation secure-secrets]$ less mysql-ephemeral.yml 
[student@workstation secure-secrets]$ cat mysql-ephemeral.yml 
apiVersion: v1
kind: Template
labels:
  template: mysql-ephemeral-template
message: |-
  The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.
   Connection URL: mysql://${DATABASE_SERVICE_NAME}:3306/

  For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
metadata:
  annotations:
    description: |-
      MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.

      WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing
    iconClass: icon-mysql-database
    openshift.io/display-name: MySQL (Ephemeral)
    tags: database,mysql
    template.openshift.io/documentation-url: https://docs.openshift.org/latest/using_images/db_images/mysql.html
    template.openshift.io/long-description: This template provides a standalone MySQL
      server with a database created.  The database is not stored on persistent storage,
      so any restart of the service will result in all data being lost.  The database
      name, username, and password are chosen via parameters when provisioning this
      service.
    template.openshift.io/provider-display-name: Red Hat, Inc.
    template.openshift.io/support-url: https://access.redhat.com
  creationTimestamp: null
  name: mysql-ephemeral
objects:
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: null
    name: ${DATABASE_SERVICE_NAME}
  spec:
    ports:
    - name: mysql
      nodePort: 0
      port: 3306
      protocol: TCP
      targetPort: 3306
    selector:
      name: ${DATABASE_SERVICE_NAME}
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    creationTimestamp: null
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - capabilities: {}
          env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-root-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_DATABASE
            value: ${MYSQL_DATABASE}
          image: ' '
          imagePullPolicy: IfNotPresent
          livenessProbe:
            initialDelaySeconds: 30
            tcpSocket:
              port: 3306
            timeoutSeconds: 1
          name: mysql
          ports:
          - containerPort: 3306
            protocol: TCP
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
          securityContext:
            capabilities: {}
            privileged: false
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: ${DATABASE_SERVICE_NAME}-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        volumes:
        - emptyDir:
            medium: ""
          name: ${DATABASE_SERVICE_NAME}-data
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - mysql
        from:
          kind: ImageStreamTag
          name: mysql:${MYSQL_VERSION}
          namespace: ${NAMESPACE}
        lastTriggeredImage: ""
      type: ImageChange
    - type: ConfigChange
  status: {}
parameters:
- description: Maximum amount of memory the container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 512Mi
- description: The OpenShift Namespace where the ImageStream resides.
  displayName: Namespace
  name: NAMESPACE
  value: openshift
- description: The name of the OpenShift Service exposed for the database.
  displayName: Database Service Name
  name: DATABASE_SERVICE_NAME
  required: true
  value: mysql
- description: Name of the MySQL database accessed.
  displayName: MySQL Database Name
  name: MYSQL_DATABASE
  required: true
  value: sampledb
- description: Version of MySQL image to be used (5.5, 5.6, 5.7, or latest).
  displayName: Version of MySQL Image
  name: MYSQL_VERSION
  required: true
  value: "5.7"

```

      spec:
        containers:
        - capabilities: {}
          env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${DATABASE_SERVICE_NAME}

- description: The name of the OpenShift Service exposed for the database.
  displayName: Database Service Name
  name: DATABASE_SERVICE_NAME
  required: true
  value: mysql

これが必須パラメータということがわかる。

2. 
```
[student@workstation secure-secrets]$ oc create secret generic mysql \
> --from-literal='database-user'='mysql' \
> --from-literal='database-password'='redhat' \
> --from-literal='database-root-password'='do280-admin'
secret "mysql" created
[student@workstation sec

[student@workstation secure-secrets]$ oc get secret mysql -o yaml
apiVersion: v1
data:
  database-password: cmVkaGF0
  database-root-password: ZG8yODAtYWRtaW4=
  database-user: bXlzcWw=
kind: Secret
metadata:
  creationTimestamp: 2019-09-25T06:42:12Z
  name: mysql
  namespace: secure-secrets
  resourceVersion: "195137"
  selfLink: /api/v1/namespaces/secure-secrets/secrets/mysql
  uid: 9af76ad2-df5f-11e9-98ef-52540000fa0a
type: Opaque
[student@workstation secure-secrets]$ 


```

3.

```
[student@workstation secure-secrets]$ oc new-app --file=mysql-ephemeral.yml 
--> Deploying template "secure-secrets/mysql-ephemeral" for "mysql-ephemeral.yml" to project secure-secrets

     MySQL (Ephemeral)
     ---------
     MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
     
     WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing

     The following service(s) have been created in your project: mysql.
      Connection URL: mysql://mysql:3306/
     
     For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.

     * With parameters:
        * Memory Limit=512Mi
        * Namespace=openshift
        * Database Service Name=mysql
        * MySQL Database Name=sampledb
        * Version of MySQL Image=5.7

--> Creating resources ...
    service "mysql" created
    deploymentconfig "mysql" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mysql' 
    Run 'oc status' to view your app.
[student@workstation secure-secrets]$ 
```

4.
```
[student@workstation secure-secrets]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
mysql-1-zdcgk   1/1       Running   0          19s

```

5.

```
[student@workstation ~]$ oc port-forward mysql-1-zdcgk 3306:3306
Forwarding from 127.0.0.1:3306 -> 3306

```

6.

```
[student@workstation ~]$ mysql -uroot -pdo280-admin -h127.0.0.1
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 12
Server version: 5.7.16 MySQL Community Server (GPL)

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sampledb           |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

[student@workstation ~]$ oc port-forward mysql-1-zdcgk 3306:3306
Forwarding from 127.0.0.1:3306 -> 3306

Handling connection for 3306



```

8. Cleanup

```
[student@workstation ~]$ oc delete project secure-secrets
project "secure-secrets" deleted

```

## Lab: Controlling Access to OpenShift Resources (P177)


- pre

```
student@workstation ~]$ lab secure-review setup

Checking prerequisites for Controlling Access to OpenShift Resources

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for Controlling Access to OpenShift Resources

 · Download exercise files.....................................  SUCCESS

Overall setup status...........................................  SUCCESS

```

1.
```
[root@master ~]# more /etc/origin/master/htpasswd 
admin:$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611
developer:$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611
[root@master ~]# htpasswd -b /etc/origin/master/htpasswd user-review redhat
Adding password for user user-review
[root@master ~]# more /etc/origin/master/htpasswd 
admin:$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611
developer:$apr1$4ZbKL26l$3eKL/6AQM8O94lRwTAu611
user-review:$apr1$isKEKcca$KXXuhQmJ0nnJtVYmH9NJk0
[root@master ~]# exit

```

2.

```
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation ~]$ oc whoami
admin
[student@workstation ~]$ oc adm policy remove-
remove-cluster-role-from-group  remove-role-from-group          remove-scc-from-user
remove-cluster-role-from-user   remove-role-from-user           remove-user
remove-group                    remove-scc-from-group           
[student@workstation ~]$ oc adm policy remove-role-from-group self-provisioner 
.ansible/       .cache/         do280-ansible/  .kube/          .mysql_history  .ssh/
.bash_history   .config/        Documents/      .local/         Pictures/       Templates/
.bash_logout    .dbus/          Downloads/      .m2/            .pki/           Videos/
.bash_profile   Desktop/        .esd_auth       .mozilla/       Public/         .viminfo
.bashrc         DO280/          .ICEauthority   Music/          .rnd            
[student@workstation ~]$ oc adm policy remove-role-from-group self-provisioner system:authenticated system:authenticated:oauth
error: unable to locate RoleBinding for /self-provisioner
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated system:authenticated:oauth
cluster role "self-provisioner" removed: ["system:authenticated" "system:authenticated:oauth"]

```


3.

```
[student@workstation ~]$ oc login -u user-review -p redhat
Login successful.

You don't have any projects. Contact your system administrator to request a project.
[student@workstation ~]$ oc whoami
user-review
[student@workstation ~]$ oc new-project hoge
Error from server (Forbidden): You may not request a new project via this API.
[student@workstation ~]$ 

```

4.

```
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console

Using project "default".
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc new-project secure-review
Now using project "secure-review" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
[student@workstation ~]$ 

```

5.

```
[student@workstation ~]$ oc project secure-review
Already on project "secure-review" on server "https://master.lab.example.com:443".
[student@workstation ~]$ 
[student@workstation ~]$ 
[student@workstation ~]$ oc adm policy add-role-to-user developer user-review
role "developer" added: "user-review"

[student@workstation ~]$ oc login -u user-review -p redhat
Login successful.

You don't have any projects. Contact your system administrator to request a project.
[student@workstation ~]$ oc project secure-review
error: You are not a member of project "secure-review".
You are not a member of any projects. You can request a project to be created with the 'new-project' command.


----------------------------------


[student@workstation ~]$ oc policy remove-role-from-user developer user-review
role "developer" removed: "user-review"
[student@workstation ~]$ 
[student@workstation ~]$ oc policy add-role-to-user edit user-review
role "edit" added: "user-review"
[student@workstation ~]$ oc login -u user-review -p redhat
Login successful.

You have one project on this server: "secure-review"

Using project "secure-review".
[student@workstation ~]$ oc project secure-review
Already on project "secure-review" on server "https://master.lab.example.com:443".
[student@workstation ~]$ 


```

6. 
```
[student@workstation ~]$ cd ~/DO280/labs/secure-review/
[student@workstation secure-review]$ ls
mysql-ephemeral.yml  patch-dc.sh
[student@workstation secure-review]$ cat mysql-ephemeral.yml 
apiVersion: v1
kind: Template
labels:
  template: mysql-ephemeral-template
message: |-
  The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.
   Connection URL: mysql://${DATABASE_SERVICE_NAME}:3306/

  For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
metadata:
  annotations:
    description: |-
      MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.

      WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing
    iconClass: icon-mysql-database
    openshift.io/display-name: MySQL (Ephemeral)
    tags: database,mysql
    template.openshift.io/documentation-url: https://docs.openshift.org/latest/using_images/db_images/mysql.html
    template.openshift.io/long-description: This template provides a standalone MySQL
      server with a database created.  The database is not stored on persistent storage,
      so any restart of the service will result in all data being lost.  The database
      name, username, and password are chosen via parameters when provisioning this
      service.
    template.openshift.io/provider-display-name: Red Hat, Inc.
    template.openshift.io/support-url: https://access.redhat.com
  creationTimestamp: null
  name: mysql-ephemeral
objects:
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: null
    name: ${DATABASE_SERVICE_NAME}
  spec:
    ports:
    - name: mysql
      nodePort: 0
      port: 3306
      protocol: TCP
      targetPort: 3306
    selector:
      name: ${DATABASE_SERVICE_NAME}
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    creationTimestamp: null
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - capabilities: {}
          env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-root-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_DATABASE
            value: ${MYSQL_DATABASE}
          image: ' '
          imagePullPolicy: IfNotPresent
          livenessProbe:
            initialDelaySeconds: 30
            tcpSocket:
              port: 3306
            timeoutSeconds: 1
          name: mysql
          ports:
          - containerPort: 3306
            protocol: TCP
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
          securityContext:
            capabilities: {}
            privileged: false
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: ${DATABASE_SERVICE_NAME}-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        volumes:
        - emptyDir:
            medium: ""
          name: ${DATABASE_SERVICE_NAME}-data
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - mysql
        from:
          kind: ImageStreamTag
          name: mysql:${MYSQL_VERSION}
          namespace: ${NAMESPACE}
        lastTriggeredImage: ""
      type: ImageChange
    - type: ConfigChange
  status: {}
parameters:
- description: Maximum amount of memory the container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 512Mi
- description: The OpenShift Namespace where the ImageStream resides.
  displayName: Namespace
  name: NAMESPACE
  value: openshift
- description: The name of the OpenShift Service exposed for the database.
  displayName: Database Service Name
  name: DATABASE_SERVICE_NAME
  required: true
  value: mysql
- description: Name of the MySQL database accessed.
  displayName: MySQL Database Name
  name: MYSQL_DATABASE
  required: true
  value: sampledb
- description: Version of MySQL image to be used (5.5, 5.6, 5.7, or latest).
  displayName: Version of MySQL Image
  name: MYSQL_VERSION
  required: true
  value: "5.7"


やっぱりここか？

     spec:
        containers:
        - capabilities: {}
          env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-root-password
                name: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_DATABASE
            value: ${MYSQL_DATABASE}

keyは3種類。
                key: database-user
                key: database-password
                key: database-root-password
```


7.

```
[student@workstation secure-review]$ oc create secret generic mysql --from-literal='database-user'='mysql' --from-literal='database-password'='redhat' --from-literal='database-root-password'='do280-admin'
secret "mysql" createdoc

[student@workstation secure-review]$ oc get secret
NAME                       TYPE                                  DATA      AGE
builder-dockercfg-hqnml    kubernetes.io/dockercfg               1         1h
builder-token-8c4tv        kubernetes.io/service-account-token   4         1h
builder-token-mm92n        kubernetes.io/service-account-token   4         1h
default-dockercfg-rx87r    kubernetes.io/dockercfg               1         1h
default-token-6x6ql        kubernetes.io/service-account-token   4         1h
default-token-mfwqp        kubernetes.io/service-account-token   4         1h
deployer-dockercfg-hpptb   kubernetes.io/dockercfg               1         1h
deployer-token-4qg7w       kubernetes.io/service-account-token   4         1h
deployer-token-k5q72       kubernetes.io/service-account-token   4         1h
mysql                      Opaque                                3         27s

```
8.

```
[student@workstation secure-review]$ oc new-app -f mysql-ephemeral.yml 
--> Deploying template "secure-review/mysql-ephemeral" for "mysql-ephemeral.yml" to project secure-review

     MySQL (Ephemeral)
     ---------
     MySQL database service, without persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.
     
     WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing

     The following service(s) have been created in your project: mysql.
      Connection URL: mysql://mysql:3306/
     
     For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/mysql-container/blob/master/5.7/README.md.

     * With parameters:
        * Memory Limit=512Mi
        * Namespace=openshift
        * Database Service Name=mysql
        * MySQL Database Name=sampledb
        * Version of MySQL Image=5.7

--> Creating resources ...
    service "mysql" created
    deploymentconfig "mysql" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mysql' 
    Run 'oc status' to view your app.


[student@workstation secure-review]$ oc get pods
NAME            READY     STATUS    RESTARTS   AGE
mysql-1-p9799   1/1       Running   0          15s


```

9. 
```

[student@workstation secure-review]$ oc port-forward mysql-1-p9799 3306:3306
Forwarding from 127.0.0.1:3306 -> 3306
Handling connection for 3306


[student@workstation ~]$ mysql -h127.0.0.1 -umysql -predhat
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 7
Server version: 5.7.16 MySQL Community Server (GPL)

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| sampledb           |
+--------------------+
2 rows in set (0.00 sec)

MySQL [(none)]> exit
Bye
[student@workstation ~]$ 


```

10.

```
[student@workstation secure-review]$ oc new-app --name=phpmyadmin --docker-image=registry.lab.example.com/phpmyadmin/phpmyadmin:4.7 -e PMA_HOST=mysql.secure-review.svc.cluster.local
--> Found Docker image f51fd61 (18 months old) from registry.lab.example.com for "registry.lab.example.com/phpmyadmin/phpmyadmin:4.7"

    * An image stream will be created as "phpmyadmin:4.7" that will track this image
    * This image will be deployed in deployment config "phpmyadmin"
    * Ports 80/tcp, 9000/tcp will be load balanced by service "phpmyadmin"
      * Other containers can access this service through the hostname "phpmyadmin"
    * WARNING: Image "registry.lab.example.com/phpmyadmin/phpmyadmin:4.7" runs as the 'root' user which may not be permitted by your cluster administrator

--> Creating resources ...
    imagestream "phpmyadmin" created
    deploymentconfig "phpmyadmin" created
    service "phpmyadmin" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/phpmyadmin' 
    Run 'oc status' to view your app.

[student@workstation secure-review]$ oc get pods
NAME                  READY     STATUS             RESTARTS   AGE
mysql-1-p9799         1/1       Running            0          5m
phpmyadmin-1-deploy   1/1       Running            0          22s
phpmyadmin-1-vhtfq    0/1       CrashLoopBackOff   1          19s


[student@workstation secure-review]$ oc get pods
NAME                  READY     STATUS    RESTARTS   AGE
mysql-1-p9799         1/1       Running   0          6m
phpmyadmin-1-deploy   1/1       Running   0          1m
phpmyadmin-1-vhtfq    0/1       Error     3          1m

```


11.

```
[student@workstation secure-review]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * secure-review

Using project "secure-review".
[student@workstation secure-review]$ oc create serviceaccount phpmyadmin-account
serviceaccount "phpmyadmin-account" created

student@workstation secure-review]$ oc adm policy add-scc-to-user anyuid -z phpmyadmin-account
scc "anyuid" added to: ["system:serviceaccount:secure-review:phpmyadmin-account"]


student@workstation secure-review]$ ls
mysql-ephemeral.yml  patch-dc.sh
[student@workstation secure-review]$ 
[student@workstation secure-review]$ 
[student@workstation secure-review]$ cat patch-dc.sh 
#!/bin/bash

oc patch dc/phpmyadmin --patch \
'{"spec":{"template":{"spec":{"serviceAccountName": "phpmyadmin-account"}}}}'

[student@workstation secure-review]$ oc patch dc/phpmyadmin --patch \
> '{"spec":{"template":{"spec":{"serviceAccountName": "phpmyadmin-account"}}}}'
deploymentconfig "phpmyadmin" patched


[student@workstation secure-review]$ oc login -u user-review -p redhat
Login successful.

You have one project on this server: "secure-review"

Using project "secure-review".
[student@workstation secure-review]$ oc get pods
NAME                 READY     STATUS    RESTARTS   AGE
mysql-1-p9799        1/1       Running   0          11m
phpmyadmin-2-5lsgl   1/1       Running   0          45s


```

12.

```
[student@workstation secure-review]$ oc expose svc/phpmyadmin --hostname=phpmyadmin.apps.lab.example.com
route "phpmyadmin" exposed
[student@workstation secure-review]$ oc get route
NAME         HOST/PORT                         PATH      SERVICES     PORT      TERMINATION   WILDCARD
phpmyadmin   phpmyadmin.apps.lab.example.com             phpmyadmin   80-tcp                  None



```

13.

```
[student@workstation secure-review]$ lab secure-review grade

Grading the student's work for Controlling Access to OpenShift Resources

· Check whether file /etc/origin/master/htpasswd exists........  PASS
· Check whether the username user-review exists................  PASS
· Check whether the password for the user-review...............  PASS
· Check whether the project autocreation was removed for users authenticated  PASS
· Check whether the project autocreation was removed...........  PASS
· Check whether the project secure-review was created..........  PASS
· Check whether the user-review can create apps in secure-review  PASS
· Check secret was created.....................................  PASS
· Check mysql pod was created..................................  PASS
· Check whether the service account phpmyadmin-account was created  PASS
· Check whether the SCC for the serviceaccount was bound to anyuid  PASS
· Check whether the DeploymentConfig was changed...............  PASS
· Check phpmyadmin was redeployed..............................  PASS
· Check phpmyadmin route was created...........................  PASS


```
------------------------------------------------

# CHAPTER6 ALLOCATING PERSISTENT STORAGE(191) 16:16


- 様々なStorageタイプに対応
- Versionによって変わるので、利用するOCPのバージョンがサポートしているストレージを確認すること。
- 4.0で少し減っているはず。

テキストは設定ファイルのことが書いてない。
https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.9/html/installation_and_configuration/persistent-storage-examples

```
例25.1 NFS を使用した永続ボリュームオブジェクトの定義

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv 1
spec:
  capacity:
    storage: 1Gi 2
  accessModes:
    - ReadWriteMany 3
  persistentVolumeReclaimPolicy: Retain 4
  nfs: 5
    path: /opt/nfs 6
    server: nfs.f22 7
    readOnly: false

1
    PV の名前。Pod 定義で参照されたり、各種の oc ボリュームコマンドで表示されたりします。 
2
    このボリュームに割り当てられるストレージの量。 
3
    accessModes は、PV と PVC を一致させるためのラベルとして使用されます。現時点で、これらはいずれの形態のアクセス制御も定義しません。 
4
    ボリューム回収ポリシー Retain は、ボリュームにアクセスする Pod が終了した後にもボリュームが維持されることを示します。 
5
    使用するボリュームタイプを定義します。この例では NFS プラグインを定義しています。 
6
    NFS マウントパスです。 
7
    NFS サーバーです。IP アドレスで指定することもできます。 

```

Sizeは、あくまでキャパシティを書いているだけ。
それをまかなう量を外部ストレージに確保する必要がある。

ReclaimPolicy 再貸し出しの定義。デフォルトは再貸し出ししない。再貸し出しする場合には、全部データが消されてから再貸し出しされる。
-> PVCを消す前に、データのバックアップを取る必要がある。同じ名前で作っても、同じところにはつながらない。UUIDで管理されている。
oc rsync コマンドなどを使う。
PVCを消すとデータが消えると思ってよい。

YAML書いてoc create -f <filename> すると、1こPVが造られる。

NFSやGlusterFSなど、タイプが変わると書式がまったく変わってくる。

Access Mode
- ReadWriteOnce  BlockStorageとか
- ReadOnlyMany   NFS とか
- ReadWriteMany  NFS とか

Disk Typeによって使えるAccess Modeが変わってくる。

PC／PCVの作成

テキストには内容書いてない。
https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.9/html/installation_and_configuration/configuring-persistent-storage#install-config-persistent-storage-persistent-storage-nfs

あくまで、リクエストするためのサイズであって、制限値ではない。
limitとかで上限をいれないと、使えるだけ使ってしまう。
disk quotaとかを入れる。

24.2.3にかいてあるやつ。


pvc は set volume というコマンドで作れる。
演習の6.2あたりにかいてあるやつ。
PV(入れ物とのつなぐもの）を作るー＞PVC（コンテナとの関連付）を作ってDCに関連づけする。
PVはYAML書くしかない。

NFSを使うときの注意点。アクセス権。
同じファイルにアクセスできなくなったりする。

NFS側で匿名接続にしてしまえばみんな同じ権限になりますね。
nfsのexport設定のなかに、all_squash を入れることで、匿名接続になる。

```
/var/export/vol *(rw,async,all_squath)
* は接続元の制限をどうするか。
```
試験のレンジには入っているので、NFSの書き方もちょっと出てくるので注意。

デフォルトではSELINUX設定でNFS禁止されている。
その設定自体はインストール時に設定は入るが、SELINUX関連設定をいじった場合、
そのあたりを忘れないようにしないといけない。

setsebool -P virt_use_nfs=true


Supplemental Group

何も設定しないと、Projectで定義されたSupplimental Idがuser-idとかに使われる。

ブロックストレージは、コンテナ実行ユーザのIDではないので注意。

SELINUX

コンテナ内のSELINUXをさらに協力にいじりたい人向けにいろいろやれます。

P196


演習飛ばして追加説明

P207 のへん。

内部レジストリの話。外部ストレージをつかっています。
インストールするときにつけたオプションで定義がされています。

デフォルトのレジストリボリュームがregistry-volume
なので、ここにPVのマウントをする必要がある。

以前のバージョンだと自分で設定する必要があったが、いまはインストール時に指定してしまえば、自動でやってくれる。


--------------------------------------

# Guided Exercise: Implementing Persistent Database Storage(P198)

認定試験うけるひとは、永続ストレージバッチリでるので注意。
間違えた場合、PVC、PVも全部けしてもう一度作成するほうがよい。
失敗したときにこうすれば回復する、というのは試しておくと良いかも。


- pre

```
[student@workstation ~]$ lab deploy-volume setup

Setting up master for lab exercise work:

 · Check that master host is reachable.........................  SUCCESS
 · OpenShift master is running.................................  SUCCESS
 · Check that node1 is reachable...............................  SUCCESS
 · Check that node2 is reachable...............................  SUCCESS
 · Check that OpenShift node service is running on node1.......  SUCCESS
 · Check that OpenShift node service is running on node2.......  SUCCESS
 · OpenShift runtime is clean..................................  SUCCESS

Downloading files for Guided Exercise: Implementing Persistent Database Storage

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.
 · Copying support files to the master VM......................  SUCCESS
```
1.

```
[student@workstation ~]$ ssh root@services
Warning: Permanently added 'services' (ECDSA) to the list of known hosts.
[root@services ~]# 
[root@services ~]# less -FiX /root/DO280/labs/deploy-volume/config-nfs.sh
#!/bin/bash

# Variable declarations
export_dir="/var/export/dbvol"
export_file="/etc/exports.d/dbvol.exports"
services_hostname="services"

# Ensure that the script runs on the OpenShift master.
  if [[ $(hostname -s) != ${services_hostname} && ${UID} -ne "0" ]]; then
    echo "This script must be run on the ${services_hostname} host as root."
    exit 1
  fi

# Check if export directory exists. If not, create it and set ownership and
# permissions.
  if [ -d ${export_dir} ]; then
    echo "Export directory ${export_dir} already exists."
  else
    mkdir -p ${export_dir}
    chown nfsnobody:nfsnobody ${export_dir}
    chmod 700 ${export_dir}
    echo "Export directory ${export_dir} created."
  fi

# Check if the export file exists in /etc/exports.d. If not, create it.
  if [ -f ${export_file} ]; then
    echo "Export file ${export_file} already exists."
  else
    echo "${export_dir} *(rw,async,all_squash)" > ${export_file}
    exportfs -a
  fi

[root@services ~]# /root/DO280/labs/deploy-volume/config-nfs.sh
Export directory /var/export/dbvol created.

[root@services ~]# showmount -e
Export list for services.lab.example.com:
/exports/prometheus-alertbuffer  *
/exports/prometheus-alertmanager *
/exports/prometheus              *
/exports/etcd-vol2               *
/exports/logging-es-ops          *
/exports/logging-es              *
/exports/metrics                 *
/exports/registry                *
/var/export/dbvol                *



```

2.

```
[student@workstation ~]$ ssh root@node1
Last login: Wed Sep 25 13:38:25 2019 from workstation.lab.example.com


[root@node1 ~]# mount -t nfs services.lab.example.com:/var/export/dbvol /mnt
[root@node1 ~]# 

[root@node1 ~]# ls -la /mnt ; mount | grep /mnt
total 0
drwx------.  2 nfsnobody nfsnobody   6 Sep 25 16:47 .
dr-xr-xr-x. 17 root      root      224 Aug 16  2018 ..
services.lab.example.com:/var/export/dbvol on /mnt type nfs4 (rw,relatime,vers=4.1,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.25.250.11,local_lock=none,addr=172.25.250.13)

[root@node1 ~]# umount /mnt
[root@node1 ~]# 
[root@node1 ~]# exit
logout


[student@workstation ~]$ ssh root@node2
Last login: Wed Sep 25 13:38:33 2019 from workstation.lab.example.com
[root@node2 ~]# 
[root@node2 ~]# mount -t nfs services.lab.example.com:/var/export/dbvol /mnt
[root@node2 ~]# ls -la /mnt ; mount | grep /mnt
total 0
drwx------.  2 nfsnobody nfsnobody   6 Sep 25 16:47 .
dr-xr-xr-x. 17 root      root      224 Aug 16  2018 ..
services.lab.example.com:/var/export/dbvol on /mnt type nfs4 (rw,relatime,vers=4.1,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.25.250.12,local_lock=none,addr=172.25.250.13)


[root@node2 ~]# umount /mnt
[root@node2 ~]# exit
logout


```


3.

```
[student@workstation ~]$ oc login -u admin -p redhat https://master.lab.example.com
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
  * secure-review

Using project "secure-review".
[student@workstation ~]$ less -FiX ~/DO280/labs/deploy-volume/mysqldb-volume.yml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysqldb-volume
spec:
  capacity:
    storage: 3Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /var/export/dbvol
    server: services.lab.example.com
  persistentVolumeReclaimPolicy: Recycle
[student@workstation ~]$ 

student@workstation ~]$ oc create -f ~/DO280/labs/deploy-volume/mysqldb-volume.yml 
persistentvolume "mysqldb-volume" created


[student@workstation ~]$ oc get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                   STORAGECLASS   REASON    AGE
etcd-vol2-volume   1G         RWO            Retain           Bound       openshift-ansible-service-broker/etcd                            1d
mysqldb-volume     3Gi        RWX            Recycle          Available                                                                    15s
registry-volume    40Gi       RWX            Retain           Bound       default/registry-claim                                           1d


```

4.

```
[student@workstation ~]$ oc login -u developer -p redhat https://master.lab.example.com
Login successful.

You have one project on this server: "hogehoge"

Using project "hogehoge".
[student@workstation ~]$ oc whoami
developer

[student@workstation ~]$ oc new-project persistent-storage
Now using project "persistent-storage" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

```

5.

```
[student@workstation ~]$ oc new-app --name=mysqldb --docker-image=registry.lab.example.com/rhscl/mysql-57-rhel7 -e MYSQL_USER=ose -e MYSQL_PASSWORD=openshift -e MYSQL_DATABASE=quotes
--> Found Docker image 4ae3a3f (2 years old) from registry.lab.example.com for "registry.lab.example.com/rhscl/mysql-57-rhel7"

    MySQL 5.7 
    --------- 
    MySQL is a multi-user, multi-threaded SQL database server. The container image provides a containerized packaging of the MySQL mysqld daemon and client application. The mysqld server daemon accepts connections from clients and provides access to content from MySQL databases on behalf of the clients.

    Tags: database, mysql, mysql57, rh-mysql57

    * An image stream will be created as "mysqldb:latest" that will track this image
    * This image will be deployed in deployment config "mysqldb"
    * Port 3306/tcp will be load balanced by service "mysqldb"
      * Other containers can access this service through the hostname "mysqldb"
    * This image declares volumes and will default to use non-persistent, host-local storage.
      You can add persistent volumes later by running 'volume dc/mysqldb --add ...'

--> Creating resources ...
    imagestream "mysqldb" created
    deploymentconfig "mysqldb" created
    service "mysqldb" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mysqldb' 
    Run 'oc status' to view your app.


```


6.

```
[student@workstation ~]$ oc status
In project persistent-storage on server https://master.lab.example.com:443

svc/mysqldb - 172.30.9.241:3306
  dc/mysqldb deploys istag/mysqldb:latest 
    deployment #1 deployed 30 seconds ago - 1 pod


2 infos identified, use 'oc status -v' to see details.


[student@workstation ~]$ oc describe pod mysqldb | grep -A 2 'Volumes'
Volumes:
  mysqldb-volume-1:
    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)

[student@workstation ~]$ oc set volume dc/mysqldb --add --overwrite --name=mysqldb-volume-1 -t pvc --claim-name=mysqldb-pvclaim \
> --claim-size=3Gi \
> --claim-mode='ReadWriteMany'
persistentvolumeclaims/mysqldb-pvclaim
info: deploymentconfigs "mysqldb" was not changed
[student@workstation ~]$ 
[student@workstation ~]$ oc describe pod mysqldb | grep -E -A 2 'Volumes|ClaimName'
Volumes:
  mysqldb-volume-1:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mysqldb-pvclaim
    ReadOnly:   false
  default-token-wgd25:
[student@workstation ~]$ 



```

7.


```
[student@workstation ~]$ oc get pvc
NAME              STATUS    VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mysqldb-pvclaim   Bound     mysqldb-volume   3Gi        RWX                           1m

```

8.

```
[student@workstation ~]$ mysql -h127.0.0.1 -uose -popenshift \
> quotes < /home/student/DO280/labs/deploy-volume/quote.sql 
[student@workstation ~]$ mysql -h127.0.0.1 -uose -popenshift quotes -e "select count(*) from quote;"
+----------+
| count(*) |
+----------+
|        3 |
+----------+
[student@workstation ~]$ 



[student@workstation ~]$ ssh root@services ls -la /var/export/dbvol/
total 41036
drwx------. 6 nfsnobody nfsnobody     4096 Sep 25 17:04 .
drwxr-xr-x. 3 root      root            19 Sep 25 16:47 ..
-rw-r-----. 1 nfsnobody nfsnobody       56 Sep 25 17:04 auto.cnf
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 ca-key.pem
-rw-r--r--. 1 nfsnobody nfsnobody     1075 Sep 25 17:04 ca.pem
-rw-r--r--. 1 nfsnobody nfsnobody     1079 Sep 25 17:04 client-cert.pem
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 client-key.pem
-rw-r-----. 1 nfsnobody nfsnobody      352 Sep 25 17:04 ib_buffer_pool
-rw-r-----. 1 nfsnobody nfsnobody 12582912 Sep 25 17:06 ibdata1
-rw-r-----. 1 nfsnobody nfsnobody  8388608 Sep 25 17:06 ib_logfile0
-rw-r-----. 1 nfsnobody nfsnobody  8388608 Sep 25 17:04 ib_logfile1
-rw-r-----. 1 nfsnobody nfsnobody 12582912 Sep 25 17:05 ibtmp1
drwxr-x---. 2 nfsnobody nfsnobody     4096 Sep 25 17:04 mysql
-rw-r-----. 1 nfsnobody nfsnobody        2 Sep 25 17:04 mysqldb-2-vfk8l.pid
drwxr-x---. 2 nfsnobody nfsnobody     8192 Sep 25 17:04 performance_schema
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 private_key.pem
-rw-r--r--. 1 nfsnobody nfsnobody      452 Sep 25 17:04 public_key.pem
drwxr-x---. 2 nfsnobody nfsnobody       54 Sep 25 17:06 quotes
-rw-r--r--. 1 nfsnobody nfsnobody     1079 Sep 25 17:04 server-cert.pem
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 server-key.pem
drwxr-x---. 2 nfsnobody nfsnobody     8192 Sep 25 17:04 sys

[student@workstation ~]$ ssh root@services ls -la /var/export/dbvol/quotes
total 212
drwxr-x---. 2 nfsnobody nfsnobody    54 Sep 25 17:06 .
drwx------. 6 nfsnobody nfsnobody  4096 Sep 25 17:04 ..
-rw-r-----. 1 nfsnobody nfsnobody    65 Sep 25 17:04 db.opt
-rw-r-----. 1 nfsnobody nfsnobody  8584 Sep 25 17:06 quote.frm
-rw-r-----. 1 nfsnobody nfsnobody 98304 Sep 25 17:06 quote.ibd




```


9. Cleanup

```
[student@workstation ~]$ oc delete project persistent-storage
project "persistent-storage" deleted

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    hogehoge
    kube-public
    kube-service-catalog
    kube-system
    logging
    management-infra
    openshift
    openshift-ansible-service-broker
    openshift-infra
    openshift-node
    openshift-template-service-broker
    openshift-web-console
    secure-review

Using project "default".
[student@workstation ~]$ oc get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                                   STORAGECLASS   REASON    AGE
etcd-vol2-volume   1G         RWO            Retain           Bound      openshift-ansible-service-broker/etcd                            1d
mysqldb-volume     3Gi        RWX            Recycle          Released   persistent-storage/mysqldb-pvclaim                               16m
registry-volume    40Gi       RWX            Retain           Bound      default/registry-claim                                           1d
[student@workstation ~]$ oc delete pv mysqldb-volume
persistentvolume "mysqldb-volume" deleted
[student@workstation ~]$ oc get pv
NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                   STORAGECLASS   REASON    AGE
etcd-vol2-volume   1G         RWO            Retain           Bound     openshift-ansible-service-broker/etcd                            1d
registry-volume    40Gi       RWX            Retain           Bound     default/registry-claim                                           1d


[student@workstation ~]$ ssh root@services ls -la /var/export/dbvol/
total 28744
drwx------. 6 nfsnobody nfsnobody     4096 Sep 25 17:09 .
drwxr-xr-x. 3 root      root            19 Sep 25 16:47 ..
-rw-r-----. 1 nfsnobody nfsnobody       56 Sep 25 17:04 auto.cnf
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 ca-key.pem
-rw-r--r--. 1 nfsnobody nfsnobody     1075 Sep 25 17:04 ca.pem
-rw-r--r--. 1 nfsnobody nfsnobody     1079 Sep 25 17:04 client-cert.pem
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 client-key.pem
-rw-r-----. 1 nfsnobody nfsnobody      323 Sep 25 17:09 ib_buffer_pool
-rw-r-----. 1 nfsnobody nfsnobody 12582912 Sep 25 17:09 ibdata1
-rw-r-----. 1 nfsnobody nfsnobody  8388608 Sep 25 17:09 ib_logfile0
-rw-r-----. 1 nfsnobody nfsnobody  8388608 Sep 25 17:04 ib_logfile1
drwxr-x---. 2 nfsnobody nfsnobody     4096 Sep 25 17:04 mysql
drwxr-x---. 2 nfsnobody nfsnobody     8192 Sep 25 17:04 performance_schema
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 private_key.pem
-rw-r--r--. 1 nfsnobody nfsnobody      452 Sep 25 17:04 public_key.pem
drwxr-x---. 2 nfsnobody nfsnobody       54 Sep 25 17:06 quotes
-rw-r--r--. 1 nfsnobody nfsnobody     1079 Sep 25 17:04 server-cert.pem
-rw-------. 1 nfsnobody nfsnobody     1680 Sep 25 17:04 server-key.pem
drwxr-x---. 2 nfsnobody nfsnobody     8192 Sep 25 17:04 sys


[student@workstation ~]$ ssh root@services rm -rf /var/export/dbvol/*
[student@workstation ~]$ ssh root@services ls -la /var/export/dbvol/
total 0
drwx------. 2 nfsnobody nfsnobody  6 Sep 25 17:11 .
drwxr-xr-x. 3 root      root      19 Sep 25 16:47 ..


[student@workstation ~]$ lab deploy-volume cleanup

Cleaning up the lab on workstation:

 · Removing lab files from workstation.........................  SUCCESS
 · Removed persistent-storage project..........................  SUCCESS
 · Removing database files.....................................  SUCCESS


```


## Lab: Allocating Persistent Storage (P212)


```
この時点で1806で手がつけられず／／／
image trainingするしかない？



```

- pre

```
[student@workstation schedule-is]$ lab storage-review setup

Checking prerequisites for Lab: Allocating Persistent Storage

 Checking all VMs are running:
 · master VM is up.............................................  SUCCESS
 · node1 VM is up..............................................  SUCCESS
 · node2 VM is up..............................................  SUCCESS
 Checking all OpenShift default pods are ready and running:
 · Check router................................................  SUCCESS
 · Check registry..............................................  SUCCESS

Downloading files for Lab: Allocating Persistent Storage

 · Downloading starter project.................................  SUCCESS
 · Downloading solution project................................  SUCCESS

Download successful.
 · Copy lab files to the services VM...........................  SUCCESS
 · Copy solution files to the services VM......................  SUCCESS

Overall setup status...........................................  SUCCESS


```
1.

```
[root@services ~]# cat /root/DO280/labs/storage-review/config-review-nfs.sh 
#!/bin/bash

# Variable declarations
export_dir="/var/export/review-dbvol"
export_file="/etc/exports.d/review-dbvol.exports"
services_hostname="services"

# Ensure that the script runs on the OpenShift master.
  if [[ $(hostname -s) != ${services_hostname} && ${UID} -ne "0" ]]; then
    echo "This script must be ran on the ${services_hostname} host as root."
    exit 1
  fi

# Check if export directory exists. If not, create it and set ownership and
# permissions.
  if [ -d ${export_dir} ]; then
    echo "Export directory ${export_dir} already exists."
  else
    mkdir -p ${export_dir}
    chown nfsnobody:nfsnobody ${export_dir}
    chmod 700 ${export_dir}
    echo "Export directory ${export_dir} created."
  fi

# Check if the export file exists in /etc/exports.d. If not, create it.
  if [ -f ${export_file} ]; then
    echo "Export file ${export_file} already exists."
  else
    echo "${export_dir} *(rw,async,all_squash)" > ${export_file}
    exportfs -a
  fi
[root@services ~]# 


[root@services ~]# showmount -e
Export list for services.lab.example.com:
/exports/prometheus-alertbuffer  *
/exports/prometheus-alertmanager *
/exports/prometheus              *
/exports/etcd-vol2               *
/exports/logging-es-ops          *
/exports/logging-es              *
/exports/metrics                 *
/exports/registry                *
/var/export/dbvol                *


[root@services ~]# /root/DO280/labs/storage-review/config-review-nfs.sh 
Export directory /var/export/review-dbvol created.
[root@services ~]# 


[root@services ~]# showmount -e
Export list for services.lab.example.com:
/var/export/dbvol                *
/var/export/review-dbvol         *
/exports/prometheus-alertbuffer  *
/exports/prometheus-alertmanager *
/exports/prometheus              *
/exports/etcd-vol2               *
/exports/logging-es-ops          *
/exports/logging-es              *
/exports/metrics                 *
/exports/registry                *

```

2.

```
[student@workstation schedule-is]$ oc whoami
admin
[student@workstation schedule-is]$ cat /home/student/DO280/labs/storage-review/review-volume-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: review-pv
spec:
  capacity:
    storage: 3Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /var/export/review-dbvol
    server: services.lab.example.com
  persistentVolumeReclaimPolicy: Recycle

[student@workstation schedule-is]$ /home/student/DO280/labs/storage-review/review-volume-pv.yaml 
bash: /home/student/DO280/labs/storage-review/review-volume-pv.yaml: Permission denied


[student@workstation schedule-is]$ oc create -f /home/student/DO280/labs/storage-review/review-volume-pv.yaml 
persistentvolume "review-pv" created
[student@workstation schedule-is]$ 

```

3.

```
[student@workstation schedule-is]$ oc project openshift
Now using project "openshift" on server "https://master.lab.example.com:443".


[student@workstation schedule-is]$ cat /home/student/DO280/labs/storage-review/instructor-template.yaml 
apiVersion: v1
kind: Template
labels:
  template: instructor
message: |-
  The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.

metadata:
  annotations:
    description: An example PHP application with a MySQL database.
    iconClass: icon-php
    openshift.io/display-name: The Instructor Application Template
    tags: quickstart,php
    template.openshift.io/documentation-url: https://github.com/openshift/
    template.openshift.io/long-description: This template defines resources needed
      to develop the Instructir application, including a build configuration, application
      deployment configuration, and database deployment configuration.
    template.openshift.io/provider-display-name: Red Hat Training
    template.openshift.io/support-url: https://access.redhat.com
  creationTimestamp: null
  name: instructor
objects:
- apiVersion: v1
  kind: Secret
  metadata:
    name: ${NAME}
  stringData:
    database-root-password: ${DATABASE_ROOT_PASSWORD}
    database-password: ${DATABASE_PASSWORD}
    database-user: ${DATABASE_USER}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      description: Exposes and load balances the application pods
      service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
        "kind": "Service"}]'
    name: ${NAME}
  spec:
    ports:
    - name: web
      port: 8080
      targetPort: 8080
    selector:
      name: ${NAME}
- apiVersion: v1
  kind: Route
  metadata:
    name: ${NAME}
  spec:
    host: ${APPLICATION_DOMAIN}
    to:
      kind: Service
      name: ${NAME}
- apiVersion: v1
  kind: ImageStream
  metadata:
    annotations:
      description: Keeps track of changes in the application image
    name: ${NAME}
- apiVersion: v1
  kind: BuildConfig
  metadata:
    annotations:
      description: Defines how to build the application
    name: ${NAME}
  spec:
    output:
      to:
        kind: ImageStreamTag
        name: ${NAME}:latest
    source:
      contextDir: ${CONTEXT_DIR}
      git:
        ref: ${SOURCE_REPOSITORY_REF}
        uri: ${SOURCE_REPOSITORY_URL}
      type: Git
    strategy:
      sourceStrategy:
        from:
          kind: ImageStreamTag
          name: php:7.0
          namespace: ${NAMESPACE}
      type: Source
    triggers:
    - type: ImageChange
    - type: ConfigChange
    - github:
        secret: ${GITHUB_WEBHOOK_SECRET}
      type: GitHub
    - generic:
        secret: ${GENERIC_WEBHOOK_SECRET}
      type: Generic
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the application server
    name: ${NAME}
  spec:
    replicas: 1
    selector:
      name: ${NAME}
    strategy:
      type: Rolling
    template:
      metadata:
        labels:
          name: ${NAME}
        name: ${NAME}
      spec:
        containers:
        - env:
          - name: DATABASE_SERVICE_NAME
            value: ${DATABASE_SERVICE_NAME}
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: MYSQL_DATABASE
            value: ${DATABASE_NAME}
          image: ' '
          name: instructor
          ports:
          - containerPort: 8080
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - instructor
        from:
          kind: ImageStreamTag
          name: ${NAME}:latest
      type: ImageChange
    - type: ConfigChange
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: ${DATABASE_SERVICE_NAME}-pvc
  spec:
    accessModes:
    - ReadWriteMany
    resources:
      requests:
        storage: ${VOLUME_CAPACITY}
    selector:
      name: review-pv
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      description: Exposes the database server
    name: ${DATABASE_SERVICE_NAME}
  spec:
    ports:
    - name: mysql
      port: 3306
      targetPort: 3306
    selector:
      name: ${DATABASE_SERVICE_NAME}
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the database
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          name: ${DATABASE_SERVICE_NAME}
        name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - env:
          - name: MYSQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: MYSQL_DATABASE
            value: ${DATABASE_NAME}
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-root-password
                name: ${NAME}
          image: ' '
          name: mysql
          ports:
          - containerPort: 3306
          resources:
            limits:
              memory: ${MEMORY_MYSQL_LIMIT}
          volumeMounts:
          - mountPath: /var/lib/mysql/data
            name: ${DATABASE_SERVICE_NAME}-data
        volumes:
        - name: ${DATABASE_SERVICE_NAME}-data
          persistentVolumeClaim:
            claimName: ${DATABASE_SERVICE_NAME}-pvc
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - mysql
        from:
          kind: ImageStreamTag
          name: mysql:5.7
          namespace: ${NAMESPACE}
      type: ImageChange
    - type: ConfigChange
parameters:
- description: The name assigned to all of the frontend objects defined in this template.
  displayName: Name
  name: NAME
  required: true
  value: instructor
- description: The OpenShift Namespace where the ImageStream resides.
  displayName: Namespace
  name: NAMESPACE
  required: true
  value: openshift
- description: Maximum amount of memory the Node.js container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 512Mi
- description: Maximum amount of memory the MySQL container can use.
  displayName: Memory Limit (MySQL)
  name: MEMORY_MYSQL_LIMIT
  required: true
  value: 512Mi
- description: Volume space available for data, e.g. 512Mi, 2Gi
  displayName: Volume Capacity
  name: VOLUME_CAPACITY
  required: true
  value: 1Gi
- description: The URL of the repository with your application source code.
  displayName: Git Repository URL
  name: SOURCE_REPOSITORY_URL
  required: true
  value: http://services.lab.example.com/instructor
- description: Set this to a branch name, tag or other ref of your repository if you
    are not using the default branch.
  displayName: Git Reference
  name: SOURCE_REPOSITORY_REF
- description: Set this to the relative path to your project if it is not in the root
    of your repository.
  displayName: Context Directory
  name: CONTEXT_DIR
- description: The exposed hostname that will route to the PHP service, if left
    blank a value will be defaulted.
  displayName: Application Hostname
  name: APPLICATION_DOMAIN
- description: A secret string used to configure the GitHub webhook.
  displayName: GitHub Webhook Secret
  from: '[a-zA-Z0-9]{40}'
  generate: expression
  name: GITHUB_WEBHOOK_SECRET
- description: A secret string used to configure the Generic webhook.
  displayName: Generic Webhook Secret
  from: '[a-zA-Z0-9]{40}'
  generate: expression
  name: GENERIC_WEBHOOK_SECRET
- displayName: Database Service Name
  name: DATABASE_SERVICE_NAME
  required: true
  value: mysql
- description: Username for MySQL user that will be used for accessing the database.
  displayName: MySQL Username
  name: DATABASE_USER
  value: instructor
- description: Password for the MySQL user.
  displayName: MySQL Password
  name: DATABASE_PASSWORD
  value: password
- displayName: Database Name
  name: DATABASE_NAME
  required: true
  value: instructor
- description: Password for the MySQL database root user.
  displayName: Database Administrator Password
  name: DATABASE_ROOT_PASSWORD
  value: redhat
[student@workstation schedule-is]$ 



[student@workstation schedule-is]$ oc create -f /home/student/DO280/labs/storage-review/instructor-template.yaml 
template "instructor" created
[student@workstation schedule-is]$ 


```

4.

```
[student@workstation schedule-is]$ oc login -u developer
Logged into "https://master.lab.example.com:443" as "developer" using existing credentials.

You have one project on this server: "hogehoge"

Using project "hogehoge".
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ 
[student@workstation schedule-is]$ oc new-project instructor
Now using project "instructor" on server "https://master.lab.example.com:443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.

```

5.

```

```


```
20190926-1329

[student@workstation schedule-is]$ lab storage-review grade

Grading the student's work for Lab: Allocating Persistent Storage

 · Check if the mysql pod is in Running state..................  FAIL
 · Check if the instructor pod is in Running state.............  FAIL
 . Checking if REST interface can be invoked successfully......  PASS
 . Checking if the instructor template was imported correctly..  PASS
 . Checking if the instructor route can be invoked successfully  FAIL

Overall exercise grade.........................................  FAIL

[student@workstation schedule-is]$ 

```
